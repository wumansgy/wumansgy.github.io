<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Docker01-基础使用]]></title>
    <url>%2F2018%2F12%2F30%2FDocker01-%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[第 1 章 Docker基础1.1 docker简介在这一部分我们主要讲两个方面：docker是什么、docker特点 1.1.1 docker是什么docker是什么？​ docker的中文解释是码头工人。 官方解释： ​ Docker是一个开源的容器引擎，它基于LCX容器技术，使用Go语言开发。 ​ 源代码托管在Github上，并遵从Apache2.0协议。 ​ Docker采用C/S架构，其可以轻松的为任何应用创建一个轻量级的、可移植的、自给自足的容器。 ​ Docker就是一种快速解决生产问题的一种技术手段,开发，运行和部署应用程序的开放管理平台。 ​ 开发人员能利用docker 开发和运行应用程序 ​ 运维人员能利用docker 部署和管理应用程序 Docker的生活场景对比： 物理机 虚拟机 容器是 单独的理解一下容器: 动画片《七龙珠》里面的胶囊 1号胶囊启动后的效果 ​ Docker提供了在一个完全隔离的环境中打包和运行应用程序的能力，这个隔离的环境被称为容器。 ​ 由于容器的隔离性和安全性，因此可以在一个主机(宿主机)上同时运行多个相互隔离的容器，互不干预。 1.1.2为什么使用Docker​ Docker使您能够将应用程序与基础架构分开，以便您可以快速交付软件。 ​ 借助Docker，您可以像管理应用程序一样管理基础架构。 ​ 通过利用Docker的方法快速进行运输，测试和部署代码，您可以显着缩短编写代码和在生产环境中运行代码之间的延迟。 例如:​ 开发人员在本地编写代码，可以使用Docker同事进行共享，实现协同工作。 ​ 使用Docker开发完成程序，可以直接对应用程序执行自动和手动测试。 ​ 当开发人员发现错误或BUG时，可以直接在开发环境中修复后，并迅速将它们重新部署到测试环境进行测试和验证。 ​ 利用Docker开发完成后，交付时，直接交付Docker，也就意味着交付完成。后续如果有提供修补程序或更新，需要推送到生成环境运行起来，也是一样的简单。 Docker主要解决的问题： ​ 保证程序运行环境的一致性； ​ 降低配置开发环境、生产环境的复杂度和成本； ​ 实现程序的快速部署和分发。 1.1.3Docker的架构与结构架构图 Docker是采用了(c/s)架构模式的应用程序 Client dockerCLI :客户端docker命令行 REST API : 一套介于客户端与服务端的之间进行通信并指示其执行的接口 Server docker daemon:服务端dacker守护进程等待客户端发送命令来执行 Docker的四大核心技术 IMAGE-镜像 CONTAINER-容器 DATA VOLUMES-数据卷 NETWORK-网络 结构图 Docker客户端(Docker Client) Docker客户端(Docker Client)是用户与Docker进行交互的最主要方式。当在终端输入docker命令时，对应的就会在服务端产生对应的作用，并把结果返回给客户端。Docker Client除了连接本地服务端，通过更改或指定DOCKER_HOST连接远程服务端。 Docker服务端(Docker Server) Docker Daemon其实就是Docker 的服务端。它负责监听Docker API请求(如Docker Client)并管理Docker对象(Docker Objects)，如镜像、容器、网络、数据卷等 Docker Registries 俗称Docker仓库，专门用于存储镜像的云服务环境. Docker Hub就是一个公有的存放镜像的地方，类似Github存储代码文件。同样的也可以类似Github那样搭建私有的仓库。 Docker 对象(Docker Objects) 镜像：一个Docker的可执行文件，其中包括运行应用程序所需的所有代码内容、依赖库、环境变量和配置文件等。 容器：镜像被运行起来后的实例。 网络：外部或者容器间如何互相访问的网络方式，如host模式、bridge模式。 数据卷：容器与宿主机之间、容器与容器之间共享存储方式，类似虚拟机与主机之间的共享文件目录。 1.1.4官方资料：Docker 官网：http://www.docker.com Github Docker 源码：https://github.com/docker/docker Docker 英文文档网址：https://docs.docker.com/ Docker 中文文档网址：http://docker-doc.readthedocs.io/zh_CN/latest/ 1.1.4docker特点三大理念：构建：龙珠里的胶囊，将你需要的场景构建好，装在一个小胶囊里 运输：随身携带着房子、车子等，非常方便 运行：只需要你轻轻按一下胶囊，找个合适的地方一放，就ok了 优点：多： 适用场景多 快： 环境部署快、更新快 好： 好多人在用 省： 省钱省力省人工 缺点：太腻歪人： 依赖操作系统 不善沟通： 依赖网络 不善理财： 银行U盾等场景不能用 1.2 docker快速入门1.2.1docker历程：​ 自2013年出现以来，发展势头很猛，现在可说是风靡全球。 ​ docker的第一版为0.1.0 发布于2013年03月23日 ​ Docker2017年改版前的版本号是1.13.1发布于2017年02月08日 ​ Docker从1.13.x版本开始，版本分为企业版EE和社区版CE，版本号也改为按照时间线来发布，比如17.03就是2017年3月，有点类似于ubuntu的版本发布方式。​ 企业版自然会提供一些额外的服务，当然肯定也是收费的。​ 企业版说明https://blog.docker.com/2017/03/docker-enterprise-edition/社区版分为stable和edge两种发布方式。 stable版本是季度发布方式，比如17.03, 17.06, 17.09 edge版本是月份发布方式， 比如17.03, 17.04…… 注： Stable：gives you reliable updates every quarter (稳定:给你可靠的每季度更新一次) Edge：gives you new features every month (优势:每个月给你新特性) 1.2.2 官方要求为什么用ubuntu学docker 图片来源：https://docs.docker.com/engine/installation/#server docker要求的ubuntu环境 ubuntu下载地址：https://www.ubuntu.com/download/desktop ubuntu主机环境需求123#执行命令$ uname -a$ ls -l /sys/class/misc/device-mapper 执行效果 1.2.3 部署docker官网参考：https://docs.docker.com/engine/installation/linux/docker-ce/ubuntu/#upgrade-docker-after-using-the-convenience-script 安装步骤123456789101112131415161718192021222324#安装基本软件$ sudo apt-get update$ sudo apt-get install apt-transport-https ca-certificates curl software-properties-common lrzsz -y#使用官方推荐源&#123;不推荐&#125;#$ sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"#使用阿里云的源&#123;推荐&#125;$ sudo curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -$ sudo add-apt-repository "deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable"#软件源升级$ sudo apt-get update#安装docker$ sudo apt-get install docker-ce -y#注：#可以指定版本安装docker：$ sudo apt-get install docker-ce=&lt;VERSION&gt; -y #查看支持的docker版本$ sudo apt-cache madison docker-ce#测试dockerdocker version 网卡区别：安装前：只有ens33和lo网卡 安装后：docker启动后，多出来了docker0网卡，网卡地址172.17.0.1 1.2.4 docker加速器在国内使用docker的官方镜像源，会因为网络的原因，造成无法下载，或者一直处于超时。所以我们使用 daocloud的方法进行加速配置。加速器文档链接：http://guide.daocloud.io/dcs/daocloud-9153151.html 方法:访问 https://dashboard.daocloud.io 网站，登录 daocloud 账户 点击右上角的 加速器 在新窗口处会显示一条命令， 1234567#我们执行这条命令curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://f1361db2.m.daocloud.io# cat /etc/docker/daemon.json&#123;"registry-mirrors": ["http://f1361db2.m.daocloud.io"]&#125;#重启dockersystemctl restart docker 1.2.5 docker 其他简介docker的基本命令格式：1234567#基本格式systemctl [参数] docker#参数详解： start 开启服务 stop 关闭 restart 重启 status 状态 删除docker命令：123$ sudo apt-get purge docker-ce -y$ sudo rm -rf /etc/docker$ sudo rm -rf /var/lib/docker/ docker基本目录简介:12/etc/docker/ #docker的认证目录/var/lib/docker/ #docker的应用目录 docker常见问题:背景​ 因为使用的是sudo安装docker，所以会导致一个问题。以普通用户登录的状况下，在使用docker images时必须添加sudo，那么如何让docker免sudo依然可用呢？ 理清问题​ 当以普通用户身份去使用docker命令时，出现以下错误： 1Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.35/images/create?fromSrc=-&amp;message=&amp;repo=ubuntu-16.04&amp;tag=: dial unix /var/run/docker.sock: connect: permission denied ​ 可以看都，最后告知我们时权限的问题。那么在linux文件权限有三个数据左右drwxrwxrwx，其中第一为d代表该文件是一个文件夹前三位、中三位、后三位分别代表这属主权限、属组权限、其他人权限。 ​ 上图是报错文件的权限展示，可以看到其属主为root，权限为rw，可读可写；其属组为docker，权限为rw，可读可写。如果要当前用户可直接读取该文件，那么我们就为docker.sock 添加一个其他用户可读写权限 或者添加1个用户组就可以了 方法1：一劳永逸12345678910#如果还没有 docker group 就添加一个：$sudo groupadd docker#将用户加入该 group 内。然后退出并重新登录就生效啦。$sudo gpasswd -a $&#123;USER&#125; docker#重启 docker 服务$systemctl restart docker#切换当前会话到新 group 或者重启 X 会话$newgrp - docker#注意:最后一步是必须的，否则因为 groups 命令获取到的是缓存的组信息，刚添加的组信息未能生效，#所以 docker images 执行时同样有错。 方法2：123#每次启动docker或者重启docker的之后$cd /var/run$sudo chmod 666 docker.sock 方法3：每条命令前面加上sudo总结Docker 安装方法 下载客户端安装 命令行安装 1.3 docker安装和卸载安装123456789101112# 1. 安装基本软件$ sudo apt-get update$ sudo apt-get install apt-transport-https ca-certificates curl software-properties-common lrzsz -y# 2. apt仓库添加并阿里云的docker软件包$ sudo curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -$ sudo add-apt-repository "deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable"# 3. 升级软件源$ sudo apt-get update# 4. 安装docker$ sudo apt-get install docker-ce -y# 5. 测试docker$ docker version 卸载123456789101112# 删除dockersudo apt-get purge docker-ce -y# 删除对应的目录# 身份认证目录sudo rm /etc/docker -r# 存储下载的docker镜像, 数据卷挂载目录sudo rm /var/lib/docker -rsudo passwd root- 输入当前用户的密码# 也可以切换到管理员身份su root docker加速器设置 -&gt; 可选123456789# 访问 https://dashboard.daocloud.io 网站，登录 daocloud 账户# 提交下载镜像的速度# 打开加速页面, 拷贝下边的命令到linux终端, 执行curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://f1361db2.m.daocloud.io# 可以去 /etc/docker/daemon.json这个文件中添加了一句话:&#123;"registry-mirrors": ["http://f1361db2.m.daocloud.io"]&#125;# 重启docker服务sudo systemctl restart docker 1.4 权限问题1234567891011121314# 只需要设置一次# 可以将当前用户加入到docker所属的组中就可以了# docker所属的组叫dockersudo gpasswd -a $USER docker# 重启docker服务sudo systemctl restart docker# 切换当前用户到docker组newgrp - docker# 第二种方式 -&gt; docker服务器每重启之后, 都需要设置# 进入到 /var/run# 修改docker.sock文件的权限sudo chmod 666 docker.sock 1.5 docker服务相关操作命令12345678# docker服务器关闭sudo systemctl stop docker# docker服务器启动sudo systemctl start docker# docker服务器重启sudo systemctl restart docker# docker服务器状态查看sudo systemctl status docker 2. docker镜像管理2.1 镜像的搜索/获取/查看 镜像搜索123456789# 命令$ docker search 镜像名称# 字段关键字NAME: 镜像的名字DESCRIPTION: 对镜像的描述STARS: 下载量OFFICIAL: 是不是官方发布 的AUTOMATED: 是不是自动启动镜像中携带的程序 举例: 在一个镜像中安装了nginx, 启动镜像的时候nginx是否跟随启动 获取镜像 1234# 下载远程仓库（如Docker Hub）中的镜像$ docker pull 镜像名称# 镜像存储目录/var/lib/docker/image 查看镜像 12345678910111213# 命令# 查看所有的镜像docker imagesdocker image ls# 查看某个镜像docker images 镜像的名字docker image ls 镜像的名字# 字段关键字REPOSITORY: 镜像的名字, 不是唯一的TAG: 镜像的版本, latest代表最新的版本IMAGE ID: 镜像的ID, 这是唯一的CREATED: 镜像创建的时间SIZE: 镜像大小 2.2 镜像重命名/删除 镜像重命名 12$ docker tag 源镜像名:tag 新的名字:tag重命名并不会重新复制一个镜像 删除镜像 1234$ docker rmi 镜像名/镜像ID# 注意事项: 如果镜像被重命名了, 通过镜像ID是不能直接删除的如果镜像被重命名了2次, 需要删除两次镜像才会被删除 2.3 镜像的导入导出 镜像导入 12docker load -i/--input 要导入的镜像文件的名字docker load &lt; 要导入的镜像文件的名字 镜像导出 1docker save -o/--output 导出之后的镜像名(自己起名) 要导出的镜像的镜像名/镜像ID 2.4 镜像的历史和详细信息 查看镜像的历史信息 12# 制作镜像的时候的操作步骤docker history 镜像名/镜像ID 查看镜像的详细信息 123# 镜像的属性信息, 以json格式输出的docker inspect 镜像名/镜像IDdocker inspect -f &#123;&#123;.&#125;&#125; 镜像名/镜像ID 2.6 总结 3. docker容器管理docker将镜像文件启动, 得到一个容器, 一个容器可以被看做一个操作系统 3.1 容器的查看/创建/启动 容器查看 1234567891011# 命令docker ps # 查看运行状态的容器docker ps -a # 查看所有状态的容器# 字段关键字CONTAINER ID: 容器的IDIMAGE: 镜像名, 容器是基于那个镜像启动的COMMAND: 内部执行的命令CREATED: 容器被创建的时间STATUS: 当前容器的状态: 运行, 暂停, 停止PORTS: 容器对外的端口NAMES: 创建出来的容器的名字, 没指定这个名字是随机的 容器创建 12345678910111213# 容器被创建, 但是还不能使用, 需要启动$ docker create [OPTIONS] IMAGE [COMMAND] [ARG...] - OPTIONS: 创建容器的一些参数 -i, --interactive: 容器是否和标准输入进行关联 -t, --tty: 创建容器的时候是不是给容器管理虚拟终端 --rm: 容器停止之后是否会自动销毁 --name: 给创建的容器指定一个名字, 如果不指定, 名字是随机生成的 - IMAGE: 镜像名/镜像ID - [COMMAND] [ARG...] 启动容器之后, 在容器中执行一个命令 - 不知道指定什么命令, 指定bash - 容器中装了redis redis-server 容器启动 启动创建的容器 1234567$ docker start 参数 容器的名字 - 参数: a: 关联标准输出/标准错误 i: 关联标准输入# 应用场景: - 创建了一个容器, 需要启动 - 容器的运行被终止stop, 可以再次启动 创建新容器并启动 1234567891011121314# 创建并运行容器 run == create + start$ docker run [OPTIONS] IMAGE [COMMAND] [ARG...]- OPTIONS: -i, --interactive: 容器是否和标准输入进行关联 -t, --tty: 创建容器的时候是不是给容器管理虚拟终端 --rm: 容器停止之后是否会自动销毁 --name: 给创建的容器指定一个名字, 如果不指定, 名字是随机生成的 -d, --detach: 容器启动之后是守护进程, 容器启动之后, 不能直接进入到容器内部- IMAGE: 容器名字/容器ID- [COMMAND] [ARG...] 启动容器之后, 在容器中执行一个命令 - 不知道指定什么命令, 指定bash == /bin/bash - 容器中装了redis redis-server 3.2 容器的暂停/重启 暂停 1docker pause 容器名/容器ID 取消暂停 1docker unpause 容器名/容器ID 重启 1234docker restart [OPTIONS] CONTAINER [CONTAINER...]- OPTIONS: -t: 延时重启, 默认10s- CONTAINER: 容器名/ 容器ID 3.3 容器的关闭/终止/删除 关闭 123# 延时关闭容器, 默认10sdocker stopdocker stop -t 19 # 指定延时关闭的时间 终止 12# 直接马上关闭docker kill 删除 删除未运行的容器 1docker rm 容器名/容器ID 删除运行的容器 1docker rm -f 容器名/容器ID 批量删除容器 12docker rm $(docker ps -aq)docker rm `docker ps -aq` # 反单引号波浪号对应的键, esc下面的键 3.4 容器的进入/退出 进入容器 创建并进入 1docker run -it --name 容器名 镜像名 shell命令 手动进入 12docker run -itd --name 容器名 镜像名 shell命令docker exec -it 容器名/容器ID bash 退出容器 12exitctrl+d 3.5 容器的日志/信息/端口/重命名 查看容器的日志信息 1docker logs 容器名/容器ID 查看容器的详细信息 123456# https://yq.aliyun.com/articles/230067# 查看镜像的详细信息docker inspect 镜像名/镜像ID# 查看容器详细信息docker inspect 容器名/容器IDdocker inspect -f &#123;&#123;.NetworkSettings.Networks.bridge.Gateway&#125;&#125; 73b8e3d09e6c 查看容器的端口信息 12# 查看本机和容器的端口映射docker port 容器名/容器ID 容器重命名 1docker rename 旧的容器名 新的容器名 3.6 总结]]></content>
      <categories>
        <category>Docker以及HyperLedger-Fabric</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务介绍]]></title>
    <url>%2F2018%2F12%2F22%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[微服务 一.微服务（microservices）近几年,微服这个词闯入了我们的实线范围。在百度与谷歌中随便搜一搜也有几千万条的结果。那么，什么是微服务呢？微服务的概念是怎么产生的呢？ 我们就来了解一下Go语言与微服务的千丝万缕与来龙去脉。 什么是微服务？在介绍微服务时，首先得先理解什么是微服务，顾名思义，微服务得从两个方面去理解，什么是”微”、什么是”服务”？微（micro） 狭义来讲就是体积小，著名的”2 pizza 团队”很好的诠释了这一解释（2 pizza 团队最早是亚马逊 CEO Bezos提出来的，意思是说单个服务的设计，所有参与人从设计、开发、测试、运维所有人加起来 只需要2个披萨就够了 ）。服务（service） 一定要区别于系统，服务一个或者一组相对较小且独立的功能单元，是用户可以感知最小功能集。 那么广义上来讲，微服务是一种分布式系统解决方案，推动细粒度服务的使用，这些服务协同工作。 微服务这个概念的由来？据说，早在2011年5月，在威尼斯附近的软件架构师讨论会上，就有人提出了微服务架构设计的概念，用它来描述与会者所见的一种通用的架构设计风格。时隔一年之后，在同一个讨论会上，大家决定将这种架构设计风格用微服务架构来表示。起初，对微服务的概念，没有一个明确的定义，大家只能从各自的角度说出了微服务的理解和看法。 有人把微服务理解为一种细粒度SOA（service-oriented Architecture，面向服务架构），一种轻量级的组件化的小型SOA。在2014年3月，詹姆斯·刘易斯（James Lewis）与马丁·福勒（Martin Fowler）所发表的一篇博客中，总结了微服务架构设计的一些共同特点，这应该是一个对微服务比较全面的描述。 1原文链接 https://martinfowler.com/articles/microservices.html 这篇文章中认为：“简而言之，微服务架构风格是将单个应用程序作为一组小型服务开发的方法，每个服务程序都在自己的进程中运行，并与轻量级机制（通常是HTTP资源API）进行通信。这些服务是围绕业务功能构建的。可以通过全自动部署机器独立部署。这些服务器可以用不同的编程语言编写，使用不同的数据存储技术，并尽量不用集中式方式进行管理” 微服务与微服务框架在这里我们可能混淆了一个点，那就是微服务和微服务架构，这应该是两个不同的概念，而我们平时说道的微服务可能就已经包含了这两个概念了，所以我们要把它们说清楚以免我们很纠结。微服务架构是一种设计方法，而微服务这是应该指使用这种方法而设计的一个应用。所以我们必要对微服务的概念做出一个比较明确的定义。 微服务框架是将复杂的系统使用组件化的方式进行拆分，并使用轻量级通讯方式进行整合的一种设计方法。 微服务是通过这种架构设计方法拆分出来的一个独立的组件化的小应用。 微服务架构定义的精髓，可以用一句话来描述，那就是“分而治之，合而用之”。 将复杂的系统进行拆分的方法，就是“分而治之”。分而治之，可以让复杂的事情变的简单，这很符合我们平时处理问题的方法。 使用轻量级通讯等方式进行整合的设计，就是“合而用之”的方法，合而用之可以让微小的力量变动强大。 微服务架构和整体式架构的区别？开发单体式（整体式）应用的不足之处 三层架构（MVC）的具体内容如下：表示层（view）： 用户使用应用程序时，看到的、听见的、输入的或者交互的部分。业务逻辑层（controller）： 根据用户输入的信息，进行逻辑计算或者业务处理的部分。数据访问层（model）： 关注有效地操作原始数据的部分，如将数据存储到存储介质（如数据库、文件系统）及从存储介质中读取数据等。虽然现在程序被分成了三层，但只是逻辑上的分层，并不是物理上的分层。也就是说，对不同层的代码而言，经过编译、打包和部署后，所有的代码最终还是运行在同一个进程中。而这，就是所谓的单块架构。单体架构在规模比较小的情况下工作情况良好，但是随着系统规模的扩大，它暴露出来的问题也越来越多，主要有以下几点： 复杂性逐渐变高 比如有的项目有几十万行代码，各个模块之间区别比较模糊，逻辑比较混乱，代码越多复杂性越高，越难解决遇到的问题。 技术债务逐渐上升公司的人员流动是再正常不过的事情，有的员工在离职之前，疏于代码质量的自我管束，导致留下来很多坑，由于单体项目代码量庞大的惊人，留下的坑很难被发觉，这就给新来的员工带来很大的烦恼，人员流动越大所留下的坑越多，也就是所谓的技术债务越来越多。 维护成本大当应用程序的功能越来越多、团队越来越大时，沟通成本、管理成本显著增加。当出现 bug 时，可能引起 bug 的原因组合越来越多，导致分析、定位和修复的成本增加；并且在对全局功能缺乏深度理解的情况下，容易在修复bug 时引入新的 bug。 持续交付周期长构建和部署时间会随着功能的增多而增加，任何细微的修改都会触发部署流水线。新人培养周期长：新成员了解背景、熟悉业务和配置环境的时间越来越长。 技术选型成本高单块架构倾向于采用统一的技术平台或方案来解决所有问题，如果后续想引入新的技术或框架，成本和风险都很大。 可扩展性差随着功能的增加，垂直扩展的成本将会越来越大；而对于水平扩展而言，因为所有代码都运行在同一个进程，没办法做到针对应用程序的部分功能做独立的扩展。 微服务架构的特性 单一职责 微服务架构中的每个服务，都是具有业务逻辑的，符合高内聚、低耦合原则以及单一职责原则的单元，不同的服务通过“管道”的方式灵活组合，从而构建出庞大的系统。 轻量级通信服务之间通过轻量级的通信机制实现互通互联，而所谓的轻量级，通常指语言无关、平台无关的交互方式。 对于轻量级通信的格式而言，我们熟悉的 XML 和 JSON，它们是语言无关、平台无关的；对于通信的协议而言，通常基于 HTTP，能让服务间的通信变得标准化、无状态化。目前大家熟悉的 REST（Representational StateTransfer）是实现服务间互相协作的轻量级通信机制之一。使用轻量级通信机制，可以让团队选择更适合的语言、工具或者平台来开发服务本身。 问：REST是什么和restful一样吗？ 答：REST 指的是一组架构约束条件和原则。满足这些约束条件和原则的应用程序或设计就是 RESTful。 独立性 每个服务在应用交付过程中，独立地开发、测试和部署。 在单块架构中所有功能都在同一个代码库，功能的开发不具有独立性；当不同小组完成多个功能后，需要经过集成和回归测试，测试过程也不具有独立性；当测试完成后，应用被构建成一个包，如果某个功能存在 bug，将导致整个部署失败或者回滚。 在微服务架构中，每个服务都是独立的业务单元，与其他服务高度解耦，只需要改变当前服务本身，就可以完成独立的开发、测试和部署。 进程隔离 单块架构中，整个系统运行在同一个进程中，当应用进行部署时，必须停掉当前正在运行的应用，部署完成后再重启进程，无法做到独立部署。有时候我们会将重复的代码抽取出来封装成组件，在单块架构中，组件通常的形态叫做共享库（如 jar 包或者DLL），但是当程序运行时，所有组件最终也会被加载到同一进程中运行。 在微服务架构中，应用程序由多个服务组成，每个服务都是高度自治的独立业务实体，可以运行在独立的进程中，不同的服务能非常容易地部署到不同的主机上。 微服务架构的缺点运维要求较高对于单体架构来讲，我们只需要维护好这一个项目就可以了，但是对于微服务架构来讲，由于项目是由多个微服务构成的，每个模块出现问题都会造成整个项目运行出现异常，想要知道是哪个模块造成的问题往往是不容易的，因为我们无法一步一步通过debug的方式来跟踪，这就对运维人员提出了很高的要求。 分布式的复杂性对于单体架构来讲，我们可以不使用分布式，但是对于微服务架构来说，分布式几乎是必会用的技术，由于分布式本身的复杂性，导致微服务架构也变得复杂起来。 接口调整成本高比如，用户微服务是要被订单微服务和电影微服务所调用的，一旦用户微服务的接口发生大的变动，那么所有依赖它的微服务都要做相应的调整，由于微服务可能非常多，那么调整接口所造成的成本将会明显提高。 重复劳动对于单体架构来讲，如果某段业务被多个模块所共同使用，我们便可以抽象成一个工具类，被所有模块直接调用，但是微服务却无法这样做，因为这个微服务的工具类是不能被其它微服务所直接调用的，从而我们便不得不在每个微服务上都建这么一个工具类，从而导致代码的重复。 传统单体架构 分布式微服务化架构 新功能开发 需要时间 容易开发和实线 部署 不经常而且容易部署 经常发布，部署复杂 隔离性 故障影响范围大 故障影响范围小 架构设计 初期设计选型难度大 设计逻辑难度大 系统性能 相应时间快，吞吐量小 相应时间慢，吞吐量大 系统运维 运维简单 运维复杂 新人上手 学习曲线大（应用逻辑） 学习曲线大（架构逻辑） 技术 技术单一而且封闭 技术多样而且开发 测试和差错 简单 复杂（每个服务都要进行单独测试，还需要集群测试） 系统拓展性 扩展性差 扩展性好 系统管理 重点在于开发成本 重点在于服务治理和调度 为什么使用微服务架构开发简单微服务架构将复杂系统进行拆分之后，让每个微服务应用都开放变得非常简单，没有太多的累赘。对于每一个开发者来说，这无疑是一种解脱，因为再也不用进行繁重的劳动了，每天都在一种轻松愉快的氛围中工作，其效率也会整备地提高 快速响应需求变化一般的需求变化都来自于局部功能的改变，这种变化将落实到每个微服务上，二每个微服务的功能相对来说都非常简单，更改起来非常容易，所以微服务非常是和敏捷开发方法，能够快速的影响业务的需求变化。 随时随地更新一方面，微服务的部署和更新并不会影响全局系统的正常运行；另一方面，使用多实例的部署方法，可以做到一个服务的重启和更新在不易察觉的情况下进行。所以每个服务任何时候都可以进行更新部署。 系统更加稳定可靠微服务运行在一个高可用的分布式环境之中，有配套的监控和调度管理机制，并且还可以提供自由伸缩的管理，充分保障了系统的稳定可靠性]]></content>
      <categories>
        <category>微服务</category>
      </categories>
      <tags>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[比特币相关交易图片]]></title>
    <url>%2F2018%2F12%2F21%2F%E6%AF%94%E7%89%B9%E5%B8%81%E7%9B%B8%E5%85%B3%E4%BA%A4%E6%98%93%E5%9B%BE%E7%89%87%2F</url>
    <content type="text"><![CDATA[几张比特币中的相关图片比特币地址生成 比特币交易大概 解锁脚本和锁定脚本相关]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go1-11新功能module的介绍及使用]]></title>
    <url>%2F2018%2F11%2F10%2FGo1-11%E6%96%B0%E5%8A%9F%E8%83%BDmodule%E7%9A%84%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Go1.1.1版本发布(2018-08-24发布)已经过去一段时间，从官方的博客中看到，有两个比较突出的特色，一个就是今天讲的module，模块概念。目前该功能还在试验阶段，有些地方还需要不断的进行完善。不过可以先尝尝鲜，感受下这个功能的魅力。 主要概念介绍module是一个相关Go包的集合，它是源代码更替和版本控制的单元。模块由源文件形成的go.mod文件的根目录定义，包含go.mod文件的目录也被称为模块根目录。moudles取代旧的的基于GOPATH方法来指定在工程中使用哪些源文件或导入包。模块路径是导入包的路径前缀，go.mod文件定义模块路径，并且列出了在项目构建过程中使用的特定版本。 go.mod文件go.mod文件还可以指定要替换和排除的版本，命令行会自动根据go.mod文件来维护需求声明中的版本。如果想获取更多的有关go.mod文件的介绍，可以使用命令go help go.mod。 1go.mod文件用“//”注释，而不用“/**/”。文件的每行都有一条指令，由一个动作加上参数组成。例如： 12345module my/thingrequire other/thing v1.0.2require new/thing v2.3.4exclude old/thing v1.2.3replace bad/thing v1.4.5 =&gt; good/thing v1.4.5 上面三个动词分别表示：项目需要的依赖包及版本、排除某些包的特别版本、取代当前项目中的某些依赖包。相同动作的命令可以放到一个动词+括号组成的结构中，例如： 1234require ( new/thing v2.3.4 old/thing v1.2.3) 其他命令的支持旧的版本，构建编译命令go build中的参数没有-mod参数，最新的版本现在多了这个，用来对go.mod文件进行更新或其他使用控制。形式如：go build -mod [mode]，其中mode有以下几种取值：readonly，release，vendor。当执行go build -mod=vendor的时候，会在生成可执行文件的同时将项目的依赖包放到主模块的vendor目录下。go get -m [packages]会将下载的依赖包放到GOPATH/pkg/mod目录下，并且将依赖写入到go.mod文件。go get -u=patch会更新主模块下的所有依赖包。 Go mod工具在最新的版本，为了支持在命令行直接控制module的操作，Go提供了mod工具。格式如下：go mod &lt;command&gt; [arguments]。其中命令有以下几种取值： 1234567download //下载模块到本地缓存，具体可以通过命令go env查看，其中环境变量GOCACHE就是缓存的地址，如果该文件夹的内容太大，可以通过命令go clean -cacheedit //从工具或脚本中编辑go.mod文件graph //打印模块需求图init //在当前目录下初始化新的模块tidy //添加缺失的模块以及移除无用的模块verify //验证依赖项是否达到预期的目的why //解释为什么需要包或模块 虚拟版本号go.mod文件和go命令通常使用语义版本作为描述模块版本的标准形式，这样可以比较不同版本的先后顺序。例如模块的版本是v1.2.3，那么通过重新对版本号进行标签处理，得到该版本的虚拟版本。形式如：v0.0.0-yyyymmddhhmmss-abcdefabcdef。其中时间是提交时的UTC时间，最后的后缀是提交的哈希值前缀。时间部分确保两个虚拟版本号可以进行比较，以确定两者顺序。下面有三种形式的虚拟版本号： vX.0.0-yyyymmddhhmmss-abcdefabcdef，这种情况适合用在在目标版本提交之前 ，没有更早的的版本。（这种形式本来是唯一的形式，所以一些老的go.mod文件使用这种形式） vX.Y.Z-pre.0.yyyymmddhhmmss-abcdefabcdef，这种情况被用在当目标版本提交之前的最新版本提交是vX.Y.Z-pre。 vX.Y.(Z+1)-0.yyyymmddhhmmss-abcdefabcdef，同理，这种情况是当目标版本提交之前的最新版本是vX.Y.Z。 虚拟版本的生成不需要你去手动操作，go命令会将接收的commit哈希值自动转化为虚拟版本号。 环境变量——GO111MODULEGo1.11 module支持一个临时的环境变量——GO111MODULE,它可以设置以下三个字符串中的一个：off,on或者auto，用来控制是否启用module功能。 GO111MODULE=off，设置成这个的时候，go命令行将不会支持module功能，寻找依赖包的方式将会沿用旧版本那种通过vendor目录或者GOPATH模式来查找。 GO111MODULE=on，这种设置，go命令行会使用modules，而一点也不会去GOPATH目录下查找。这种方式也称为“模块-感知”或者“模块-感知 模式”。 GO111MODULE=auto，或者不设置此变量的值，go命令行将会根据当前目录来决定是否启用module功能。这种情况下可以分为两种情形：当前目录在GOPATH/src之外且该目录包含go.mod文件，或者当前文件在包含go.mod文件的目录下面。 当module功能启用时，GOPATH在项目构建过程中不再担当import的角色，但它仍然存储下载的依赖包，具体位置在GOPATH/pkg/mod。 具体使用步骤： 首先将你的版本更新到最新的Go版本1.11，如何更新版本可以自行百度。 2.通过go命令行，进入到你当前的工程目录下，在命令行设置临时环境变量set GO111MODULE=on； 3.执行命令go mod init在当前目录下生成一个go.mod文件，执行这条命令时，当前目录不能存在go.mod文件。如果之前生成过，要先删除； 4.如果你工程中存在一些不能确定版本的包，那么生成的go.mod文件可能就不完整，因此继续执行下面的命令； 5.执行go mod tidy命令，它会添加缺失的模块以及移除不需要的模块。执行后会生成go.sum文件(模块下载条目)。添加参数-v，例如go mod tidy -v可以将执行的信息，即删除和添加的包打印到命令行； 6.执行命令go mod verify来检查当前模块的依赖是否全部下载下来，是否下载下来被修改过。如果所有的模块都没有被修改过，那么执行这条命令之后，会打印all modules verified。 7.执行命令go mod vendor生成vendor文件夹，该文件夹下将会放置你go.mod文件描述的依赖包，文件夹下同时还有一个文件modules.txt，它是你整个工程的所有模块。在执行这条命令之前，如果你工程之前有vendor目录，应该先进行删除。同理go mod vendor -v会将添加到vendor中的模块打印出来；]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Derek解读-Bytom源码分析-持久化存储LevelDB2-cache缓存]]></title>
    <url>%2F2018%2F11%2F10%2FDerek%E8%A7%A3%E8%AF%BB-Bytom%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8LevelDB2-cache%E7%BC%93%E5%AD%98%2F</url>
    <content type="text"><![CDATA[Derek解读-Bytom源码分析-持久化存储LevelDB(二)-cache缓存简介https://github.com/Bytom/bytom 本章介绍Derek解读-Bytom源码分析-持久化存储LevelDB(二)-cache缓存 作者使用MacOS操作系统，其他平台也大同小异 Golang Version: 1.8 block cache缓存介绍上一篇《Derek解读-Bytom源码分析-持久化存储LevelDB》文章介绍了比原链数据持久化到磁盘。当执行读取数据时从cache中得到，实现快速访问。 比原链的block cache主要实现有两个部分： lru: 缓存淘汰算法 fillFn: 回调函数 比原链的block cache实现过程如下： 执行GetBlock函数 从block cache中查询，如果命中缓存则返回 block cache中未命中则执行fillFn回调函数从磁盘中获取并缓存block cache中，然后返回 blockCache分析blockCache结构database/leveldb/cache.go 123456type blockCache struct &#123; mu sync.Mutex lru *lru.Cache fillFn func(hash *bc.Hash) *types.Block single singleflight.Group&#125; mu: 互斥锁，保证共享数据操作的一致性 lru: 缓存淘汰算法 fillFn: 回调函数 single: 回调函数的调用机制，同一时间保证只有一个回调函数在执行。 缓存命中和不命中的操作database/leveldb/cache.go 1234567891011121314151617181920212223242526272829func (c *blockCache) lookup(hash *bc.Hash) (*types.Block, error) &#123; if b, ok := c.get(hash); ok &#123; return b, nil &#125; block, err := c.single.Do(hash.String(), func() (interface&#123;&#125;, error) &#123; b := c.fillFn(hash) if b == nil &#123; return nil, fmt.Errorf(&quot;There are no block with given hash %s&quot;, hash.String()) &#125; c.add(b) return b, nil &#125;) if err != nil &#123; return nil, err &#125; return block.(*types.Block), nil&#125;func (c *blockCache) get(hash *bc.Hash) (*types.Block, bool) &#123; c.mu.Lock() block, ok := c.lru.Get(*hash) c.mu.Unlock() if block == nil &#123; return nil, ok &#125; return block.(*types.Block), ok&#125; lookup函数从block cache中查询，如果命中缓存则返回。如果block cache未命中则single.Do执行fillFn回调函数从磁盘中获取并缓存block cache中，然后返回 LRU缓存淘汰算法LRU（Least recently used）算法根据数据的历史访问记录来进行淘汰数据，如果数据最近被访问过，那么缓存被命中的几率也更高。 新的数据插入到链表头部 每当缓存命中，则将数据移到链表头部 当链表满的时候，将链表尾部的数据丢弃 LRU算法实现LRU一般使用hash map和doubly linked list双向链表实现 实例化LRU CACHEvendor/github.com/golang/groupcache/lru/lru.go 1234567func New(maxEntries int) *Cache &#123; return &amp;Cache&#123; MaxEntries: maxEntries, ll: list.New(), cache: make(map[interface&#123;&#125;]*list.Element), &#125;&#125; MaxEntries:最大缓存条目数，如果为0则表示无限制 ll: 双向链表数据结构 LRU查询元素12345678910func (c *Cache) Get(key Key) (value interface&#123;&#125;, ok bool) &#123; if c.cache == nil &#123; return &#125; if ele, hit := c.cache[key]; hit &#123; c.ll.MoveToFront(ele) return ele.Value.(*entry).value, true &#125; return&#125; Get获取元素，如果元素命中则将元素移至链表头部，保持最新的访问 LRU添加元素12345678910111213141516func (c *Cache) Add(key Key, value interface&#123;&#125;) &#123; if c.cache == nil &#123; c.cache = make(map[interface&#123;&#125;]*list.Element) c.ll = list.New() &#125; if ee, ok := c.cache[key]; ok &#123; c.ll.MoveToFront(ee) ee.Value.(*entry).value = value return &#125; ele := c.ll.PushFront(&amp;entry&#123;key, value&#125;) c.cache[key] = ele if c.MaxEntries != 0 &amp;&amp; c.ll.Len() &gt; c.MaxEntries &#123; c.RemoveOldest() &#125;&#125; Add添加元素有三步操作： 如果当前缓存中存在该元素则将元素移至链表头并返回 如果缓存中不存在该元素则将元素插入链表头部 如果缓存的元素超过MaxEntries限制则移除链表最末尾的元素 转载链接：https://shanhuhai5739.github.io/2018/08/23/Derek%E8%A7%A3%E8%AF%BB-Bytom%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8LevelDB-%E4%BA%8C-cache%E7%BC%93%E5%AD%98/]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Derek解读-Bytom源码分析-持久化存储LevelDB]]></title>
    <url>%2F2018%2F11%2F10%2FDerek%E8%A7%A3%E8%AF%BB-Bytom%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8LevelDB%2F</url>
    <content type="text"><![CDATA[Derek解读-Bytom源码分析-持久化存储LevelDB简介https://github.com/Bytom/bytom 本章介绍Derek解读-Bytom源码分析-持久化存储LevelDB 作者使用MacOS操作系统，其他平台也大同小异 Golang Version: 1.8 LevelDB介绍比原链默认使用leveldb数据库。Leveldb是一个google实现的非常高效的kv数据库。LevelDB是单进程的服务，性能非常之高，在一台4核Q6600的CPU机器上，每秒钟写数据超过40w，而随机读的性能每秒钟超过10w。由于Leveldb是单进程服务，不能同时有多个进程进行对一个数据库进行读写。同一时间只能有一个进程，或一个进程多并发的方式进行读写。比原链在数据存储层上存储所有链上地址、资产交易等信息。 LevelDB的增删改查操作LevelDB是google开发的一个高性能K/V存储，本节我们介绍下LevelDB如何对LevelDB增删改查。 123456789101112131415161718192021222324252627282930package mainimport ( &quot;fmt&quot; dbm &quot;github.com/tendermint/tmlibs/db&quot;)var ( Key = &quot;TESTKEY&quot; LevelDBDir = &quot;/tmp/data&quot;)func main() &#123; db := dbm.NewDB(&quot;test&quot;, &quot;leveldb&quot;, LevelDBDir) defer db.Close() db.Set([]byte(Key), []byte(&quot;This is a test.&quot;)) value := db.Get([]byte(Key)) if value == nil &#123; return &#125; fmt.Printf(&quot;key:%v, value:%v\n&quot;, Key, string(value)) db.Delete([]byte(Key))&#125;// Output// key:TESTKEY, value:This is a test. 以上Output是执行该程序得到的输出结果。 该程序对leveld进行了增删改查操作。dbm.NewDB得到db对象，在/tmp/data目录下会生成一个叫test.db的目录。该目录存放该数据库的所有数据。db.Set 设置key的value值，key不存在则新建，key存在则修改。db.Get 得到key中value数据。db.Delete 删除key及value的数据。 比原链的数据库默认情况下，数据存储目录在–home参数下的data目录。以Darwin平台为例，默认数据库存储在 $HOME/Library/Bytom/data。 accesstoken.db token信息(钱包访问控制权限) core.db 核心数据库，存储主链相关数据。包括块信息、交易信息、资产信息等 discover.db 分布式网络中端到端的节点信息 trusthistory.db txdb.db 存储交易相关信息 txfeeds.db 目前比原链代码版本未使用该功能，暂不介绍 wallet.db 本地钱包数据库。存储用户、资产、交易、utox等信息 以上所有数据库都由database模块管理 比原数据库接口在比原链中数据持久化存储由database模块管理，但是持久化相关接口在protocol/store.go中 12345678910111213type Store interface &#123; BlockExist(*bc.Hash) bool GetBlock(*bc.Hash) (*types.Block, error) GetStoreStatus() *BlockStoreState GetTransactionStatus(*bc.Hash) (*bc.TransactionStatus, error) GetTransactionsUtxo(*state.UtxoViewpoint, []*bc.Tx) error GetUtxo(*bc.Hash) (*storage.UtxoEntry, error) LoadBlockIndex() (*state.BlockIndex, error) SaveBlock(*types.Block, *bc.TransactionStatus) error SaveChainStatus(*state.BlockNode, *state.UtxoViewpoint) error&#125; BlockExist 根据hash判断区块是否存在 GetBlock 根据hash获取该区块 GetStoreStatus 获取store的存储状态 GetTransactionStatus 根据hash获取该块中所有交易的状态 GetTransactionsUtxo 缓存与输入txs相关的所有utxo GetUtxo(*bc.Hash) 根据hash获取该块内的所有utxo LoadBlockIndex 加载块索引，从db中读取所有block header信息并缓存在内存中 SaveBlock 存储块和交易状态 SaveChainStatus 设置主链的状态，当节点第一次启动时，节点会根据key为blockStore的内容判断是否初始化主链。 比原链数据库key前缀database/leveldb/store.go 123456var ( blockStoreKey = []byte(&quot;blockStore&quot;) blockPrefix = []byte(&quot;B:&quot;) blockHeaderPrefix = []byte(&quot;BH:&quot;) txStatusPrefix = []byte(&quot;BTS:&quot;)) blockStoreKey 主链状态前缀 blockPrefix 块信息前缀 blockHeaderPrefix 块头信息前缀 txStatusPrefix 交易状态前缀 GetBlock查询块过程分析database/leveldb/store.go 123func (s *Store) GetBlock(hash *bc.Hash) (*types.Block, error) &#123; return s.cache.lookup(hash)&#125; database/leveldb/cache.go 12345678910111213141516171819func (c *blockCache) lookup(hash *bc.Hash) (*types.Block, error) &#123; if b, ok := c.get(hash); ok &#123; return b, nil &#125; block, err := c.single.Do(hash.String(), func() (interface&#123;&#125;, error) &#123; b := c.fillFn(hash) if b == nil &#123; return nil, fmt.Errorf(&quot;There are no block with given hash %s&quot;, hash.String()) &#125; c.add(b) return b, nil &#125;) if err != nil &#123; return nil, err &#125; return block.(*types.Block), nil&#125; GetBlock函数最终会执行lookup函数。lookup函数总共操作有两步： 从缓存中查询hash值，如果查到则返回 如果为从缓存中查询到则回调fillFn回调函数。fillFn回调函数会将从磁盘上获得到块信息存储到缓存中并返回该块的信息。 fillFn回调函数实际上调取的是database/leveldb/store.go下的GetBlock，它会从磁盘中获取block信息并返回。 转载链接：https://shanhuhai5739.github.io/2018/08/22/Derek%E8%A7%A3%E8%AF%BB-Bytom%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8LevelDB/]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Derek解读-Bytom源码分析-创世区块]]></title>
    <url>%2F2018%2F11%2F10%2FDerek%E8%A7%A3%E8%AF%BB-Bytom%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-%E5%88%9B%E4%B8%96%E5%8C%BA%E5%9D%97%2F</url>
    <content type="text"><![CDATA[Derek解读-Bytom源码分析-创世区块简介https://github.com/Bytom/bytom 本章介绍Derek解读-Bytom源码分析-创世区块 作者使用MacOS操作系统，其他平台也大同小异 Golang Version: 1.8 创世区块介绍区块链里的第一个区块创被称为创世区块。它是区块链里面所有区块的共同祖先。 在比原链中创世区块被硬编码到bytomd中，每一个比原节点都始于同一个创世区块，这能确保创世区块不会被改变。每个节点都把创世区块作为区块链的首区块，从而构建了一个安全的、可信的区块链。 获取创世区块1234567891011121314151617181920212223242526272829303132333435363738394041424344./bytomcli get-block 0&#123; &quot;bits&quot;: 2161727821137910500, &quot;difficulty&quot;: &quot;15154807&quot;, &quot;hash&quot;: &quot;a75483474799ea1aa6bb910a1a5025b4372bf20bef20f246a2c2dc5e12e8a053&quot;, &quot;height&quot;: 0, &quot;nonce&quot;: 9253507043297, &quot;previous_block_hash&quot;: &quot;0000000000000000000000000000000000000000000000000000000000000000&quot;, &quot;size&quot;: 546, &quot;timestamp&quot;: 1524549600, &quot;transaction_merkle_root&quot;: &quot;58e45ceb675a0b3d7ad3ab9d4288048789de8194e9766b26d8f42fdb624d4390&quot;, &quot;transaction_status_hash&quot;: &quot;c9c377e5192668bc0a367e4a4764f11e7c725ecced1d7b6a492974fab1b6d5bc&quot;, &quot;transactions&quot;: [ &#123; &quot;id&quot;: &quot;158d7d7c6a8d2464725d508fafca76f0838d998eacaacb42ccc58cfb0c155352&quot;, &quot;inputs&quot;: [ &#123; &quot;amount&quot;: 0, &quot;arbitrary&quot;: &quot;496e666f726d6174696f6e20697320706f7765722e202d2d204a616e2f31312f323031332e20436f6d707574696e6720697320706f7765722e202d2d204170722f32342f323031382e&quot;, &quot;asset_definition&quot;: &#123;&#125;, &quot;asset_id&quot;: &quot;0000000000000000000000000000000000000000000000000000000000000000&quot;, &quot;type&quot;: &quot;coinbase&quot; &#125; ], &quot;outputs&quot;: [ &#123; &quot;address&quot;: &quot;bm1q3jwsv0lhfmndnlag3kp6avpcq6pkd3xy8e5r88&quot;, &quot;amount&quot;: 140700041250000000, &quot;asset_definition&quot;: &#123;&#125;, &quot;asset_id&quot;: &quot;ffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff&quot;, &quot;control_program&quot;: &quot;00148c9d063ff74ee6d9ffa88d83aeb038068366c4c4&quot;, &quot;id&quot;: &quot;e3325bf07c4385af4b60ad6ecc682ee0773f9b96e1cfbbae9f0f12b86b5f1093&quot;, &quot;position&quot;: 0, &quot;type&quot;: &quot;control&quot; &#125; ], &quot;size&quot;: 151, &quot;status_fail&quot;: false, &quot;time_range&quot;: 0, &quot;version&quot;: 1 &#125; ], &quot;version&quot;: 1&#125; 使用bytomcli客户端查询高度为0的区块信息。我们可以看到以上输出结果。 bits: 目标值,挖矿时计算的hash之后要小于等于的目标值则新块构建成功 difficulty: 难度值，矿工找到下一个有效区块的难度。该参数并不存储在区块链上，是由bits计算得出 hash: 当前区块hash height: 当前区块高度 nonce: 随机数，挖矿时反复使用不同的nonce来生成不同哈希值 previous_block_hash: 当前区块的父区块hash值 size: 当前区块的字节数 timestamp: 出块时间 transaction_merkle_root: 创世区块的merkle树根节点 transactions: 当前块中的utxo交易 由于创世区块是第一个块，创世区块的父区块，也就是previous_block_hash参数，默认情况下为0000000000000000000000000000000000000000000000000000000000000000 时间戳timestamp为1524549600，时间为2018-04-24 14:00:00也就是比原链上主网的时间。 源码分析获取区块链状态protocol/protocol.go 1234567891011func NewChain(store Store, txPool *TxPool) (*Chain, error) &#123; // ... storeStatus := store.GetStoreStatus() if storeStatus == nil &#123; if err := c.initChainStatus(); err != nil &#123; return nil, err &#125; storeStatus = store.GetStoreStatus() &#125; // ...&#125; 当我们第一次启动比原链节点时，store.GetStoreStatus会从db中获取存储状态，获取存储状态的过程是从LevelDB中查询key为blockStore的数据，如果查询出错则认为是第一次运行比原链节点，那么就需要初始化比原主链。 初始化主链protocol/protocol.go 1234567891011121314151617181920212223func (c *Chain) initChainStatus() error &#123; genesisBlock := config.GenesisBlock() txStatus := bc.NewTransactionStatus() for i := range genesisBlock.Transactions &#123; txStatus.SetStatus(i, false) &#125; if err := c.store.SaveBlock(genesisBlock, txStatus); err != nil &#123; return err &#125; utxoView := state.NewUtxoViewpoint() bcBlock := types.MapBlock(genesisBlock) if err := utxoView.ApplyBlock(bcBlock, txStatus); err != nil &#123; return err &#125; node, err := state.NewBlockNode(&amp;genesisBlock.BlockHeader, nil) if err != nil &#123; return err &#125; return c.store.SaveChainStatus(node, utxoView)&#125; 初始化主链有几步操作： config.GenesisBlock()获取创世区块 设置创世区块中所有交易状态 存储创世区块到LevelDB state.NewUtxoViewpoint()用于临时小部分utxo状态存储集合 实例化BlockNode，BlockNode用于选择最佳链作为主链 保存最新主链状态 被硬编码的创世区块config/genesis.go 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748func genesisTx() *types.Tx &#123; contract, err := hex.DecodeString(&quot;00148c9d063ff74ee6d9ffa88d83aeb038068366c4c4&quot;) if err != nil &#123; log.Panicf(&quot;fail on decode genesis tx output control program&quot;) &#125; txData := types.TxData&#123; Version: 1, Inputs: []*types.TxInput&#123; types.NewCoinbaseInput([]byte(&quot;Information is power. -- Jan/11/2013. Computing is power. -- Apr/24/2018.&quot;)), &#125;, Outputs: []*types.TxOutput&#123; types.NewTxOutput(*consensus.BTMAssetID, consensus.InitialBlockSubsidy, contract), &#125;, &#125; return types.NewTx(txData)&#125;func mainNetGenesisBlock() *types.Block &#123; tx := genesisTx() txStatus := bc.NewTransactionStatus() txStatus.SetStatus(0, false) txStatusHash, err := bc.TxStatusMerkleRoot(txStatus.VerifyStatus) if err != nil &#123; log.Panicf(&quot;fail on calc genesis tx status merkle root&quot;) &#125; merkleRoot, err := bc.TxMerkleRoot([]*bc.Tx&#123;tx.Tx&#125;) if err != nil &#123; log.Panicf(&quot;fail on calc genesis tx merkel root&quot;) &#125; block := &amp;types.Block&#123; BlockHeader: types.BlockHeader&#123; Version: 1, Height: 0, Nonce: 9253507043297, Timestamp: 1524549600, Bits: 2161727821137910632, BlockCommitment: types.BlockCommitment&#123; TransactionsMerkleRoot: merkleRoot, TransactionStatusHash: txStatusHash, &#125;, &#125;, Transactions: []*types.Tx&#123;tx&#125;, &#125; return block&#125; mainNetGenesisBlock主要有如下操作： 生成创世区块中的交易，默认就一笔交易 设置块中的交易状态为false 将创世区块设置为merkle树的根节点 实例化Block块并返回 genesisTx函数生成创世区块中的交易，默认就一笔交易，一笔交易中包含input输入和output输出。 input输入：输入中有一句话”Information is power. – Jan/11/2013. Computing is power. – Apr/24/2018.”这是为了纪念Aaron Swartz的精神 output输出：输出中我们看到consensus.InitialBlockSubsidy创世区块的奖励。总共140700041250000000/1e8 = 1407000412。也就是14亿个BTM币。 计算即权力引用比原链创始人长铗的话： 4月24号，我们主网上线，信息即权力，2013年Jaruary11；计算即权力，2018年April24。这句话是为了纪念Aaron Swartz的精神，信息即权力可以视为互联网宣言，致力于信息自由传播，让公民隐私得到保护。计算即权力，致力于让资产自由的交易，自由的流动，让公民的财富得到保护，我觉得这是非常好的纪念。 转载链接：https://shanhuhai5739.github.io/2018/08/18/Derek%E8%A7%A3%E8%AF%BB-Bytom%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-%E5%88%9B%E4%B8%96%E5%8C%BA%E5%9D%97/]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bytom源码分析-protobuf生成核心代码]]></title>
    <url>%2F2018%2F11%2F10%2Fbytom%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-p2p%E7%BD%91%E7%BB%9C-protobuf%E7%94%9F%E6%88%90%E6%A0%B8%E5%BF%83%E4%BB%A3%E7%A0%81%2F</url>
    <content type="text"><![CDATA[bytom源码分析-protobuf生成比原核心代码简介https://github.com/Bytom/bytom 本章介绍bytom代码Api-Server接口服务 作者使用MacOS操作系统，其他平台也大同小异 Golang Version: 1.8 protobuf生成比原核心代码protobuf介绍Protocol buffers是一个灵活的、高效的、自动化的用于对结构化数据进行序列化的协议。Protocol buffers序列化后的码流更小、速度更快、操作更简单。只需要将序列化的数据结构(.proto文件)，便可以生成的源代码。 protobuf 3.0语法介绍protobuf 语法 protobuf 安装安装protobuf 3.4.0protobuf download 1234./configuremakemake installprotoc —version 安装grpc-go123export PATH=$PATH:$GOPATH/bingo get -u google.golang.org/grpcgo get -u github.com/golang/protobuf/protoc-gen-go 查看比原bc.proto核心文件protocol/bc/bc.proto 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116syntax = &quot;proto3&quot;;package bc;message Hash &#123; fixed64 v0 = 1; fixed64 v1 = 2; fixed64 v2 = 3; fixed64 v3 = 4;&#125;message Program &#123; uint64 vm_version = 1; bytes code = 2;&#125;// This message type duplicates Hash, above. One alternative is to// embed a Hash inside an AssetID. But it&apos;s useful for AssetID to be// plain old data (without pointers). Another alternative is use Hash// in any protobuf types where an AssetID is called for, but it&apos;s// preferable to have type safety.message AssetID &#123; fixed64 v0 = 1; fixed64 v1 = 2; fixed64 v2 = 3; fixed64 v3 = 4;&#125;message AssetAmount &#123; AssetID asset_id = 1; uint64 amount = 2;&#125;message AssetDefinition &#123; Program issuance_program = 1; Hash data = 2;&#125;message ValueSource &#123; Hash ref = 1; AssetAmount value = 2; uint64 position = 3;&#125;message ValueDestination &#123; Hash ref = 1; AssetAmount value = 2; uint64 position = 3;&#125;message BlockHeader &#123; uint64 version = 1; uint64 height = 2; Hash previous_block_id = 3; uint64 timestamp = 4; Hash transactions_root = 5; Hash transaction_status_hash = 6; uint64 nonce = 7; uint64 bits = 8; TransactionStatus transaction_status = 9;&#125;message TxHeader &#123; uint64 version = 1; uint64 serialized_size = 2; uint64 time_range = 3; repeated Hash result_ids = 4;&#125;message TxVerifyResult &#123; bool status_fail = 1;&#125;message TransactionStatus &#123; uint64 version = 1; repeated TxVerifyResult verify_status = 2;&#125;message Mux &#123; repeated ValueSource sources = 1; // issuances, spends, and muxes Program program = 2; repeated ValueDestination witness_destinations = 3; // outputs, retirements, and muxes repeated bytes witness_arguments = 4;&#125;message Coinbase &#123; ValueDestination witness_destination = 1; bytes arbitrary = 2;&#125;message Output &#123; ValueSource source = 1; Program control_program = 2; uint64 ordinal = 3;&#125;message Retirement &#123; ValueSource source = 1; uint64 ordinal = 2;&#125;message Issuance &#123; Hash nonce_hash = 1; AssetAmount value = 2; ValueDestination witness_destination = 3; AssetDefinition witness_asset_definition = 4; repeated bytes witness_arguments = 5; uint64 ordinal = 6;&#125;message Spend &#123; Hash spent_output_id = 1; ValueDestination witness_destination = 2; repeated bytes witness_arguments = 3; uint64 ordinal = 4;&#125; 根据bc.proto生成bc.pb.go代码1234protoc -I/usr/local/include -I. \ -I$&#123;GOPATH&#125;/src \ --go_out=plugins=grpc:. \ ./*.proto 执行完上面命令，我们会看到当前目录下生成的bc.pb.go文件，该文件在比原链中承载这block、transaction、coinbase等重要数据结构 转载链接：https://shanhuhai5739.github.io/2018/08/17/bytom%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-protobuf%E7%94%9F%E6%88%90%E6%AF%94%E5%8E%9F%E6%A0%B8%E5%BF%83%E4%BB%A3%E7%A0%81/]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bytom源码分析-Api-Server接口服务]]></title>
    <url>%2F2018%2F11%2F10%2Fbytom%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-p2p%E7%BD%91%E7%BB%9C-Api-Server%E6%8E%A5%E5%8F%A3%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[bytom源码分析-Api-Server接口服务简介https://github.com/Bytom/bytom 本章介绍bytom代码Api-Server接口服务 作者使用MacOS操作系统，其他平台也大同小异 Golang Version: 1.8 Api-Server接口服务Api Server是比原链中非常重要的一个功能，在比原链的架构中专门服务于bytomcli和dashboard，他的功能是接收并处理用户和矿池相关的请求。默认启动9888端口。总之主要功能如下： 接收并处理用户或矿池发送的请求 管理交易：打包、签名、提交等操作 管理本地比原钱包 管理本地p2p节点信息 管理本地矿工挖矿操作等 在Api Server服务过程中，在监听地址listener上接收bytomcli或dashboard的请求访问。对每一个请求，Api Server均会创建一个新的goroutine来处理请求。首先Api Server读取请求内容，解析请求，接着匹配相应的路由项，随后调用路由项的Handler回调函数来处理。最后Handler处理完请求之后给bytomcli响应该请求。 Api-Server源码分析在bytomd启动过程中，bytomd使用golang标准库http.NewServeMux()创建一个router路由器，提供请求的路由分发功能。创建Api Server主要有三部分组成： 初始化http.NewServeMux()得到mux 为mux.Handle添加多个有效的router路由项。每一个路由项由HTTP请求方法（GET、POST、PUT、DELET）、URL和Handler回调函数组成 将监听地址作为参数，最终执行Serve(listener)开始服务于外部请求 创建Api对象node/node.go 1234567func (n *Node) initAndstartApiServer() &#123; n.api = api.NewAPI(n.syncManager, n.wallet, n.txfeed, n.cpuMiner, n.miningPool, n.chain, n.config, n.accessTokens) listenAddr := env.String(&quot;LISTEN&quot;, n.config.ApiAddress) env.Parse() n.api.StartServer(*listenAddr)&#125; api/api.go 123456789101112131415func NewAPI(sync *netsync.SyncManager, wallet *wallet.Wallet, txfeeds *txfeed.Tracker, cpuMiner *cpuminer.CPUMiner, miningPool *miningpool.MiningPool, chain *protocol.Chain, config *cfg.Config, token *accesstoken.CredentialStore) *API &#123; api := &amp;API&#123; sync: sync, wallet: wallet, chain: chain, accessTokens: token, txFeedTracker: txfeeds, cpuMiner: cpuMiner, miningPool: miningPool, &#125; api.buildHandler() api.initServer(config) return api&#125; 首先，实例化api对象。Api-server管理的事情很多，所以参数也相对较多。listenAddr本地端口，如果系统没有设置LISTEN变量则使用config.ApiAddress配置地址，默认为9888 NewAPI函数我们看到有三个操作： 实例化api对象 api.buildHandler添加router路由项 api.initServer实例化http.Server，配置auth验证等 router路由项123456789101112func (a *API) buildHandler() &#123; walletEnable := false m := http.NewServeMux() if a.wallet != nil &#123; walletEnable = true m.Handle(&quot;/create-account&quot;, jsonHandler(a.createAccount)) m.Handle(&quot;/list-accounts&quot;, jsonHandler(a.listAccounts)) m.Handle(&quot;/delete-account&quot;, jsonHandler(a.deleteAccount)) // ... &#125;&#125; router路由项过多。这里只介绍关于账号相关的handler。其他的handler大同小异。 1m.Handle(&quot;/create-account&quot;, jsonHandler(a.createAccount)) 我们可以看到一条router项由url和对应的handle回调函数组成。当我们请求的url匹配到/create-account时，Api-Server会执行a.createAccount函数，并将用户的传参也带过去。 启动Api-Server服务api/api.go 12345678910111213func (a *API) StartServer(address string) &#123; log.WithField(&quot;api address:&quot;, address).Info(&quot;Rpc listen&quot;) listener, err := net.Listen(&quot;tcp&quot;, address) if err != nil &#123; cmn.Exit(cmn.Fmt(&quot;Failed to register tcp port: %v&quot;, err)) &#125; go func() &#123; if err := a.server.Serve(listener); err != nil &#123; log.WithField(&quot;error&quot;, errors.Wrap(err, &quot;Serve&quot;)).Error(&quot;Rpc server&quot;) &#125; &#125;()&#125; 通过golang标准库net.listen方法，监听本地的地址端口。由于http服务是一个持久运行的服务，我们启动一个go程专门运行http服务。当运行a.server.Serve没有任何报错时，我们可以看到服务器上启动的9888端口。此时Api-Server已经处于等待接收用户的请求。 转载链接：https://shanhuhai5739.github.io/2018/08/16/bytom%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-Api-Server%E6%8E%A5%E5%8F%A3%E6%9C%8D%E5%8A%A1/]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bytom源码分析-孤快管理]]></title>
    <url>%2F2018%2F11%2F10%2Fbytom%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-p2p%E7%BD%91%E7%BB%9C-%E5%AD%A4%E5%BF%AB%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[bytom源码分析-孤块管理简介https://github.com/Bytom/bytom 本章介绍bytom代码孤块管理 作者使用MacOS操作系统，其他平台也大同小异 Golang Version: 1.8 孤块介绍什么是孤块当节点收到了一个有效的区块，而在现有的主链中却未找到它的父区块，那么这个区块被认为是“孤块”。父区块是指当前区块的PreviousBlockHash字段指向上一区块的hash值。 接收到的孤块会被存储在孤块池中，直到它们的父区块被节点收到。一旦收到了父区块，节点就会将孤块从孤块池中取出，并且连接到它的父区块，让它作为区块链的一部分。 孤块出现的原因当两个或多个区块在很短的时间间隔内被挖出来，节点有可能会以不同的顺序接收到它们，这个时候孤块现象就会出现。 我们假设有三个高度分别为100、101、102的块，分别以102、101、100的颠倒顺序被节点接收。此时节点将102、101放入到孤块管理缓存池中，等待彼此的父块。当高度为100的区块被同步进来时，会被验证区块和交易，然后存储到区块链上。这时会对孤块缓存池进行递归查询，根据高度为100的区块找到101的区块并存储到区块链上，再根据高度为101的区块找到102的区块并存储到区块链上。 孤块源码分析孤块管理缓存池结构体protocol/orphan_manage.go 123456789101112type OrphanManage struct &#123; orphan map[bc.Hash]*types.Block prevOrphans map[bc.Hash][]*bc.Hash mtx sync.RWMutex&#125;func NewOrphanManage() *OrphanManage &#123; return &amp;OrphanManage&#123; orphan: make(map[bc.Hash]*types.Block), prevOrphans: make(map[bc.Hash][]*bc.Hash), &#125;&#125; orphan 存储孤块，key为block hash，value为block结构体 prevOrphans 存储孤块的父块 mtx 互斥锁，保护map结构在多并发读写状态下保持数据一致 添加孤块到缓存池1234567891011121314func (o *OrphanManage) Add(block *types.Block) &#123; blockHash := block.Hash() o.mtx.Lock() defer o.mtx.Unlock() if _, ok := o.orphan[blockHash]; ok &#123; return &#125; o.orphan[blockHash] = block o.prevOrphans[block.PreviousBlockHash] = append(o.prevOrphans[block.PreviousBlockHash], &amp;blockHash) log.WithFields(log.Fields&#123;&quot;hash&quot;: blockHash.String(), &quot;height&quot;: block.Height&#125;).Info(&quot;add block to orphan&quot;)&#125; 当一个孤块被添加到缓存池中，还需要记录该孤块的父块hash。用于父块hash的查询 查询孤块和父孤块12345678910111213func (o *OrphanManage) Get(hash *bc.Hash) (*types.Block, bool) &#123; o.mtx.RLock() block, ok := o.orphan[*hash] o.mtx.RUnlock() return block, ok&#125;func (o *OrphanManage) GetPrevOrphans(hash *bc.Hash) ([]*bc.Hash, bool) &#123; o.mtx.RLock() prevOrphans, ok := o.prevOrphans[*hash] o.mtx.RUnlock() return prevOrphans, ok&#125; 删除孤块12345678910111213141516171819202122func (o *OrphanManage) Delete(hash *bc.Hash) &#123; o.mtx.Lock() defer o.mtx.Unlock() block, ok := o.orphan[*hash] if !ok &#123; return &#125; delete(o.orphan, *hash) prevOrphans, ok := o.prevOrphans[block.PreviousBlockHash] if !ok || len(prevOrphans) == 1 &#123; delete(o.prevOrphans, block.PreviousBlockHash) return &#125; for i, preOrphan := range prevOrphans &#123; if preOrphan == hash &#123; o.prevOrphans[block.PreviousBlockHash] = append(prevOrphans[:i], prevOrphans[i+1:]...) return &#125; &#125;&#125; 删除孤块的过程中，同时删除父块 孤块处理逻辑protocol/block.go 12345678910111213141516171819func (c *Chain) processBlock(block *types.Block) (bool, error) &#123;blockHash := block.Hash() if c.BlockExist(&amp;blockHash) &#123; log.WithFields(log.Fields&#123;&quot;hash&quot;: blockHash.String(), &quot;height&quot;: block.Height&#125;).Info(&quot;block has been processed&quot;) return c.orphanManage.BlockExist(&amp;blockHash), nil &#125; if parent := c.index.GetNode(&amp;block.PreviousBlockHash); parent == nil &#123; c.orphanManage.Add(block) return true, nil &#125; if err := c.saveBlock(block); err != nil &#123; return false, err &#125; bestBlock := c.saveSubBlock(block) // ...&#125; processBlock函数处理block块加入区块链上之前的过程。 c.BlockExist判断当前block块是否存在于区块链上或是否存在孤块缓存池中，如果存在则返回。 c.index.GetNode判断block块的父节点是否存在。如果在现有的主链中却未找到它的父区块则将block块添加到孤块缓存池。 c.saveBlock走到了这一步说明，block父节点是存在于区块链，则将block块存储到区块链。该函数会验证区块和交易有效性。 saveSubBlock 代码如下： 12345678910111213141516171819202122232425func (c *Chain) saveSubBlock(block *types.Block) *types.Block &#123; blockHash := block.Hash() prevOrphans, ok := c.orphanManage.GetPrevOrphans(&amp;blockHash) if !ok &#123; return block &#125; bestBlock := block for _, prevOrphan := range prevOrphans &#123; orphanBlock, ok := c.orphanManage.Get(prevOrphan) if !ok &#123; log.WithFields(log.Fields&#123;&quot;hash&quot;: prevOrphan.String()&#125;).Warning(&quot;saveSubBlock fail to get block from orphanManage&quot;) continue &#125; if err := c.saveBlock(orphanBlock); err != nil &#123; log.WithFields(log.Fields&#123;&quot;hash&quot;: prevOrphan.String(), &quot;height&quot;: orphanBlock.Height&#125;).Warning(&quot;saveSubBlock fail to save block&quot;) continue &#125; if subBestBlock := c.saveSubBlock(orphanBlock); subBestBlock.Height &gt; bestBlock.Height &#123; bestBlock = subBestBlock &#125; &#125; return bestBlock&#125; saveSubBlock 在孤块缓存池中查询是否存在当前区块的下一个区块。比如当前区块高度为100，则在孤块缓存池中查询是否有区块高度为101的区块。如果存在则将101区块存储到区块链并从孤块缓存池中删除该区块。 saveSubBlock是一个递归函数的实现。目的是为了寻找最深叶子节点的递归方式。比如当前区块高度为100的，递归查询出高度为99、98、97等高度的区块。 转载链接：https://shanhuhai5739.github.io/2018/08/16/bytom%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-%E5%AD%A4%E5%84%BF%E5%9D%97%E7%AE%A1%E7%90%86/]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bytom源码分析-p2p网络-地址簙]]></title>
    <url>%2F2018%2F11%2F10%2Fbytom%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-p2p%E7%BD%91%E7%BB%9C-%E5%9C%B0%E5%9D%80%E7%B0%99%2F</url>
    <content type="text"><![CDATA[bytom源码分析-P2P网络-地址簿简介https://github.com/Bytom/bytom 本章介绍bytom代码P2P网络中addrbook地址簿 作者使用MacOS操作系统，其他平台也大同小异 Golang Version: 1.8 addrbook介绍addrbook用于存储P2P网络中保留最近的对端节点地址在MacOS下，默认的地址簿路径存储在~/Library/Bytom/addrbook.json 地址簿格式~/Library/Bytom/addrbook.json 1234567891011121314151617181920212223&#123; &quot;Key&quot;: &quot;359be6d08bc0c6e21c84bbb2&quot;, &quot;Addrs&quot;: [ &#123; &quot;Addr&quot;: &#123; &quot;IP&quot;: &quot;122.224.11.144&quot;, &quot;Port&quot;: 46657 &#125;, &quot;Src&quot;: &#123; &quot;IP&quot;: &quot;198.74.61.131&quot;, &quot;Port&quot;: 46657 &#125;, &quot;Attempts&quot;: 0, &quot;LastAttempt&quot;: &quot;2018-05-04T12:58:23.894057702+08:00&quot;, &quot;LastSuccess&quot;: &quot;0001-01-01T00:00:00Z&quot;, &quot;BucketType&quot;: 1, &quot;Buckets&quot;: [ 181, 10 ] &#125; ]&#125; 地址类型在addrbook中存储的地址有两种：p2p/addrbook.go 1234const ( bucketTypeNew = 0x01 // 标识新地址，不可靠地址（未成功连接过）。只存储在一个bucket中 bucketTypeOld = 0x02 // 标识旧地址，可靠地址（已成功连接过）。可以存储在多个bucket中，最多为maxNewBucketsPerAddress个) 注意: 一个地址的类型变更不在此文章中做介绍，后期的文章会讨论该问题 地址簿相关结构体地址簿 12345678910111213141516type AddrBook struct &#123; cmn.BaseService mtx sync.Mutex filePath string // 地址簿路径 routabilityStrict bool // 是否可路由，默认为true rand *rand.Rand key string // 地址簿标识，用于计算addrNew和addrOld的索引 ourAddrs map[string]*NetAddress // 存储本地网络地址，用于添加p2p地址时做排除使用 addrLookup map[string]*knownAddress // 存储新、旧地址集，用于查询 addrNew []map[string]*knownAddress // 存储新地址 addrOld []map[string]*knownAddress // 存储旧地址 wg sync.WaitGroup nOld int // 旧地址数量 nNew int // 新地址数量&#125; 已知地址 123456789type knownAddress struct &#123; Addr *NetAddress // 已知peer的addr Src *NetAddress // 已知peer的addr的来源addr Attempts int32 // 连接peer的重试次数 LastAttempt time.Time // 最近一次尝试连接的时间 LastSuccess time.Time // 最近一次尝试成功连接的时间 BucketType byte // 地址的类型（表示可靠地址或不可靠地址） Buckets []int // 当前addr所属的buckets&#125; routabilityStrict参数表示地址簿是否存储的ip是否可路由。可路由是根据RFC划分，具体参考资料：RFC标准 初始化地址簿123456789101112131415161718192021222324252627282930// NewAddrBook creates a new address book.// Use Start to begin processing asynchronous address updates.func NewAddrBook(filePath string, routabilityStrict bool) *AddrBook &#123; am := &amp;AddrBook&#123; rand: rand.New(rand.NewSource(time.Now().UnixNano())), ourAddrs: make(map[string]*NetAddress), addrLookup: make(map[string]*knownAddress), filePath: filePath, routabilityStrict: routabilityStrict, &#125; am.init() am.BaseService = *cmn.NewBaseService(nil, &quot;AddrBook&quot;, am) return am&#125;// When modifying this, don&apos;t forget to update loadFromFile()func (a *AddrBook) init() &#123; // 地址簿唯一标识 a.key = crypto.CRandHex(24) // 24/2 * 8 = 96 bits // New addr buckets， 默认为256个大小 a.addrNew = make([]map[string]*knownAddress, newBucketCount) for i := range a.addrNew &#123; a.addrNew[i] = make(map[string]*knownAddress) &#125; // Old addr buckets，默认为64个大小 a.addrOld = make([]map[string]*knownAddress, oldBucketCount) for i := range a.addrOld &#123; a.addrOld[i] = make(map[string]*knownAddress) &#125;&#125; bytomd启动时加载本地地址簿loadFromFile在bytomd启动时，首先会加载本地的地址簿 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// OnStart implements Service.func (a *AddrBook) OnStart() error &#123; a.BaseService.OnStart() a.loadFromFile(a.filePath) a.wg.Add(1) go a.saveRoutine() return nil&#125;// Returns false if file does not exist.// cmn.Panics if file is corrupt.func (a *AddrBook) loadFromFile(filePath string) bool &#123; // If doesn&apos;t exist, do nothing. // 如果本地地址簿不存在则直接返回 _, err := os.Stat(filePath) if os.IsNotExist(err) &#123; return false &#125; // 加载地址簿json内容 // Load addrBookJSON&#123;&#125; r, err := os.Open(filePath) if err != nil &#123; cmn.PanicCrisis(cmn.Fmt(&quot;Error opening file %s: %v&quot;, filePath, err)) &#125; defer r.Close() aJSON := &amp;addrBookJSON&#123;&#125; dec := json.NewDecoder(r) err = dec.Decode(aJSON) if err != nil &#123; cmn.PanicCrisis(cmn.Fmt(&quot;Error reading file %s: %v&quot;, filePath, err)) &#125; // 填充addrNew、addrOld等 // Restore all the fields... // Restore the key a.key = aJSON.Key // Restore .addrNew &amp; .addrOld for _, ka := range aJSON.Addrs &#123; for _, bucketIndex := range ka.Buckets &#123; bucket := a.getBucket(ka.BucketType, bucketIndex) bucket[ka.Addr.String()] = ka &#125; a.addrLookup[ka.Addr.String()] = ka if ka.BucketType == bucketTypeNew &#123; a.nNew++ &#125; else &#123; a.nOld++ &#125; &#125; return true&#125; 定时更新地址簿bytomd会定时更新本地地址簿，默认2分钟一次 12345678910111213141516171819202122232425262728293031323334353637383940414243444546func (a *AddrBook) saveRoutine() &#123; dumpAddressTicker := time.NewTicker(dumpAddressInterval)out: for &#123; select &#123; case &lt;-dumpAddressTicker.C: a.saveToFile(a.filePath) case &lt;-a.Quit: break out &#125; &#125; dumpAddressTicker.Stop() a.saveToFile(a.filePath) a.wg.Done() log.Info(&quot;Address handler done&quot;)&#125;func (a *AddrBook) saveToFile(filePath string) &#123; log.WithField(&quot;size&quot;, a.Size()).Info(&quot;Saving AddrBook to file&quot;) a.mtx.Lock() defer a.mtx.Unlock() // Compile Addrs addrs := []*knownAddress&#123;&#125; for _, ka := range a.addrLookup &#123; addrs = append(addrs, ka) &#125; aJSON := &amp;addrBookJSON&#123; Key: a.key, Addrs: addrs, &#125; jsonBytes, err := json.MarshalIndent(aJSON, &quot;&quot;, &quot;\t&quot;) if err != nil &#123; log.WithField(&quot;err&quot;, err).Error(&quot;Failed to save AddrBook to file&quot;) return &#125; err = cmn.WriteFileAtomic(filePath, jsonBytes, 0644) if err != nil &#123; log.WithFields(log.Fields&#123; &quot;file&quot;: filePath, &quot;err&quot;: err, &#125;).Error(&quot;Failed to save AddrBook to file&quot;) &#125;&#125; 添加新地址当peer之间交换addr时，节点会收到对端节点已知的地址信息，这些信息会被当前节点添加到地址簿中 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152func (a *AddrBook) AddAddress(addr *NetAddress, src *NetAddress) &#123; a.mtx.Lock() defer a.mtx.Unlock() log.WithFields(log.Fields&#123; &quot;addr&quot;: addr, &quot;src&quot;: src, &#125;).Debug(&quot;Add address to book&quot;) a.addAddress(addr, src)&#125;func (a *AddrBook) addAddress(addr, src *NetAddress) &#123; // 验证地址是否为可路由地址 if a.routabilityStrict &amp;&amp; !addr.Routable() &#123; log.Error(cmn.Fmt(&quot;Cannot add non-routable address %v&quot;, addr)) return &#125; // 验证地址是否为本地节点地址 if _, ok := a.ourAddrs[addr.String()]; ok &#123; // Ignore our own listener address. return &#125; // 验证地址是否存在地址集中 // 如果存在：则判断该地址是否为old可靠地址、是否超过了最大buckets中。否则根据该地址已经被ka.Buckets引用的个数来随机决定是否添加到地址集中 // 如果不存在：则添加到地址集中。并标识为bucketTypeNew地址类型 ka := a.addrLookup[addr.String()] if ka != nil &#123; // Already old. if ka.isOld() &#123; return &#125; // Already in max new buckets. if len(ka.Buckets) == maxNewBucketsPerAddress &#123; return &#125; // The more entries we have, the less likely we are to add more. factor := int32(2 * len(ka.Buckets)) if a.rand.Int31n(factor) != 0 &#123; return &#125; &#125; else &#123; ka = newKnownAddress(addr, src) &#125; // 找到该地址在地址集的索引位置并添加 bucket := a.calcNewBucket(addr, src) a.addToNewBucket(ka, bucket) log.Info(&quot;Added new address &quot;, &quot;address:&quot;, addr, &quot; total:&quot;, a.size())&#125; 选择最优节点地址簿中存储众多地址，在p2p网络中需选择最优的地址去连接PickAddress(newBias int)函数中newBias是由pex_reactor产生的地址评分。如何计算地址分数在其他章节中再讲根据地址评分随机选择地址可增加区块链安全性 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364// Pick an address to connect to with new/old bias.func (a *AddrBook) PickAddress(newBias int) *NetAddress &#123; a.mtx.Lock() defer a.mtx.Unlock() if a.size() == 0 &#123; return nil &#125; // newBias地址分数限制在0-100分数之间 if newBias &gt; 100 &#123; newBias = 100 &#125; if newBias &lt; 0 &#123; newBias = 0 &#125; // Bias between new and old addresses. oldCorrelation := math.Sqrt(float64(a.nOld)) * (100.0 - float64(newBias)) newCorrelation := math.Sqrt(float64(a.nNew)) * float64(newBias) // 根据地址分数计算是否从addrOld或addrNew中随机选择一个地址 if (newCorrelation+oldCorrelation)*a.rand.Float64() &lt; oldCorrelation &#123; // pick random Old bucket. var bucket map[string]*knownAddress = nil num := 0 for len(bucket) == 0 &amp;&amp; num &lt; oldBucketCount &#123; bucket = a.addrOld[a.rand.Intn(len(a.addrOld))] num++ &#125; if num == oldBucketCount &#123; return nil &#125; // pick a random ka from bucket. randIndex := a.rand.Intn(len(bucket)) for _, ka := range bucket &#123; if randIndex == 0 &#123; return ka.Addr &#125; randIndex-- &#125; cmn.PanicSanity(&quot;Should not happen&quot;) &#125; else &#123; // pick random New bucket. var bucket map[string]*knownAddress = nil num := 0 for len(bucket) == 0 &amp;&amp; num &lt; newBucketCount &#123; bucket = a.addrNew[a.rand.Intn(len(a.addrNew))] num++ &#125; if num == newBucketCount &#123; return nil &#125; // pick a random ka from bucket. randIndex := a.rand.Intn(len(bucket)) for _, ka := range bucket &#123; if randIndex == 0 &#123; return ka.Addr &#125; randIndex-- &#125; cmn.PanicSanity(&quot;Should not happen&quot;) &#125; return nil&#125; 移除一个地址当一个地址被标记为Bad时则从地址集中移除。目前bytomd的代码版本并未调用过 1234567891011121314151617181920212223242526272829func (a *AddrBook) MarkBad(addr *NetAddress) &#123; a.RemoveAddress(addr)&#125;// RemoveAddress removes the address from the book.func (a *AddrBook) RemoveAddress(addr *NetAddress) &#123; a.mtx.Lock() defer a.mtx.Unlock() ka := a.addrLookup[addr.String()] if ka == nil &#123; return &#125; log.WithField(&quot;addr&quot;, addr).Info(&quot;Remove address from book&quot;) a.removeFromAllBuckets(ka)&#125;func (a *AddrBook) removeFromAllBuckets(ka *knownAddress) &#123; for _, bucketIdx := range ka.Buckets &#123; bucket := a.getBucket(ka.BucketType, bucketIdx) delete(bucket, ka.Addr.String()) &#125; ka.Buckets = nil if ka.BucketType == bucketTypeNew &#123; a.nNew-- &#125; else &#123; a.nOld-- &#125; delete(a.addrLookup, ka.Addr.String())&#125; 转载链接：https://shanhuhai5739.github.io/2018/04/28/bytom%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-P2P%E7%BD%91%E7%BB%9C-%E5%9C%B0%E5%9D%80%E7%B0%BF/]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bytom源码分析-P2P网络-upnp端口映射]]></title>
    <url>%2F2018%2F11%2F10%2Fbytom%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-p2p%E7%BD%91%E7%BB%9C-upnp%E7%AB%AF%E5%8F%A3%E6%98%A0%E5%B0%84%2F</url>
    <content type="text"><![CDATA[bytom源码分析-P2P网络-upnp端口映射简介https://github.com/Bytom/bytom 本章介绍bytom代码P2P网络中upnp端口映射 作者使用MacOS操作系统，其他平台也大同小异 Golang Version: 1.8 UPNP介绍UPNP（Universal Plug and Play）通用即插即用。UPNP端口映射将一个外部端口映射到一个内网ip:port。从而实现p2p网络从外网能够穿透网关访问到内网的bytomd节点。 UPNP协议SSDP（Simple Service Discovery Protocol 简单服务发现协议）GENA（Generic Event Notification Architecture 通用事件通知结构）SOAP（Simple Object Access Protocol 简单对象访问协议）XML（Extensible Markup Language 可扩张标记语言） UPNP代码p2p/upnp/upnp.go 发现网络中支持UPNP功能的设备从网络中发现支持UPNP功能的设备，并得到该设备的location和url等相关信息 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485type upnpNAT struct &#123; serviceURL string // 设备的描述文件URL，用于得到该设备的描述信息 ourIP string // 节点本地ip地址 urnDomain string // 设备类型&#125;func Discover() (nat NAT, err error) &#123; ssdp, err := net.ResolveUDPAddr(&quot;udp4&quot;, &quot;239.255.255.250:1900&quot;) if err != nil &#123; return &#125; conn, err := net.ListenPacket(&quot;udp4&quot;, &quot;:0&quot;) if err != nil &#123; return &#125; socket := conn.(*net.UDPConn) defer socket.Close() err = socket.SetDeadline(time.Now().Add(3 * time.Second)) if err != nil &#123; return &#125; st := &quot;InternetGatewayDevice:1&quot; // 多播请求：M-SEARCH SSDP协议定义的发现请求。 buf := bytes.NewBufferString( &quot;M-SEARCH * HTTP/1.1\r\n&quot; + &quot;HOST: 239.255.255.250:1900\r\n&quot; + &quot;ST: ssdp:all\r\n&quot; + &quot;MAN: \&quot;ssdp:discover\&quot;\r\n&quot; + &quot;MX: 2\r\n\r\n&quot;) message := buf.Bytes() answerBytes := make([]byte, 1024) for i := 0; i &lt; 3; i++ &#123; // 向239.255.255.250:1900发送一条多播请求 _, err = socket.WriteToUDP(message, ssdp) if err != nil &#123; return &#125; // 如果从网络中发现UPNP设备则会从239.255.255.250:1900收到响应消息 var n int n, _, err = socket.ReadFromUDP(answerBytes) for &#123; n, _, err = socket.ReadFromUDP(answerBytes) if err != nil &#123; break &#125; answer := string(answerBytes[0:n]) if strings.Index(answer, st) &lt; 0 &#123; continue &#125; // HTTP header field names are case-insensitive. // http://www.w3.org/Protocols/rfc2616/rfc2616-sec4.html#sec4.2 // 获得设备location locString := &quot;\r\nlocation:&quot; answer = strings.ToLower(answer) locIndex := strings.Index(answer, locString) if locIndex &lt; 0 &#123; continue &#125; loc := answer[locIndex+len(locString):] endIndex := strings.Index(loc, &quot;\r\n&quot;) if endIndex &lt; 0 &#123; continue &#125; // 获得设备的描述url和设备类型 locURL := strings.TrimSpace(loc[0:endIndex]) var serviceURL, urnDomain string serviceURL, urnDomain, err = getServiceURL(locURL) if err != nil &#123; return &#125; var ourIP net.IP ourIP, err = localIPv4() if err != nil &#123; return &#125; nat = &amp;upnpNAT&#123;serviceURL: serviceURL, ourIP: ourIP.String(), urnDomain: urnDomain&#125; return &#125; &#125; err = errors.New(&quot;UPnP port discovery failed.&quot;) return&#125; 添加端口映射向upnp设备发送一条http post请求，将内部网络ip:port和外部网络ip:port做映射 123456789101112131415161718192021222324252627282930func (n *upnpNAT) AddPortMapping(protocol string, externalPort, internalPort int, description string, timeout int) (mappedExternalPort int, err error) &#123; // A single concatenation would break ARM compilation. message := &quot;&lt;u:AddPortMapping xmlns:u=\&quot;urn:&quot; + n.urnDomain + &quot;:service:WANIPConnection:1\&quot;&gt;\r\n&quot; + &quot;&lt;NewRemoteHost&gt;&lt;/NewRemoteHost&gt;&lt;NewExternalPort&gt;&quot; + strconv.Itoa(externalPort) message += &quot;&lt;/NewExternalPort&gt;&lt;NewProtocol&gt;&quot; + protocol + &quot;&lt;/NewProtocol&gt;&quot; message += &quot;&lt;NewInternalPort&gt;&quot; + strconv.Itoa(internalPort) + &quot;&lt;/NewInternalPort&gt;&quot; + &quot;&lt;NewInternalClient&gt;&quot; + n.ourIP + &quot;&lt;/NewInternalClient&gt;&quot; + &quot;&lt;NewEnabled&gt;1&lt;/NewEnabled&gt;&lt;NewPortMappingDescription&gt;&quot; message += description + &quot;&lt;/NewPortMappingDescription&gt;&lt;NewLeaseDuration&gt;&quot; + strconv.Itoa(timeout) + &quot;&lt;/NewLeaseDuration&gt;&lt;/u:AddPortMapping&gt;&quot; var response *http.Response response, err = soapRequest(n.serviceURL, &quot;AddPortMapping&quot;, message, n.urnDomain) if response != nil &#123; defer response.Body.Close() &#125; if err != nil &#123; return &#125; // TODO: check response to see if the port was forwarded // log.Println(message, response) // JAE: // body, err := ioutil.ReadAll(response.Body) // fmt.Println(string(body), err) mappedExternalPort = externalPort _ = response return&#125; 删除端口映射向upnp设备发送一条http post请求，将内部网络ip:port和外部网络ip:port删除映射关系 123456789101112131415161718192021func (n *upnpNAT) DeletePortMapping(protocol string, externalPort, internalPort int) (err error) &#123; message := &quot;&lt;u:DeletePortMapping xmlns:u=\&quot;urn:&quot; + n.urnDomain + &quot;:service:WANIPConnection:1\&quot;&gt;\r\n&quot; + &quot;&lt;NewRemoteHost&gt;&lt;/NewRemoteHost&gt;&lt;NewExternalPort&gt;&quot; + strconv.Itoa(externalPort) + &quot;&lt;/NewExternalPort&gt;&lt;NewProtocol&gt;&quot; + protocol + &quot;&lt;/NewProtocol&gt;&quot; + &quot;&lt;/u:DeletePortMapping&gt;&quot; var response *http.Response response, err = soapRequest(n.serviceURL, &quot;DeletePortMapping&quot;, message, n.urnDomain) if response != nil &#123; defer response.Body.Close() &#125; if err != nil &#123; return &#125; // TODO: check response to see if the port was deleted // log.Println(message, response) _ = response return&#125; 获取映射后的公网地址1234567891011121314151617181920212223242526272829303132333435func (n *upnpNAT) GetExternalAddress() (addr net.IP, err error) &#123; info, err := n.getExternalIPAddress() if err != nil &#123; return &#125; addr = net.ParseIP(info.externalIpAddress) return&#125;func (n *upnpNAT) getExternalIPAddress() (info statusInfo, err error) &#123; message := &quot;&lt;u:GetExternalIPAddress xmlns:u=\&quot;urn:&quot; + n.urnDomain + &quot;:service:WANIPConnection:1\&quot;&gt;\r\n&quot; + &quot;&lt;/u:GetExternalIPAddress&gt;&quot; var response *http.Response response, err = soapRequest(n.serviceURL, &quot;GetExternalIPAddress&quot;, message, n.urnDomain) if response != nil &#123; defer response.Body.Close() &#125; if err != nil &#123; return &#125; var envelope Envelope data, err := ioutil.ReadAll(response.Body) reader := bytes.NewReader(data) xml.NewDecoder(reader).Decode(&amp;envelope) info = statusInfo&#123;envelope.Soap.ExternalIP.IPAddress&#125; if err != nil &#123; return &#125; return&#125; 转载链接：https://shanhuhai5739.github.io/2018/04/28/bytom%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-P2P%E7%BD%91%E7%BB%9C-upnp%E7%AB%AF%E5%8F%A3%E6%98%A0%E5%B0%84/]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bytom源码分析-启动与停止]]></title>
    <url>%2F2018%2F11%2F10%2Fbytom%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-%E5%90%AF%E5%8A%A8%E4%B8%8E%E5%81%9C%E6%AD%A2%2F</url>
    <content type="text"><![CDATA[bytom源码分析-启动与停止简介https://github.com/Bytom/bytom本章介绍bytom代码启动、节点初始化、及停止的过程 作者使用MacOS操作系统，其他平台也大同小异Golang Version: 1.8 预备工作编译安装详细步骤见官方 bytom install 设置debug日志输出开启debug输出文件、函数、行号等详细信息 1export BYTOM_DEBUG=debug 初始化并启动bytomd初始化 1./bytomd init --chain_id testnet bytomd目前支持两种网络，这里我们使用测试网mainnet：主网testnet：测试网 启动bytomd 1./bytomd node --mining --prof_laddr=&quot;:8011&quot; –prof_laddr=”:8080” // 开启pprof输出性能指标访问：http://127.0.0.1:8080/debug/pprof/ bytomd init初始化入口函数cmd/bytomd/main.go 123456789101112131415func init() &#123; log.SetFormatter(&amp;log.TextFormatter&#123;FullTimestamp: true, DisableColors: true&#125;) // If environment variable BYTOM_DEBUG is not empty, // then add the hook to logrus and set the log level to DEBUG if os.Getenv(&quot;BYTOM_DEBUG&quot;) != &quot;&quot; &#123; log.AddHook(ContextHook&#123;&#125;) log.SetLevel(log.DebugLevel) &#125;&#125;func main() &#123; cmd := cli.PrepareBaseCmd(commands.RootCmd, &quot;TM&quot;, os.ExpandEnv(config.DefaultDataDir())) cmd.Execute()&#125; init函数会在main执行之前做初始化操作，可以看到init中bytomd加载BYTOM_DEBUG变量来设置debug日志输出 command cli传参初始化bytomd的cli解析使用cobra库 cmd/bytomd/commands cmd/bytomd/commands/root.go初始化–root传参。bytomd存储配置、keystore、数据的root目录。在MacOS下，默认路径是~/Library/Bytom/ cmd/bytomd/commands/init.go初始化–chain_id传参。选择网络类型，在启动bytomd时我们选择了testnet也就是测试网络 cmd/bytomd/commands/version.go初始化version传参 cmd/bytomd/commands/run_node.go初始化node节点运行时所需要的传参 初始化默认配置用户传参只有一部分参数，那节点所需的其他参数需要从默认配置中加载。cmd/bytomd/commands/root.go 123var ( config = cfg.DefaultConfig()) 在root.go中有一个config全局变量加载了node所需的所有默认参数 12345678910// Default configurable parameters.func DefaultConfig() *Config &#123; return &amp;Config&#123; BaseConfig: DefaultBaseConfig(), // node基础相关配置 P2P: DefaultP2PConfig(), // p2p网络相关配置 Wallet: DefaultWalletConfig(), // 钱包相关配置 Auth: DefaultRPCAuthConfig(), // 验证相关配置 Web: DefaultWebConfig(), // web相关配置 &#125;&#125; 后面的文章会一一介绍每个配置的作用 bytomd 守护进程启动与退出cmd/bytomd/commands/run_node.go 1234567891011121314func runNode(cmd *cobra.Command, args []string) error &#123; // Create &amp; start node n := node.NewNode(config) if _, err := n.Start(); err != nil &#123; return fmt.Errorf(&quot;Failed to start node: %v&quot;, err) &#125; else &#123; log.WithField(&quot;nodeInfo&quot;, n.SyncManager().Switch().NodeInfo()).Info(&quot;Started node&quot;) &#125; // Trap signal, run forever. n.RunForever() return nil&#125; runNode函数有三步操作：node.NewNode：初始化node运行环境n.Start：启动noden.RunForever：监听退出信号，收到ctrl+c操作则退出node。在linux中守进程一般监听SIGTERM信号(ctrl+c)作为退出守护进程的信号 初始化node运行环境在bytomd中有五个db数据库存储在–root参数下的data目录 accesstoken.db // 存储token相关信息(钱包访问控制权限) trusthistory.db // 存储p2p网络同步相关信息 txdb.db // 存储交易相关信息 txfeeds.db // wallet.db // 存储钱包相关信息 node/node.go 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495func NewNode(config *cfg.Config) *Node &#123; ctx := context.Background() initActiveNetParams(config) // Get store 初始化txdb数据库 txDB := dbm.NewDB(&quot;txdb&quot;, config.DBBackend, config.DBDir()) store := leveldb.NewStore(txDB) // 初始化accesstoken数据库 tokenDB := dbm.NewDB(&quot;accesstoken&quot;, config.DBBackend, config.DBDir()) accessTokens := accesstoken.NewStore(tokenDB) // 初始化event事件调度器，也叫任务调度器。一个任务可以被多次调用 // Make event switch eventSwitch := types.NewEventSwitch() _, err := eventSwitch.Start() if err != nil &#123; cmn.Exit(cmn.Fmt(&quot;Failed to start switch: %v&quot;, err)) &#125; // 初始化交易池 txPool := protocol.NewTxPool() chain, err := protocol.NewChain(store, txPool) if err != nil &#123; cmn.Exit(cmn.Fmt(&quot;Failed to create chain structure: %v&quot;, err)) &#125; var accounts *account.Manager = nil var assets *asset.Registry = nil var wallet *w.Wallet = nil var txFeed *txfeed.Tracker = nil // 初始化txfeeds数据库 txFeedDB := dbm.NewDB(&quot;txfeeds&quot;, config.DBBackend, config.DBDir()) txFeed = txfeed.NewTracker(txFeedDB, chain) if err = txFeed.Prepare(ctx); err != nil &#123; log.WithField(&quot;error&quot;, err).Error(&quot;start txfeed&quot;) return nil &#125; // 初始化keystore hsm, err := pseudohsm.New(config.KeysDir()) if err != nil &#123; cmn.Exit(cmn.Fmt(&quot;initialize HSM failed: %v&quot;, err)) &#125; // 初始化钱包，默认wallet是开启状态 if !config.Wallet.Disable &#123; walletDB := dbm.NewDB(&quot;wallet&quot;, config.DBBackend, config.DBDir()) accounts = account.NewManager(walletDB, chain) assets = asset.NewRegistry(walletDB, chain) wallet, err = w.NewWallet(walletDB, accounts, assets, hsm, chain) if err != nil &#123; log.WithField(&quot;error&quot;, err).Error(&quot;init NewWallet&quot;) &#125; // Clean up expired UTXO reservations periodically. go accounts.ExpireReservations(ctx, expireReservationsPeriod) &#125; newBlockCh := make(chan *bc.Hash, maxNewBlockChSize) // 初始化网络节点同步管理 syncManager, _ := netsync.NewSyncManager(config, chain, txPool, newBlockCh) // 初始化pprof，pprof用于输出性能指标，需要制定--prof_laddr参数来开启，在文章开头我们已经开启该功能 // run the profile server profileHost := config.ProfListenAddress if profileHost != &quot;&quot; &#123; // Profiling bytomd programs.see (https://blog.golang.org/profiling-go-programs) // go tool pprof http://profileHose/debug/pprof/heap go func() &#123; http.ListenAndServe(profileHost, nil) &#125;() &#125; // 初始化节点，填充节点所需的所有参数环境 node := &amp;Node&#123; config: config, syncManager: syncManager, evsw: eventSwitch, accessTokens: accessTokens, wallet: wallet, chain: chain, txfeed: txFeed, miningEnable: config.Mining, &#125; // 初始化挖矿 node.cpuMiner = cpuminer.NewCPUMiner(chain, accounts, txPool, newBlockCh) node.miningPool = miningpool.NewMiningPool(chain, accounts, txPool, newBlockCh) node.BaseService = *cmn.NewBaseService(nil, &quot;Node&quot;, node) return node&#125; 目前bytomd只支持cpu挖矿，所以在代码中只有cpuminer的初始化信息 启动nodenode/node.go 1234567891011121314151617181920212223242526272829// Lanch web broser or notfunc lanchWebBroser() &#123; log.Info(&quot;Launching System Browser with :&quot;, webAddress) if err := browser.Open(webAddress); err != nil &#123; log.Error(err.Error()) return &#125;&#125;func (n *Node) initAndstartApiServer() &#123; n.api = api.NewAPI(n.syncManager, n.wallet, n.txfeed, n.cpuMiner, n.miningPool, n.chain, n.config, n.accessTokens) listenAddr := env.String(&quot;LISTEN&quot;, n.config.ApiAddress) env.Parse() n.api.StartServer(*listenAddr)&#125;func (n *Node) OnStart() error &#123; if n.miningEnable &#123; n.cpuMiner.Start() &#125; n.syncManager.Start() n.initAndstartApiServer() if !n.config.Web.Closed &#123; lanchWebBroser() &#125; return nil&#125; OnStart() 启动node进程如下： 启动挖矿功能 启动p2p网络同步 启动http协议的apiserver服务 打开浏览器访问bytond的交易页面 停止nodebytomd在启动时执行了n.RunForever()函数，该函数是由tendermint框架启动了监听信号的功能：vendor/github.com/tendermint/tmlibs/common/os.go 1234567891011121314func TrapSignal(cb func()) &#123; c := make(chan os.Signal, 1) signal.Notify(c, os.Interrupt, syscall.SIGTERM) go func() &#123; for sig := range c &#123; fmt.Printf(&quot;captured %v, exiting...\n&quot;, sig) if cb != nil &#123; cb() &#125; os.Exit(1) &#125; &#125;() select &#123;&#125;&#125; TrapSignal函数监听了SIGTERM信号，bytomd才能成为不退出的守护进程。只有当触发了ctrl+c或kill bytomd_pid才能终止bytomd进程退出。退出时bytomd执行如下操作node/node.go 123456789func (n *Node) OnStop() &#123; n.BaseService.OnStop() if n.miningEnable &#123; n.cpuMiner.Stop() &#125; n.syncManager.Stop() log.Info(&quot;Stopping Node&quot;) // TODO: gracefully disconnect from peers.&#125; bytomd会将挖矿功能停止，p2p网络停止等操作。 转载链接：https://shanhuhai5739.github.io/2018/04/21/bytom%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-%E5%90%AF%E5%8A%A8%E4%B8%8E%E5%81%9C%E6%AD%A2/]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GO语言希尔排序]]></title>
    <url>%2F2018%2F11%2F07%2FGO%E8%AF%AD%E8%A8%80%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[希尔排序希尔排序，也称递减增量排序算法，是插入排序的一种高速而稳定的改进版本。 希尔排序是基于插入排序的以下两点性质而提出改进方法的： 1、插入排序在对几乎已经排好序的数据操作时， 效率高， 即可以达到线性排序的效率 2、但插入排序一般来说是低效的， 因为插入排序每次只能将数据移动一位&gt; 12345678910111213141516171819202122232425262728293031323334353637func shellshort(items []int) &#123; var ( n = len(items) gaps = []int&#123;1&#125; k = 1 ) for &#123; gap := pow(2, k) + 1 if gap &gt; n-1 &#123; break &#125; gaps = append([]int&#123;gap&#125;, gaps...) k++ &#125; for _, gap := range gaps &#123; for i := gap; i &lt; n; i += gap &#123; j := i for j &gt; 0 &#123; if items[j-gap] &gt; items[j] &#123; items[j-gap], items[j] = items[j], items[j-gap] &#125; j = j - gap &#125; &#125; &#125;&#125;func pow(a, b int) int &#123; p := 1 for b &gt; 0 &#123; if b&amp;1 != 0 &#123; p *= a &#125; b &gt;&gt;= 1 a *= a &#125; return p&#125;]]></content>
      <categories>
        <category>数据结构和算法</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>排序问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GO语言归并排序]]></title>
    <url>%2F2018%2F11%2F07%2FGO%E8%AF%AD%E8%A8%80%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[归并排序 归并排序（Merge sort，台湾译作：合并排序）是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用 1234567891011121314151617181920212223242526272829303132333435363738394041424344func mergeSort(items []int) []int &#123; var n = len(items) if n == 1 &#123; return items &#125; middle := int(n / 2) var ( left = make([]int, middle) right = make([]int, n-middle) ) for i := 0; i &lt; n; i++ &#123; if i &lt; middle &#123; left[i] = items[i] &#125; else &#123; right[i-middle] = items[i] &#125; &#125; return merge(mergeSort(left), mergeSort(right))&#125;func merge(left, right []int) (result []int) &#123; result = make([]int, len(left)+len(right)) i := 0 for len(left) &gt; 0 &amp;&amp; len(right) &gt; 0 &#123; if left[0] &lt; right[0] &#123; result[i] = left[0] left = left[1:] &#125; else &#123; result[i] = right[0] right = right[1:] &#125; i++ &#125; // Either left or right may have elements left; consume them. // (Only one of the following loops will actually be entered.) for j := 0; j &lt; len(left); j++ &#123; result[i] = left[j] i++ &#125; for j := 0; j &lt; len(right); j++ &#123; result[i] = right[j] i++ &#125; return&#125;]]></content>
      <categories>
        <category>数据结构和算法</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>排序问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GO语言快速排序]]></title>
    <url>%2F2018%2F11%2F07%2FGO%E8%AF%AD%E8%A8%80%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[快速排序是由东尼·霍尔所发展的一种排序算法。在平均状况下，排序 n 个项目要Ο(n log n)次比较。在最坏状况下则需要Ο(n2)次比较，但这种状况并不常见。事实上，快速排序通常明显比其他Ο(n log n) 算法更快，因为它的内部循环（inner loop）可以在大部分的架构上很有效率地被实现出来，且在大部分真实世界的数据，可以决定设计的选择，减少所需时间的二次方项之可能性。 123456789101112131415161718192021222324252627282930313233343536func QuickSort(src []int, first, last int) &#123; flag := first left := first right := last if first &gt;= last &#123; return &#125; for first &lt; last &#123; for first &lt; last &#123; if src[last] &gt;= src[flag] &#123; last -= 1 continue &#125; else &#123; tmp := src[last] src[last] = src[flag] src[flag] = tmp flag = last break &#125; &#125; for first &lt; last &#123; if src[first] &lt;= src[flag] &#123; first += 1 continue &#125; else &#123; tmp := src[first] src[first] = src[flag] src[flag] = tmp flag = first break &#125; &#125; &#125; QuickSort(src, left, flag-1) QuickSort(src, flag+1, right)&#125;]]></content>
      <categories>
        <category>数据结构和算法</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>排序问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GO语言选择排序]]></title>
    <url>%2F2018%2F11%2F07%2FGO%E8%AF%AD%E8%A8%80%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[选择排序选择排序(Selection sort)是一种简单直观的排序算法。它的工作原理为：首先在未排序序列中找到最小元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小元素，然后放到排序序列末尾。以此类推，直到所有元素均排序完毕。 123456789101112func selectionSort(items []int) &#123; var n = len(items) for i := 0; i &lt; n; i++ &#123; var minIdx = i for j := i; j &lt; n; j++ &#123; if items[j] &lt; items[minIdx] &#123; minIdx = j &#125; &#125; items[i], items[minIdx] = items[minIdx], items[i] &#125;&#125; 使用sort包： 123456789101112func selectionSortUsingSortPackage(data sort.Interface) &#123; r := data.Len() - 1 for i := 0; i &lt; r; i++ &#123; min := i for j := i + 1; j &lt;= r; j++ &#123; if data.Less(j, min) &#123; min = j &#125; &#125; data.Swap(i, min) &#125;&#125;]]></content>
      <categories>
        <category>数据结构和算法</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>排序问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[非对称加密RSA详解]]></title>
    <url>%2F2018%2F11%2F03%2F%E9%9D%9E%E5%AF%B9%E7%A7%B0%E5%8A%A0%E5%AF%86RSA%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[非对称加密通信流程下面我们来看一看使用公钥密码的通信流程。和以前一样、我们还是假设Alice要给Bob发送一条消息，Alice是发送者，Bob是接收者，而这一次窃听者Eve依然能够窃所到他们之间的通信内容。 在公非对称加密通信中，通信过程是由接收者Bob来启动的。 Bob生成一个包含公钥和私钥的密钥对。 私钥由Bob自行妥善保管。 Bob将自己的公钥发送给Alice Bob的公钥被窃听者Eve截获也没关系。 将公钥发送给Alice，表示Bob请Alice用这个公钥对消息进行加密并发送给他。 Alice用Bob的公钥对消息进行加密。 加密后的消息只有用Bob的私钥才能够解密。 虽然Alice拥有Bob的公钥，但用Bob的公钥是无法对密文进行解密的。 Alice将密文发送给Bobo 密文被窃听者Eve截获也没关系。Eve可能拥有Bob的公钥，但是用Bob的公钥是无法进行解密的。 Bob用自己的私钥对密文进行解密。 请参考下图, 看一看在Alice和Bob之间到底传输了哪些信息。其实它们之间所传输的信息只有两个：Bob的公钥以及用Bob的公钥加密的密文。由于Bob的私钥没有出现在通信内容中，因此窃听者Eve无法对密文进行解密。 窃听者Eve可能拥有Bob的公钥，但是Bob的公钥只是加密密钥，而不是解密密钥，因此窃听者Eve就无法完成解密操作。 1.RSA1234567非对称加密的密钥分为加密密钥和解密密钥，但这到底是怎样做到的呢？本节中我们来讲解现在使用最广泛的公钥密码算法一一RSA。RSA是一种非对称加密算法，它的名字是由它的三位开发者，即RonRivest、AdiShamir和LeonardAdleman 的姓氏的首字母组成的（Rivest-Shamir-Adleman ）。RSA可以被用于非对称加密和数字签名，关于数字签名我们将在后面章节进行讲解。1983年，RSA公司为RSA算法在美国取得了专利，但现在该专利已经过期。 2.RSA加密12345678910111213141516171819&gt; 下面我们终于可以讲一讲非对称加密的代表—RSA的加密过程了。在RSA中，明文、密钥和密文都是数字。RSA的加密过程可以用下列公式来表达，如下。密文=明文 ^ E mod N（RSA加密）&gt; 也就是说，RSA的密文是对代表明文的数字的E次方求modN的结果。换句话说，就是将明文自己做E次乘法，然后将其结果除以N求余数，这个余数就是密文。&gt;&gt; 咦，就这么简单？&gt;&gt; 对，就这么简单。仅仅对明文进行乘方运算并求mod即可，这就是整个加密的过程。在对称密码中，出现了很多复杂的函数和操作，就像做炒鸡蛋一样将比特序列挪来挪去，还要进行XOR(按位异或)等运算才能完成，但RSA却不同，它非常简洁。&gt;&gt; 对了，加密公式中出现的两个数一一一E和N，到底都是什么数呢？RSA的加密是求明文的E次方modN，因此只要知道E和N这两个数，任何人都可以完成加密的运算。所以说，E和N是RSA加密的密钥，也就是说，**E和N的组合就是公钥**。&gt;&gt; 不过，E和N并不是随便什么数都可以的，它们是经过严密计算得出的。顺便说一句，E是加密（Encryption）的首字母，N是数字（Number)的首字母。&gt;&gt; 有一个很容易引起误解的地方需要大家注意一一E和N这两个数并不是密钥对（公钥和私钥的密钥对）。E和N两个数才组成了一个公钥，因此我们一般会写成 “公钥是(E，N)” 或者 “公钥是&#123;E, N&#125;&quot; 这样的形式，将E和N用括号括起来。&gt;&gt; 现在大家应该已经知道，==RSA的加密就是 “求E次方的modN&quot;==，接下来我们来看看RSA的解密。 3.RSA解密12345678910111213141516&gt; RSA的解密和加密一样简单，可以用下面的公式来表达：明文=密文^DmodN（RSA解密）&gt; 也就是说，对表示密文的数字的D次方求modN就可以得到明文。换句话说，将密文自己做D次乘法，再对其结果除以N求余数，就可以得到明文。&gt;&gt; 这里所使用的数字N和加密时使用的数字N是相同的。数D和数N组合起来就是RSA的解密密钥，因此D和N的组合就是私钥。只有知道D和N两个数的人才能够完成解密的运算。&gt;&gt; 大家应该已经注意到，在RSA中，加密和解密的形式是相同的。加密是求 &quot;E次方的mod N”，而解密则是求 &quot;D次方的modN”，这真是太美妙了。&gt;&gt; 当然，D也并不是随便什么数都可以的，作为解密密钥的D，和数字E有着相当紧密的联系。否则，用E加密的结果可以用D来解密这样的机制是无法实现的。&gt;&gt; 顺便说一句，D是解密〈Decryption）的首字母，N是数字（Number）的首字母。&gt; 我们将上面讲过的内容整理一下，如下表所示。 RSA的加密和解密 Go中生成公钥和私钥需要引入的包 1234567import ( "crypto/rsa" "crypto/rand" "crypto/x509" "encoding/pem" "os") 生成私钥操作流程概述 12341. 使用rsa中的GenerateKey方法生成私钥2. 通过x509标准将得到的ras私钥序列化为ASN.1 的 DER编码字符串3. 将私钥字符串设置到pem格式块中4. 通过pem将设置好的数据进行编码, 并写入磁盘文件中 生成公钥操作流程 12341. 从得到的私钥对象中将公钥信息取出2. 通过x509标准将得到 的rsa公钥序列化为字符串3. 将公钥字符串设置到pem格式块中4. 通过pem将设置好的数据进行编码, 并写入磁盘文件 生成公钥和私钥的源代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// 参数bits: 指定生成的秘钥的长度, 单位: bitfunc RsaGenKey(bits int) error&#123; // 1. 生成私钥文件 // GenerateKey函数使用随机数据生成器random生成一对具有指定字位数的RSA密钥 // 参数1: Reader是一个全局、共享的密码用强随机数生成器 // 参数2: 秘钥的位数 - bit privateKey, err := rsa.GenerateKey(rand.Reader, bits) if err != nil&#123; return err &#125; // 2. MarshalPKCS1PrivateKey将rsa私钥序列化为ASN.1 PKCS#1 DER编码 derStream := x509.MarshalPKCS1PrivateKey(privateKey) // 3. Block代表PEM编码的结构, 对其进行设置 block := pem.Block&#123; Type: &quot;RSA PRIVATE KEY&quot;,//&quot;RSA PRIVATE KEY&quot;, Bytes: derStream, &#125; // 4. 创建文件 privFile, err := os.Create(&quot;private.pem&quot;) if err != nil&#123; return err &#125; // 5. 使用pem编码, 并将数据写入文件中 err = pem.Encode(privFile, &amp;block) if err != nil&#123; return err &#125; // 6. 最后的时候关闭文件 defer privFile.Close() // 7. 生成公钥文件 publicKey := privateKey.PublicKey derPkix, err := x509.MarshalPKIXPublicKey(&amp;publicKey) if err != nil&#123; return err &#125; block = pem.Block&#123; Type: &quot;RSA PUBLIC KEY&quot;,//&quot;PUBLIC KEY&quot;, Bytes: derPkix, &#125; pubFile, err := os.Create(&quot;public.pem&quot;) if err != nil&#123; return err &#125; // 8. 编码公钥, 写入文件 err = pem.Encode(pubFile, &amp;block) if err != nil&#123; panic(err) return err &#125; defer pubFile.Close() return nil&#125; 调用完会在本地生成两个文件。保存好私钥文件和公钥文件 重要的函数介绍: GenerateKey函数使用随机数据生成器random生成一对具有指定字位数的RSA密钥。 12345678"crypto/rsa" 包中的函数func GenerateKey(random io.Reader, bits int) (priv *PrivateKey, err error) - 参数1: io.Reader: 赋值为: rand.Reader -- rand包实现了用于加解密的更安全的随机数生成器。 -- var Reader io.Reader (rand包中的变量) - 参数2: bits: 秘钥长度 - 返回值1: 代表一个RSA私钥。 - 返回值2: 错误信息 通过x509 将rsa私钥序列化为ASN.1 PKCS1 DER编码 1234"crypto/x509" 包中的函数 (x509包解析X.509编码的证书和密钥)。func MarshalPKCS1PrivateKey(key *rsa.PrivateKey) []byte - 参数1: 通过rsa.GenerateKey得到的私钥 - 返回值: 将私钥通过ASN.1序列化之后得到的私钥编码数据 设置Pem编码结构 123456Block代表PEM编码的结构。type Block struct &#123; Type string // 得自前言的类型（如"RSA PRIVATE KEY"） Headers map[string]string // 可选的头项，Headers是可为空的多行键值对。 Bytes []byte // 内容解码后的数据，一般是DER编码的ASN.1结构&#125; 将得到的Pem格式私钥通过文件指针写入磁盘中 1234"encoding/pem" 包中的函数func Encode(out io.Writer, b *Block) error - 参数1: 可进行写操作的IO对象, 此处需要指定一个文件指针 - 参数2: 初始化完成的Pem块对象, 即Block对象 通过RSA私钥得到公钥 123456789101112131415// 私钥type PrivateKey struct &#123; PublicKey // 公钥 D *big.Int // 私有的指数 Primes []*big.Int // N的素因子，至少有两个 // 包含预先计算好的值，可在某些情况下加速私钥的操作 Precomputed PrecomputedValues&#125;// 公钥type PublicKey struct &#123; N *big.Int // 模 E int // 公开的指数&#125;通过私钥获取公钥publicKey := privateKey.PublicKey // privateKey为私钥对象 通过x509将公钥序列化为PKIX格式DER编码。 12345"crypto/x509" 包中的函数func MarshalPKIXPublicKey(pub interface&#123;&#125;) ([]byte, error) - 参数1: 通过私钥对象得到的公钥 - 返回值1：将公钥通过ASN.1序列化之后得到的编码数据 - 返回值2: 错误信息 将公钥编码之后的数据格式化为Pem结构, 参考私钥的操作 将得到的Pem格式公钥通过文件指针写入磁盘中 生成的私钥和公钥文件数据 Go中使用RSA 操作步骤 公钥加密 12341. 将公钥文件中的公钥读出, 得到使用pem编码的字符串2. 将得到的字符串解码3. 使用x509将编码之后的公钥解析出来4. 使用得到的公钥通过rsa进行数据加密 私钥解密 12341. 将私钥文件中的私钥读出, 得到使用pem编码的字符串2. 将得到的字符串解码3. 使用x509将编码之后的私钥解析出来4. 使用得到的私钥通过rsa进行数据解密 代码实现 RSA公钥加密 1234567891011121314151617181920212223242526272829func RSAEncrypt(src, filename []byte) []byte &#123; // 1. 根据文件名将文件内容从文件中读出 file, err := os.Open(string(filename)) if err != nil &#123; return nil &#125; // 2. 读文件 info, _ := file.Stat() allText := make([]byte, info.Size()) file.Read(allText) // 3. 关闭文件 file.Close() // 4. 从数据中查找到下一个PEM格式的块 block, _ := pem.Decode(allText) if block == nil &#123; return nil &#125; // 5. 解析一个DER编码的公钥 pubInterface, err := x509.ParsePKIXPublicKey(block.Bytes) if err != nil &#123; return nil &#125; pubKey := pubInterface.(*rsa.PublicKey) // 6. 公钥加密 result, _ := rsa.EncryptPKCS1v15(rand.Reader, pubKey, src) return result&#125; RSA私钥解密 123456789101112131415161718192021func RSADecrypt(src, filename []byte) []byte &#123; // 1. 根据文件名将文件内容从文件中读出 file, err := os.Open(string(filename)) if err != nil &#123; return nil &#125; // 2. 读文件 info, _ := file.Stat() allText := make([]byte, info.Size()) file.Read(allText) // 3. 关闭文件 file.Close() // 4. 从数据中查找到下一个PEM格式的块 block, _ := pem.Decode(allText) // 5. 解析一个pem格式的私钥 privateKey , err := x509.ParsePKCS1PrivateKey(block.Bytes) // 6. 私钥解密 result, _ := rsa.DecryptPKCS1v15(rand.Reader, privateKey, src) return result &#125; 重要的函数介绍 将得到的Pem格式私钥通过文件指针写入磁盘中 12345"encoding/pem" 包中的函数func Decode(data []byte) (p *Block, rest []byte) - 参数 data: 需要解析的数据块 - 返回值1: 从参数中解析出的PEM格式的块 - 返回值2: 参数data剩余的未被解码的数据 解析一个DER编码的公钥 , pem中的Block结构体中的数据格式为ASN.1编码 12345函数所属的包: "crypto/x509"func ParsePKIXPublicKey(derBytes []byte) (pub interface&#123;&#125;, err error) - 参数 derBytes: 从pem的Block结构体中取的ASN.1编码数据 - 返回值 pub: 接口对象, 实际是公钥数据 - 参数 err: 错误信息 解析一个DER编码的私钥 , pem中的Block结构体中的数据格式为ASN.1编码 12345函数所属的包: "crypto/x509"func ParsePKCS1PrivateKey(der []byte) (key *rsa.PrivateKey, err error) - 参数 der: 从pem的Block结构体中取的ASN.1编码数据 - 返回值 key: 解析出的私钥 - 返回值 err: 错误信息 将接口转换为公钥 123pubKey := pubInterface.(*rsa.PublicKey) - pubInterface: ParsePKIXPublicKey函数返回的 interface&#123;&#125; 对象 - pubInterface.(*rsa.PublicKey): 将pubInterface转换为公钥类型 rsa.PublicKey 使用公钥加密数据 1234567函数所属的包: "crypto/rsa"func EncryptPKCS1v15(rand io.Reader, pub *PublicKey, msg []byte) (out []byte, err error) - 参数 rand: 随机数生成器, 赋值为 rand.Reader - 参数 pub: 非对称加密加密使用的公钥 - 参数 msg: 要使用公钥加密的原始数据 - 返回值 out: 加密之后的数据 - 返回值 err: 错误信息 使用私钥解密数据 1234567函数所属的包: "crypto/rsa"func DecryptPKCS1v15(rand io.Reader, priv *PrivateKey, ciphertext []byte) (out []byte, err error) - 参数 rand: 随机数生成器, 赋值为 rand.Reader - 参数 priv: 非对称加密解密使用的私钥 - 参数 ciphertext: 需要使用私钥解密的数据 - 返回值 out: 解密之后得到的数据 - 返回值 err: 错误信 这就是大概RSA的加密解密。]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>加密算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AES的CTR模式加密解密详解]]></title>
    <url>%2F2018%2F11%2F03%2FAES%E7%9A%84CTR%E6%A8%A1%E5%BC%8F%E5%8A%A0%E5%AF%86%E8%A7%A3%E5%AF%86%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[CTR 模式123CTR模式的全称是CounTeR模式（计数器模式）。CTR摸式是一种通过将逐次累加的计数器进行加密来生成密钥流的流密码（下图）。CTR模式中，每个分组对应一个逐次累加的计数器，并通过对计数器进行加密来生成密钥流。也就是说，最终的密文分组是通过将计数器加密得到的比特序列，与明文分组进行XOR而得到的。 这里我们就简单介绍一下AES的CTR模式的实现。 计数器的生成方法1每次加密时都会生成一个不同的值（nonce）来作为计数器的初始值。当分组长度为128比特（16字节）时，计数器的初始值可能是像下面这样的形式。 1其中前8个字节为nonce（随机数），这个值在每次加密时必须都是不同的，后8个字节为分组序号，这个部分是会逐次累加的。在加密的过程中，计数器的值会产生如下变化： 按照上述生成方法，可以保证计数器的值每次都不同。由于计数器的值每次都不同，因此每个分组中将计数器进行加密所得到的密钥流也是不同的。也是说，这种方法就是用分组密码来模拟生成随机的比特序列。 OFB模式与CTR模式对比1CTR模式和OFB模式一样，都属于流密码。如果我们将单个分组的加密过程拿出来，那么OFB模式和CTR模式之间的差异还是很容易理解的（下图）。OFB模式是将加密的输出反愦到输入，而CTR模式则是将计数器的值用作输入。 CTR模式的特点 CTR模式的加密和解密使用了完全相同的结构，因此在程序实现上比较容易。这一特点和同为流密码的OFB模式是一样的。 此外，CTR模式中可以以任意顺序对分组进行加密和解密，因此在加密和解密时需要用到的“计数器”的值可以由nonce和分组序号直接计算出来。这一性质是OFB模式所不具备的。 能够以任意顺序处理分组，就意味着能够实现并行计算。在支持并行计算的系统中，CTR模式的速度是非常快的。 几种模式的对比总结 AES的CTR模式代码实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253func AesCTR_Encrypt(plainText,key[]byte)[]byte&#123; //判断用户传过来的key是否符合16字节，如果不符合16字节加以处理 keylen:=len(key) if keylen==0&#123; //如果用户传入的密钥为空那么就用默认密钥 key=[]byte(&quot;wumansgygoaescbc&quot;) //默认密钥 &#125;else if keylen&gt;0&amp;&amp;keylen&lt;16&#123; //如果密钥长度在0到16之间，那么用0补齐剩余的 key=append(key,bytes.Repeat([]byte&#123;0&#125;,(16-keylen))...) &#125;else if keylen&gt;16&#123; key=key[:16] &#125; //1.指定使用的加密aes算法 block, err := aes.NewCipher(key) if err!=nil&#123; panic(err) &#125; //2.不需要填充,直接获取ctr分组模式的stream // 返回一个计数器模式的、底层采用block生成key流的Stream接口，初始向量iv的长度必须等于block的块尺寸。 iv := []byte(&quot;wumansgygoaesctr&quot;) stream := cipher.NewCTR(block, iv) //3.加密操作 cipherText := make([]byte,len(plainText)) stream.XORKeyStream(cipherText,plainText) return cipherText&#125;func AesCTR_Decrypt(cipherText,key []byte)[]byte&#123; //判断用户传过来的key是否符合16字节，如果不符合16字节加以处理 keylen:=len(key) if keylen==0&#123; //如果用户传入的密钥为空那么就用默认密钥 key=[]byte(&quot;wumansgygoaescbc&quot;) //默认密钥 &#125;else if keylen&gt;0&amp;&amp;keylen&lt;16&#123; //如果密钥长度在0到16之间，那么用0补齐剩余的 key=append(key,bytes.Repeat([]byte&#123;0&#125;,(16-keylen))...) &#125;else if keylen&gt;16&#123; key=key[:16] &#125; //1.指定算法:aes block, err:= aes.NewCipher(key) if err!=nil&#123; panic(err) &#125; //2.返回一个计数器模式的、底层采用block生成key流的Stream接口，初始向量iv的长度必须等于block的块尺寸。 iv := []byte(&quot;wumansgygoaesctr&quot;) stream := cipher.NewCTR(block, iv) //3.解密操作 plainText := make([]byte,len(cipherText)) stream.XORKeyStream(plainText,cipherText) return plainText&#125;]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>加密算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AES的CBC模式加密解密详解]]></title>
    <url>%2F2018%2F11%2F03%2FAES%E7%9A%84CBC%E6%A8%A1%E5%BC%8F%E5%8A%A0%E5%AF%86%E8%A7%A3%E5%AF%86%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[1.AES12345678910111213AES（Advanced Encryption Standard）是取代其前任标准（DES）而成为新标准的一种对称密码算法。全世界的企业和密码学家提交了多个对称密码算法作为AES的候选，最终在2000年从这些候选算法中选出了一种名为==Rijndael==的对称密码算法，并将其确定为了AES。Rijndael是由比利时密码学家Joan Daemen和Vincent Rijmen设汁的分组密码算法，今后会有越来越多的密码软件支持这种算法。==Rijndael的分组长度为128比特==，密钥长度可以以32比特为单位在128比特到256比特的范围内进行选择（不过==在AES的规格中，密钥长度只有128、192和256比特三种==）。128bit = 16字节192bit = 24字节256bit = 32字节在go提供的接口中秘钥长度只能是16字节 2.AES的加密和解密0123456789ABCDEF ABCDEFGHIJKLMNOP 1234567891011和DES—样，AES算法也是由多个轮所构成的，下图展示了每一轮的大致计算步骤。DES使用Feistel网络作为其基本结构，而AES没有使用Feistel网络，而是使用了SPN Rijndael的输人分组为128比特，也就是16字节。首先，需要逐个字节地对16字节的输入数据进行SubBytes处理。所谓SubBytes,就是以每个字节的值（0～255中的任意值）为索引，从一张拥有256个值的替换表（S-Box）中查找出对应值的处理，也是说，将一个1字节的值替换成另一个1字节的值。SubBytes之后需要进行ShiftRows处理，即将SubBytes的输出以字节为单位进行打乱处理。从下图的线我们可以看出，这种打乱处理是有规律的。ShiftRows之后需要进行MixColumns处理，即对一个4字节的值进行比特运算，将其变为另外一个4字节值。最后，需要将MixColumns的输出与轮密钥进行XOR，即进行AddRoundKey处理。到这里，AES的一轮就结東了。实际上，在AES中需要重复进行10 ~ 14轮计算。通过上面的结构我们可以发现输入的所有比特在一轮中都会被加密。和每一轮都只加密一半输人的比特的Feistel网络相比，这种方式的优势在于加密所需要的轮数更少。此外，这种方式还有一个优势，即SubBytes，ShiftRows和MixColumns可以分别按字节、行和列为单位进行并行计算。 1234567SubBytes -- 字节代换ShiftRows -- 行移位代换MixColumns -- 列混淆 AddRoundKey -- 轮密钥加 下图展示了AES中一轮的解密过程。从图中我们可以看出，SubBytes、ShiftRows、MixColumns分别存在反向运算InvSubBytes、InvShiftRows、InvMixColumns，这是因为AES不像Feistel网络一样能够用同一种结构实现加密和解密。 12345InvSubBytes -- 逆字节替代InvShiftRows -- 逆行移位InvMixColumns -- 逆列混淆 3.Go中对AES的使用加解密实现思路 加密 - CBC分组模式 12345671. 创建并返回一个使用AES算法的cipher.Block接口 - 秘钥长度为128bit, 即 128/8 = 16字节(byte)2. 对最后一个明文分组进行数据填充 - AES是以128比特的明文（比特序列）为一个单位来进行加密的 - 最后一组不够128bit, 则需要进行数据填充3. 创建一个密码分组为链接模式的, 底层使用AES加密的BlockMode接口4. 加密连续的数据块 解密 12341. 创建并返回一个使用AES算法的cipher.Block接口2. 创建一个密码分组为链接模式的, 底层使用AES解密的BlockMode接口3. 数据块解密4. 去掉最后一组的填充数据 加解密的代码实现AES加密代码 12345678910111213141516// AES加密func AESEncrypt(src, key []byte) []byte&#123; // 1. 创建一个使用AES加密的块对象 block, err := aes.NewCipher(key) if err != nil&#123; panic(err) &#125; // 2. 最后一个分组进行数据填充 src = PKCS5Padding(src, block.BlockSize()) // 3. 创建一个分组为链接模式, 底层使用AES加密的块模型对象 blockMode := cipher.NewCBCEncrypter(block, key[:block.BlockSize()]) // 4. 加密 dst := src blockMode.CryptBlocks(dst, src) return dst&#125; AES解密 12345678910111213141516// AES解密func AESDecrypt(src, key []byte) []byte&#123; // 1. 创建一个使用AES解密的块对象 block, err := aes.NewCipher(key) if err != nil&#123; panic(err) &#125; // 2. 创建分组为链接模式, 底层使用AES的解密模型对象 blockMode := cipher.NewCBCDecrypter(block, key[:block.BlockSize()]) // 3. 解密 dst := src blockMode.CryptBlocks(dst, src) // 4. 去掉尾部填充的字 dst = PKCS5UnPadding(dst) return dst&#125; 重要的函数说明 生成一个底层使用AES加解密的Block接口对象 1234函数对应的包: import "crypto/aes"func NewCipher(key []byte) (cipher.Block, error) - 参数 key: aes对称加密使用的密码, 密码长度为128bit, 即16byte - 返回值 cipher.Block: 创建出的使用AES加/解密的Block接口对象 创建一个密码分组为CBC模式, 底层使用b加密的BlockMode接口对象 12345函数对应的包: import "crypto/cipher"func NewCBCEncrypter(b Block, iv []byte) BlockMode - 参数 b: 使用aes.NewCipher函数创建出的Block接口对象 - 参数 iv: 事先准备好的一个长度为一个分组长度的比特序列, 每个分组为64bit, 即8byte - 返回值: 得到的BlockMode接口对象 使用cipher包的BlockMode接口对象对数据进行加解密 12345678910接口对应的包: import "crypto/cipher"type BlockMode interface &#123; // 返回加密字节块的大小 BlockSize() int // 加密或解密连续的数据块，src的尺寸必须是块大小的整数倍，src和dst可指向同一内存地址 CryptBlocks(dst, src []byte)&#125;接口中的 CryptBlocks(dst, src []byte) 方法: - 参数 dst: 传出参数, 存储加密或解密运算之后的结果 - 参数 src: 传入参数, 需要进行加密或解密的数据切片(字符串) 创建一个密码分组为CBC模式, 底层使用b解密的BlockMode接口对象 123456函数对应的包: import "crypto/cipher"func NewCBCDecrypter(b Block, iv []byte) BlockMode - 参数 b: 使用des.NewCipher函数创建出的Block接口对象 - 参数 iv: 事先准备好的一个长度为一个分组长度的比特序列, 每个分组为128bit, 即16byte, 该序列的值需要和NewCBCEncrypter函数的第二个参数iv值相同 - 返回值: 得到的BlockMode接口对象]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>加密算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三重DES加密解密详解]]></title>
    <url>%2F2018%2F11%2F03%2F%E4%B8%89%E9%87%8DDES%E5%8A%A0%E5%AF%86%E8%A7%A3%E5%AF%86%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[1.三重DES 现在DES已经可以在现实的时间内被暴力破解，因此我们需要一种用来替代DES的分组密码，三重DES就是出于这个目的被开发出来的。 1三重DES（triple-DES）是为了增加DES的强度，==将DES重复3次所得到的一种密码算法==，通常缩写为3DES。 2.三重DES的加密12345678910111213141516171819三重DES的加解密机制如图所示：加-&gt;解-&gt;加 -&gt; 目的是为了兼容des3des秘钥长度24字节 = 1234567a 1234567b 1234567a明文: 10秘钥1: 2秘钥2: 3秘钥3: 4加密算法: 明文+秘钥解密算法: 密文-秘钥10+2-3+4 1234567891011&gt; 明文经过三次DES处理才能变成最后的密文，由于**DES密钥的长度实质上是56比特**，因此三重DES的密钥长度就是56×3=168比特, 加上用于错误检测的标志位8x3, 共192bit。&gt;&gt; 从上图我们可以发现，三重DES并不是进行三次DES加密（加密--&gt;加密--&gt;加密），而是**加密--&gt;解密--&gt;加密**的过程。在加密算法中加人解密操作让人感觉很不可思议，实际上这个方法是IBM公司设计出来的，目的是为了让三重DES能够兼容普通的DES。&gt;&gt; 当三重DES中所有的密钥都相同时，三重DES也就等同于普通的DES了。这是因为在前两步加密--&gt;解密之后，得到的就是最初的明文。因此，以前用DES加密的密文，就可以通过这种方式用三重DES来进行解密。也就是说，三重DES对DES具备向下兼容性。&gt;&gt; 如果密钥1和密钥3使用相同的密钥，而密钥2使用不同的密钥（也就是只使用两个DES密钥），这种三重DES就称为DES-EDE2。EDE表示的是加密（Encryption) --&gt;解密（Decryption)--&gt;加密（Encryption）这个流程。&gt;&gt; 密钥1、密钥2、密钥3全部使用不同的比特序列的三重DES称为DES-EDE3。&gt; 尽管三重DES目前还被银行等机构使用，但其处理速度不高，而且在安全性方面也逐渐显现出了一些问题。 3.Go中对3DES的操作加解密实现思路 加密 - CBC分组模式 12345671. 创建并返回一个使用3DES算法的cipher.Block接口 - **秘钥长度为64bit*3=192bit, 即 192/8 = 24字节(byte)**2. 对最后一个明文分组进行数据填充 - 3DES是以64比特的明文（比特序列）为一个单位来进行加密的 - 最后一组不够64bit, 则需要进行数据填充( **参考第三章**)3. 创建一个密码分组为链接模式的, 底层使用3DES加密的BlockMode接口4. 加密连续的数据块 解密 12341. 创建并返回一个使用3DES算法的cipher.Block接口2. 创建一个密码分组为链接模式的, 底层使用3DES解密的BlockMode接口3. 数据块解密4. 去掉最后一组的填充数据 加解密的代码实现3DES加密代码 12345678910111213141516// 3DES加密func TripleDESEncrypt(src, key []byte) []byte &#123; // 1. 创建并返回一个使用3DES算法的cipher.Block接口 block, err := des.NewTripleDESCipher(key) if err != nil&#123; panic(err) &#125; // 2. 对最后一组明文进行填充 src = PKCS5Padding(src, block.BlockSize()) // 3. 创建一个密码分组为链接模式, 底层使用3DES加密的BlockMode模型 blockMode := cipher.NewCBCEncrypter(block, key[:8]) // 4. 加密数据 dst := src blockMode.CryptBlocks(dst, src) return dst&#125; 3DES解密代码 12345678910111213141516// 3DES解密func TripleDESDecrypt(src, key []byte) []byte &#123; // 1. 创建3DES算法的Block接口对象 block, err := des.NewTripleDESCipher(key) if err != nil&#123; panic(err) &#125; // 2. 创建密码分组为链接模式, 底层使用3DES解密的BlockMode模型 blockMode := cipher.NewCBCDecrypter(block, key[:8]) // 3. 解密 dst := src blockMode.CryptBlocks(dst, src) // 4. 去掉尾部填充的数据 dst = PKCS5UnPadding(dst) return dst&#125; 重要的函数说明 生成一个底层使用3DES加解密的Block接口对象 1234函数对应的包: import "crypto/des"func NewTripleDESCipher(key []byte) (cipher.Block, error) - 参数 key: 3des对称加密使用的密码, 密码长度为(64*3)bit, 即(8*3)byte - 返回值 cipher.Block: 创建出的使用DES加/解密的Block接口对象 创建一个密码分组为CBC模式, 底层使用b加密的BlockMode接口对象 12345函数对应的包: import "crypto/cipher"func NewCBCEncrypter(b Block, iv []byte) BlockMode - 参数 b: 使用des.NewTripleDESCipher 函数创建出的Block接口对象 - 参数 iv: 事先准备好的一个长度为一个分组长度的比特序列, 每个分组为64bit, 即8byte - 返回值: 得到的BlockMode接口对象 使用cipher包的BlockMode接口对象对数据进行加解密 12345678910接口对应的包: import "crypto/cipher"type BlockMode interface &#123; // 返回加密字节块的大小 BlockSize() int // 加密或解密连续的数据块，src的尺寸必须是块大小的整数倍，src和dst可指向同一内存地址 CryptBlocks(dst, src []byte)&#125;接口中的 CryptBlocks(dst, src []byte) 方法: - 参数 dst: 传出参数, 存储加密或解密运算之后的结果 - 参数 src: 传入参数, 需要进行加密或解密的数据切片(字符串) 创建一个密码分组为CBC模式, 底层使用b解密的BlockMode接口对象 123456函数对应的包: import "crypto/cipher"func NewCBCDecrypter(b Block, iv []byte) BlockMode - 参数 b: 使用des.NewTripleDESCipher 函数创建出的Block接口对象 - 参数 iv: 事先准备好的一个长度为一个分组长度的比特序列, 每个分组为64bit, 即8byte, 该序列的值需要和NewCBCEncrypter函数的第二个参数iv值相同 - 返回值: 得到的BlockMode接口对象]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>加密算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拜占庭容错算法之PBFT全面理解]]></title>
    <url>%2F2018%2F10%2F30%2F%E6%8B%9C%E5%8D%A0%E5%BA%AD%E5%AE%B9%E9%94%99%E7%AE%97%E6%B3%95%E4%B9%8BPBFT%E5%85%A8%E9%9D%A2%E7%90%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[PBFT是Practical Byzantine Fault Tolerance的缩写，意为实用拜占庭容错算法。该算法是Miguel Castro (卡斯特罗)和Barbara Liskov（利斯科夫）在1999年提出来的，解决了原始拜占庭容错算法效率不高的问题，将算法复杂度由指数级降低到多项式级，使得拜占庭容错算法在实际系统应用中变得可行。该论文发表在1999年的操作系统设计与实现国际会议上（OSDI99）。没错，这个Loskov就是提出著名的里氏替换原则（LSP）的人，2008年图灵奖得主。 摘要部分OSDI99这篇论文描述了一种副本复制（replication）算法解决拜占庭容错问题。作者认为拜占庭容错算法将会变得更加重要，因为恶意攻击和软件错误的发生将会越来越多，并且导致失效的节点产生任意行为。（拜占庭节点的任意行为有可能误导其他副本节点产生更大的危害，而不仅仅是宕机失去响应。）而早期的拜占庭容错算法或者基于同步系统的假设，或者由于性能太低而不能在实际系统中运作。这篇论文中描述的算法是实用的，因为该算法可以工作在异步环境中，并且通过优化在早期算法的基础上把响应性能提升了一个数量级以上。作者使用这个算法实现了拜占庭容错的网络文件系统（NFS），性能测试证明了该系统仅比无副本复制的标准NFS慢了3%。 1.概要介绍第一段大片废话就是说明拜占庭算法越来越重要了，然后说这篇论文提出解决拜占庭容错的状态机副本复制（state machine replication）算法。这个算法在保证活性和安全性（liveness &amp; safety）的前提下提供了(n-1)/3的容错性。从Lamport教授在1982年提出拜占庭问题开始，已经有一大堆算法去解决拜占庭容错了。但是一句话概括：这些算法都是狗屎！PBFT算法跟这些妖艳贱货完全不同，在只读操作中只使用1次消息往返（message round trip），在只写操作中只使用2次消息往返，并且在正常操作中使用了消息验证编码（Message Authentication Code,简称MAC），而造成妖艳贱货性能低下的公钥加密（public-key cryptography）只在发生失效的情况下使用。作者不仅提出算法，而且使用这个算法实现了一个牛逼的系统（拜占庭容错的NFS），反正性能杠杠的。作者先炫耀一下这边论文的贡献亮瞎你们的狗眼： 1）首次提出在异步网络环境下使用状态机副本复制协议2）使用多种优化使性能显著提升3）实现了一种拜占庭容错的分布式文件系统4）为副本复制的性能损耗提供试验数据支持 2.系统模型 这部分主要对节点行为和网络环境进行剧情设定，然后赋予了消息的加密属性，最后对大魔王的能力进行设定。 系统假设为异步分布式的，通过网络传输的消息可能丢失、延迟、重复或者乱序。作者假设节点的失效必须是独立发生的，也就是说代码、操作系统和管理员密码这些东西在各个节点上是不一样的。（那么如果节点失效不独立发生，PBFT算法就失效了吗？） 作者使用了加密技术来防止欺骗攻击和重播攻击，以及检测被破坏的消息。消息包含了公钥签名（其实就是RSA算法）、消息验证编码（MAC）和无碰撞哈希函数生成的消息摘要（message digest）。使用m表示消息，mi表示由节点i签名的消息，D(m)表示消息m的摘要。按照惯例，只对消息的摘要签名，并且附在消息文本的后面。并且假设所有的节点都知道其他节点的公钥以进行签名验证。 系统允许大魔王可以操纵多个失效节点、延迟通讯、甚至延迟正确节点来毁灭世界。但是作者限定大魔王不能无限期地延迟正确的节点，并且大魔王算力有限不能破解加密算法。例如，大魔王不能伪造正确节点的有效签名，不能从摘要数据反向计算出消息内容，或者找到两个有同样摘要的消息。 3.服务属性 这部分描述了副本复制服务的特性 论文算法实现的是一个具有确定性的副本复制服务，这个服务包括了一个状态（state）和多个操作（operations）。这些操作不仅能够进行简单读写，而且能够基于状态和操作参数进行任意确定性的计算。客户端向副本复制服务发起请求来执行操作，并且阻塞以等待回复。副本复制服务由n个节点组成。 针对安全性 算法在失效节点数量不超过（n-1)/3的情况下同时保证安全性和活性（safety &amp; liveness）。安全性是指副本复制服务满足线性一致性（linearizability）,就像中心化系统一样原子化执行操作。安全性要求失效副本的数量不超过上限，但是对客户端失效的数量和是否与副本串谋不做限制。系统通过访问控制来限制失效客户端可能造成的破坏，审核客户端并阻止客户端发起无权执行的操作。同时，服务可以提供操作来改变一个客户端的访问权限。因为算法保证了权限撤销操作可以被所有客户端观察到，这种方法可以提供强大的机制从失效的客户端攻击中恢复。 针对活性 算法不依赖同步提供安全性，因此必须依靠同步提供活性。否则，这个算法就可以被用来在异步系统中实现共识，而这是不可能的（由Fischer1985的论文证明）。本文的算法保证活性，即所有客户端最终都会收到针对他们请求的回复，只要失效副本的数量不超过（n-1)/3，并且延迟delay(t)不会无限增长。这个delay(t)表示t时刻发出的消息到它被目标最终接收的时间间隔，假设发送者持续重传直到消息被接收。这时一个相当弱的同步假设，因为在真实系统中网络失效最终都会被修复。但是这就规避了Fischer1985提出的异步系统无法达成共识的问题。 下面这段话是关键 本文的算法弹性是达到最优的：当存在f个失效节点时必须保证存在至少3f+1个副本数量，这样才能保证在异步系统中提供安全性和活性。这么多数量的副本是需要的，因为在同n-f个节点通讯后系统必须做出正确判断，由于f个副本有可能失效而不发回响应。但是，有可能f个没有失效的副本不发回响应（是因为网络延迟吗？），因此f个发回响应的副本有可能是失效的。尽管如此，系统仍旧需要足够数量非失效节点的响应，并且这些非失效节点的响应数量必须超过失效节点的响应数量，即n-2f&gt;f，因此得到n&gt;3f。 算法不能解决信息保密的问题，失效的副本有可能将信息泄露给攻击者。在一般情况下不可能提供信息保密，因为服务操作需要使用参数和服务状态处理任意的计算，所有的副本都需要这些信息来有效执行操作。当然，还是有可能在存在恶意副本的情况下通过秘密分享模式（secret sharing scheme）来实现私密性，因为参数和部分状态对服务操作来说是不可见的。 4.算法PBFT是一种状态机副本复制算法，即服务作为状态机进行建模，状态机在分布式系统的不同节点进行副本复制。每个状态机的副本都保存了服务的状态，同时也实现了服务的操作。将所有的副本组成的集合使用大写字母R表示，使用0到|R|-1的整数表示每一个副本。为了描述方便，假设|R|=3f+1，这里f是有可能失效的副本的最大个数。尽管可以存在多于3f+1个副本，但是额外的副本除了降低性能之外不能提高可靠性。 PBFT的剧情缓缓展开，首先介绍舞台（view）、演员（replica）和角色（primary、backups） 所有的副本在一个被称为视图（View）的轮换过程（succession of configuration）中运作。在某个视图中，一个副本作为主节点（primary），其他的副本作为备份（backups）。视图是连续编号的整数。主节点由公式p = v mod |R|计算得到，这里v是视图编号，p是副本编号，|R|是副本集合的个数。当主节点失效的时候就需要启动视图更换（view change）过程。Viewstamped Replication算法和Paxos算法就是使用类似方法解决良性容错的。 PBFT算法的狗血剧情如下：1.客户端向主节点发送请求调用服务操作2.主节点通过广播将请求发送给其他副本3.所有副本都执行请求并将结果发回客户端4.客户端需要等待f+1个不同副本节点发回相同的结果，作为整个操作的最终结果。 同所有的状态机副本复制技术一样，PBFT对每个副本节点提出了两个限定条件：（1）所有节点必须是确定性的。也就是说，在给定状态和参数相同的情况下，操作执行的结果必须相同；（2）所有节点必须从相同的状态开始执行。在这两个限定条件下，即使失效的副本节点存在，PBFT算法对所有非失效副本节点的请求执行总顺序达成一致，从而保证安全性。 接下去描述简化版本的PBFT算法，忽略磁盘空间不足和消息重传等细节内容。并且，本文假设消息验证过程是通过数字签名方法实现的，而不是更加高效的基于消息验证编码（MAC）的方法。 4.1客户端客户端c向主节点发送&lt;REQUEST,o,t,c&gt;请求执行状态机操作o，这里时间戳t用来保证客户端请求只会执行一次。客户端c发出请求的时间戳是全序排列的，后续发出的请求比早先发出的请求拥有更高的时间戳。例如，请求发起时的本地时钟值可以作为时间戳。 每个由副本节点发给客户端的消息都包含了当前的视图编号，使得客户端能够跟踪视图编号，从而进一步推算出当前主节点的编号。客户端通过点对点消息向它自己认为的主节点发送请求，然后主节点自动将该请求向所有备份节点进行广播。 副本发给客户端的响应为&lt;REPLY,v,t,c,i,r&gt;，v是视图编号，t是时间戳，i是副本的编号，r是请求执行的结果。 客户端等待f+1个从不同副本得到的同样响应，同样响应需要保证签名正确，并且具有同样的时间戳t和执行结果r。这样客户端才能把r作为正确的执行结果，因为失效的副本节点不超过f个，所以f+1个副本的一致响应必定能够保证结果是正确有效的。 如果客户端没有在有限时间内收到回复，请求将向所有副本节点进行广播。如果请求已经在副本节点处理过了，副本就向客户端重发一遍执行结果。如果请求没有在副本节点处理过，该副本节点将把请求转发给主节点。如果主节点没有将该请求进行广播，那么就有认为主节点失效，如果有足够多的副本节点认为主节点失效，则会触发一次视图变更。 本文假设客户端会等待上一个请求完成才会发起下一个请求，但是只要能够保证请求顺序，可以允许请求是异步的。 4.2 PBFT算法主线流程（正常情况） 世界格局 每个副本节点的状态都包含了服务的整体状态，副本节点上的消息日志(message log)包含了该副本节点接受(accepted)的消息，并且使用一个整数表示副本节点的当前视图编号。 事件的导火索 当主节点p收到客户端的请求m，主节点将该请求向所有副本节点进行广播，由此一场轰轰烈烈的三阶段协议（three-phase protocol）拉开了序幕。在这里，至于什么消息过多需要缓存的情况我们就不管了，这不是重点。 三个阶段的任务 我们重点讨论预准备（pre-prepare）、准备(prepare)和确认(commit)这三个历史性阶段。预准备和准备两个阶段用来确保同一个视图中请求发送的时序性（即使对请求进行排序的主节点失效了），准备和确认两个阶段用来确保在不同的视图之间的确认请求是严格排序的。 预准备阶段 在预准备阶段，主节点分配一个序列号n给收到的请求，然后向所有备份节点群发预准备消息，预准备消息的格式为&lt;&lt;PRE-PREPARE,v,n,d&gt;,m&gt;，这里v是视图编号，m是客户端发送的请求消息，d是请求消息m的摘要。 请求本身是不包含在预准备的消息里面的，这样就能使预准备消息足够小，因为预准备消息的目的是作为一种证明，确定该请求是在视图v中被赋予了序号n，从而在视图变更的过程中可以追索。另外一个层面，将“请求排序协议”和“请求传输协议”进行解耦，有利于对消息传输的效率进行深度优化。 备份节点对预准备消息的态度 只有满足以下条件，各个备份节点才会接受一个预准备消息： 请求和预准备消息的签名正确，并且d与m的摘要一致。 当前视图编号是v。 该备份节点从未在视图v中接受过序号为n但是摘要d不同的消息m。（许仙在这辈子从未见过名字叫白素贞的美貌女子） 预准备消息的序号n必须在水线（watermark）上下限h和H之间。 水线存在的意义在于防止一个失效节点使用一个很大的序号消耗序号空间。 进入准备阶段 如果备份节点i接受了预准备消息&lt;&lt;PRE-PREPARE,v,n,d&gt;,m&gt;，则进入准备阶段。在准备阶段的同时，该节点向所有副本节点发送准备消息&lt;PREPARE,v,n,d,i&gt;，并且将预准备消息和准备消息写入自己的消息日志。如果看预准备消息不顺眼，就什么都不做。 接受准备消息需要满足的条件 包括主节点在内的所有副本节点在收到准备消息之后，对消息的签名是否正确，视图编号是否一致，以及消息序号是否满足水线限制这三个条件进行验证，如果验证通过则把这个准备消息写入消息日志中。 准备阶段完成的标志 我们定义准备阶段完成的标志为副本节点i将(m,v,n,i)记入其消息日志，其中m是请求内容，预准备消息m在视图v中的编号n，以及2f个从不同副本节点收到的与预准备消息一致的准备消息。每个副本节点验证预准备和准备消息的一致性主要检查：视图编号v、消息序号n和摘要d。 预准备阶段和准备阶段确保所有正常节点对同一个视图中的请求序号达成一致。接下去是对这个结论的形式化证明：如果prepared(m,v,n,i)为真，则prepared(m&#39;,v,n,j)必不成立，这就意味着至少f+1个正常节点在视图v的预准备或者准备阶段发送了序号为n的消息m。 进入确认阶段 当(m,v,n,i)条件为真的时候，副本i将&lt;COMMIT,v,n,D(m),i&gt;向其他副本节点广播，于是就进入了确认阶段。每个副本接受确认消息的条件是：1）签名正确；2）消息的视图编号与节点的当前视图编号一致；3）消息的序号n满足水线条件，在h和H之间。一旦确认消息的接受条件满足了，则该副本节点将确认消息写入消息日志中。（补充：需要将针对某个请求的所有接受的消息写入日志，这个日志可以是在内存中的）。 接受确认消息需要满足的条件 我们定义确认完成committed(m,v,n)为真得条件为：任意f+1个正常副本节点集合中的所有副本i其prepared(m,v,n,i)为真；本地确认完成committed-local(m,v,n,i)为真的条件为：prepared(m,v,n,i)为真，并且i已经接受了2f+1个确认（包括自身在内）与预准备消息一致。确认与预准备消息一致的条件是具有相同的视图编号、消息序号和消息摘要。 确认被接受的形式化描述 确认阶段保证了以下这个不变式（invariant）：对某个正常节点i来说，如果committed-local(m,v,n,i)为真则committed(m,v,n)也为真。这个不变式和视图变更协议保证了所有正常节点对本地确认的请求的序号达成一致，即使这些请求在每个节点的确认处于不同的视图。更进一步地讲，这个不变式保证了任何正常节点的本地确认最终会确认f+1个更多的正常副本。 故事的终结 每个副本节点i在committed-local(m,v,n,i)为真之后执行m的请求，并且i的状态反映了所有编号小于n的请求依次顺序执行。这就确保了所有正常节点以同样的顺序执行所有请求，这样就保证了算法的正确性（safety）。在完成请求的操作之后，每个副本节点都向客户端发送回复。副本节点会把时间戳比已回复时间戳更小的请求丢弃，以保证请求只会被执行一次。 我们不依赖于消息的顺序传递，因此某个副本节点可能乱序确认请求。因为每个副本节点在请求执行之前已经将预准备、准备和确认这三个消息记录到了日志中，这样乱序就不成问题了。（为什么？） 下图展示了在没有发生主节点失效的情况下算法的正常执行流程，其中副本0是主节点，副本3是失效节点，而C是客户端。 PBFT算法流程 4.3 垃圾回收为了节省内存，系统需要一种将日志中的无异议消息记录删除的机制。为了保证系统的安全性，副本节点在删除自己的消息日志前，需要确保至少f+1个正常副本节点执行了消息对应的请求，并且可以在视图变更时向其他副本节点证明。另外，如果一些副本节点错过部分消息，但是这些消息已经被所有正常副本节点删除了，这就需要通过传输部分或者全部服务状态实现该副本节点的同步。因此，副本节点同样需要证明状态的正确性。 在每一个操作执行后都生成这样的证明是非常消耗资源的。因此，证明过程只有在请求序号可以被某个常数（比如100）整除的时候才会周期性地进行。我们将这些请求执行后得到的状态称作检查点（checkpoint），并且将具有证明的检查点称作稳定检查点（stable checkpoint）。 副本节点保存了服务状态的多个逻辑拷贝，包括最新的稳定检查点，零个或者多个非稳定的检查点，以及一个当前状态。写时复制技术可以被用来减少存储额外状态拷贝的空间开销。 检查点的正确性证明的生成过程如下：当副本节点i生成一个检查点后，向其他副本节点广播检查点消息&lt;CHECKPOINT,n,d,i&gt;，这里n是最近一个影响状态的请求序号，d是状态的摘要。每个副本节点都默默地在各自的日志中收集并记录其他节点发过来的检查点消息，直到收到来自2f+1个不同副本节点的具有相同序号n和摘要d的检查点消息。这2f+1个消息就是这个检查点的正确性证明。 具有证明的检查点成为稳定检查点，然后副本节点就可以将所有序号小于等于n的预准备、准备和确认消息从日志中删除。同时也可以将之前的检查点和检查点消息一并删除。 检查点协议可以用来更新水线（watermark）的高低值（h和H），这两个高低值限定了可以被接受的消息。水线的低值h与最近稳定检查点的序列号相同，而水线的高值H=h+k，k需要足够大才能使副本不至于为了等待稳定检查点而停顿。加入检查点每100个请求产生一次，k的取值可以是200。 4.4 视图变更，改朝换代 使用计时器的超时机制触发视图变更事件 视图变更协议在主节点失效的时候仍然保证系统的活性。视图变更可以由超时触发，以防止备份节点无期限地等待请求的执行。备份节点等待一个请求，就是该节点接收到一个有效请求，但是还没有执行它。当备份节点接收到一个请求但是计时器还未运行，那么它就启动计时器；当它不再等待请求的执行就把计时器停止，但是当它等待其他请求执行的时候再次情动计时器。 转载：http://www.jianshu.com/p/fb5edf031afd]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[椭圆曲线加密算法详解]]></title>
    <url>%2F2018%2F10%2F30%2F%E6%A4%AD%E5%9C%86%E6%9B%B2%E7%BA%BF%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[椭圆曲线加密算法，即：Elliptic Curve Cryptography，简称ECC，是基于椭圆曲线数学理论实现的一种非对称加密算法。相比RSA，ECC优势是可以使用更短的密钥，来实现与RSA相当或更高的安全。据研究，160位ECC加密安全性相当于1024位RSA加密，210位ECC加密安全性相当于2048位RSA加密。 椭圆曲线在密码学中的使用，是1985年由Neal Koblitz和Victor Miller分别独立提出的。 椭圆曲线一般情况下，椭圆曲线可用下列方程式来表示，其中a,b,c,d为系数。 E:y2=ax3+ bx2+cx+d 例如，当a=1,b=0,c=-2,d=4时，所得到的椭圆曲线为: E:y2=x3-2x+4 该椭圆曲线E的图像如图X-1所示，可以看出根本就不是椭圆形。 定义椭圆曲线的运算规则加法过曲线上的两点A、B画一条直线，找到直线与椭圆曲线的交点，交点关于x轴对称位置的点，定义为A+B，即为加法。如下图所示：A + B = C 二倍运算上述方法无法解释A + A，即两点重合的情况。因此在这种情况下，将椭圆曲线在A点的切线，与椭圆曲线的交点，交点关于x轴对称位置的点，定义为A + A，即2A，即为二倍运算。 正负取反将A关于x轴对称位置的点定义为-A，即椭圆曲线的正负取反运算。如下图所示： 无穷远点如果将A与-A相加，过A与-A的直线平行于y轴，可以认为直线与椭圆曲线相交于无穷远点。 综上，定义了A+B、2A运算，因此给定椭圆曲线的某一点G，可以求出2G、3G（即G + 2G）、4G……。即：当给定G点时，已知x，求xG点并不困难。反之，已知xG点，求x则非常困难。此即为椭圆曲线加密算法背后的数学原理。 有限域上的椭圆曲线运算椭圆曲线要形成一条光滑的曲线，要求x,y取值均为实数，即实数域上的椭圆曲线。但椭圆曲线加密算法，并非使用实数域，而是使用有限域。按数论定义，有限域GF(p)指给定某个质数p，由0、1、2……p-1共p个元素组成的整数集合中定义的加减乘除运算。 假设椭圆曲线为y² = x³ + x + 1，其在有限域GF(23)上时，写作： y² ≡ x³ + x + 1 (mod 23) 此时，椭圆曲线不再是一条光滑曲线，而是一些不连续的点，如下图所示。以点(1,7)为例，7² ≡ 1³ + 1 + 1 ≡ 3 (mod 23)。如此还有如下点： (0,1) (0,22) (1,7) (1,16) (3,10) (3,13) (4,0) (5,4) (5,19) (6,4) (6,19) (7,11) (7,12) (9,7) (9,16) (11,3) (11,20) 等等。 另外，如果P(x,y)为椭圆曲线上的点，则-P即(x,-y)也为椭圆曲线上的点。如点P(0,1)，-P=(0,-1)=(0,22)也为椭圆曲线上的点。 计算xG 相关公式如下： 有限域GF(p)上的椭圆曲线y² = x³ + ax + b，若P(Xp, Yp), Q(Xq, Yq)，且P≠-Q，则R(Xr,Yr) = P+Q 由如下规则确定： Xr = (λ² - Xp - Xq) mod p Yr = (λ(Xp - Xr) - Yp) mod p 其中λ = (Yq - Yp)/(Xq - Xp) mod p（若P≠Q）, λ = (3Xp² + a)/2Yp mod p（若P=Q） 因此，有限域GF(23)上的椭圆曲线y² ≡ x³ + x + 1 (mod 23)，假设以(0,1)为G点，计算2G、3G、4G…xG等等，方法如下： 计算2G： λ = (3x0² + 1)/2x1 mod 23 = (1/2) mod 23 = 12 Xr = (12² - 0 - 0) mod 23 = 6 Yr = (12(0 - 6) - 1) mod 23 = 19 即2G为点(6,19) 计算3G： 3G = G + 2G，即(0,1) + (6,19) λ = (19 - 1)/(6 - 0) mod 23 = 3 Xr = (3² - 0 - 6) mod 23 = 3 Yr = (3(0 - 3) - 1) mod 23 = 13 即3G为点(3, 13) 同理计算4G、5G…xG，分布如下图： [图片上传失败…(image-2d5c43-1526642990683)] 椭圆曲线加解密算法原理 建立基于椭圆曲线的加密机制，需要找到类似RSA质因子分解或其他求离散对数这样的难题。而椭圆曲线上的已知G和xG求x，是非常困难的，此即为椭圆曲线上的的离散对数问题。此处x即为私钥，xG即为公钥。 椭圆曲线加密算法原理如下： 设私钥、公钥分别为k、K，即K = kG，其中G为G点。 公钥加密： 选择随机数r，将消息M生成密文C，该密文是一个点对，即： C = {rG, M+rK}，其中K为公钥 私钥解密： M + rK - k(rG) = M + r(kG) - k(rG) = M 其中k、K分别为私钥、公钥。 椭圆曲线签名算法原理 椭圆曲线签名算法，即ECDSA。 设私钥、公钥分别为k、K，即K = kG，其中G为G点。 私钥签名： 1、选择随机数r，计算点rG(x, y)。 2、根据随机数r、消息M的哈希h、私钥k，计算s = (h + kx)/r。 3、将消息M、和签名{rG, s}发给接收方。 公钥验证签名： 1、接收方收到消息M、以及签名{rG=(x,y), s}。 2、根据消息求哈希h。 3、使用发送方公钥K计算：hG/s + xK/s，并与rG比较，如相等即验签成功。 原理如下： hG/s + xK/s = hG/s + x(kG)/s = (h+xk)G/s = r(h+xk)G / (h+kx) = rG 代码实现：1234567891011121314151617181920212223242526272829303132333435363738394041424344package mainimport ( &quot;crypto/ecdsa&quot; &quot;crypto/elliptic&quot; &quot;crypto/rand&quot; &quot;crypto/sha256&quot; &quot;math/big&quot; &quot;fmt&quot;)//通过椭圆曲线完成签名和验证func main() &#123; //声明明文 message := []byte(&quot;hello world&quot;) //生成私钥 privateKey, _ := ecdsa.GenerateKey(elliptic.P256(), rand.Reader) //生成公钥 pub := privateKey.PublicKey //将明文散列 digest := sha256.Sum256(message) //签名 r, s, _ := ecdsa.Sign(rand.Reader, privateKey, digest[:]) //设置私钥的参数类型为曲线类型 param := privateKey.Curve.Params() //获得私钥byte长度 curveOrderByteSize := param.P.BitLen() / 8 //获得签名返回值的字节 rByte, sByte := r.Bytes(), s.Bytes() //创建数组 signature := make([]byte, curveOrderByteSize*2) //通过数组保存了签名结果的返回值 copy(signature[curveOrderByteSize-len(rByte):], rByte) copy(signature[curveOrderByteSize*2-len(sByte):], sByte) //认证 //将明文做hash散列，为了验证的内容对比 digest = sha256.Sum256(message) curveOrderByteSize = pub.Curve.Params().P.BitLen() / 8 //创建两个整形对象 r, s = new(big.Int), new(big.Int) //设置证书值 r.SetBytes(signature[:curveOrderByteSize]) s.SetBytes(signature[curveOrderByteSize:]) ​ //认证e := ecdsa.Verify(&amp;pub, digest[:], r, s)if e == true { fmt.Println(“OK”)} else { fmt.Println(“failed”)}​12 &#125;]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>加密算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链100篇之第九篇--默克尔树]]></title>
    <url>%2F2018%2F10%2F25%2F%E5%8C%BA%E5%9D%97%E9%93%BE100%E7%AF%87%E4%B9%8B%E7%AC%AC%E4%B9%9D%E7%AF%87-%E9%BB%98%E5%85%8B%E5%B0%94%E6%A0%91%2F</url>
    <content type="text"><![CDATA[此区块链100篇文章为在博客上看到较好的区块链解释，所以搜集而来，转载博客原地址https://blog.csdn.net/weixin_37504041 中本聪在他的创世论文中一个概念，就是SPV，中文意思是简单支付验证，从这里我们可以看出SPV指的是“支付验证”而不是“交易验证”，那这两者有什么区别吗？简单的说就是支付验证只需验证该笔交易是否被确认过了，而交易验证是需要验证该笔交易是否满足一些条件如“余额”是否足够，还有该笔交易有没有存在双花等等一些问题，只有一切都没什么问题后该笔交易才算验证通过，可以看出交易验证要比支付验证更加复杂，所以它一般是由挖矿节点来完成的，而支付验证只要普通的轻钱包就可以完成。那现在有一个问题了，SPV是如何实现的？答案就是默克尔树，也就是今天要讲的主题，理解了默克尔树之后在回过头来就能理解SPV了。 什么是Merkle Tree？ 默克尔树是一种二叉树，由一组叶节点、一组中间节点和一个根节点构成，看下图： 来简单讲一下这幅图，我们从最底部开始看，D0、D1、D2和D3是叶子节点包含的数据，也就是叶子节点的value，继续往上看，N0、N1、N2和N3是就是叶子节点，它是将数据（也就是D0、D1、D2和D3）进行hash运算后得到的hash值；继续往上看，N4和N5是中间节点，它们各是N0和N1经过hash运算得到的哈希值以及N2和N3经过hash运算得到的哈希值，注意，它们是把相邻的两个叶子结点合并成一个字符串，然后运算这个字符串的哈希；接着往上，Root节点是N4和N5经过hash运算后得到的哈希值，这就是这颗默克尔树的根哈希。 分析到这里我们大概可以知道在默克尔树中最下面的大量的叶节点包含基础数据；每个中间节点是它的两个叶子节点的哈希，根节点也是由它的两个子节点的哈希，代表了默克尔树的顶部。 还有从默克尔树的结构可以看出，任意一个叶子节点的交易被修改，叶子节点hash值就会变更，最终根节点的hash值就会改变。所以确定的根节点的hash值可以准确的作为一组交易的唯一摘要。 现在可以总结一下默克尔树的特点：1.首先是它的树的结构，默克尔树常见的结构是二叉树，但它也可以是多叉树，它具有树结构的全部特点。 2.默克尔树的基础数据不是固定的，想存什么数据由你说了算，因为它只要数据经过哈希运算得到的hash值。 3.默克尔树是从下往上逐层计算的，就是说每个中间节点是根据相邻的两个叶子节点组合计算得出的，而根节点是根据两个中间节点组合计算得出的，所以叶子节点是基础。 如何通过Merkle树验证一笔交易？ 大概了解了什么是默克尔树后，可能会有一个疑问，就是默克尔树是如何验证一笔交易的？也就是我们上文提到的SPV（支付验证）。看下面一幅图: 假设我们要验证区块中存在Hash值为9Dog:64（绿色框）的交易，我们仅需要知道1FXq:18、ec20、8f74（黄色框）即可计算出781a、5c71与Root节点（藕粉色框）的哈希，如果最终计算得到的Root节点哈希与区块头中记录的哈希（6c0a）一致，即代表该交易在区块中存在。这是因为我上文提到的两个点，一个是默克尔树是从下往上逐层计算的，所以只要知道相邻的另一个节点的hash值就可以一直往上计算直到根节点，另一个是根节点的hash值可以准确的作为一组交易的唯一摘要，依据这两点就可以来验证一笔交易是否存在。 比特币中的默克尔树 介绍完默克尔树的基本知识，我们来看一下比特币中的默克尔树长什么样，看下面一幅图： 可以看到区块头包含了根节点的hash值，而中间节点、叶子节点还有基础数据在放在了区块体中。 这里有一点需要提的就是在比特网络中的Merkle树是二叉树，所以它需要偶数个叶子节点。如果仅有奇数个交易需要归纳，那最后的交易就会被复制一份以构成偶数个叶子节点，这种偶数个叶子节点的树也被称为平衡树。 默克尔树的典型应用场景 默克尔树的应用场景其实很广泛，比较典型的就是P2P下载。在点对点网络中作数据传输的时候，会同时从多个机器上下载数据，而且很多机器可以认为是不稳定或者不可信的。为了校验数据的完整性，更好的办法是把大的文件分割成小的数据块（例如，把分割成2K为单位的数据块）。这样的好处是，如果小块数据在传输过程中损坏了，那么只要重新下载这一快数据就行了，不用重新下载整个文件。 怎么确定小的数据块没有损坏哪？只需要为每个数据块做Hash。BT下载的时候，在下载到真正数据之前，我们会先下载一个Hash列表。那么问题又来了，怎么确定这个Hash列表本事是正确的哪？答案是把每个小块数据的Hash值拼到一起，然后对这个长字符串在作一次Hash运算，这样就得到Hash列表的根Hash。下载数据的时候，首先从可信的数据源得到正确的根Hash，就可以用它来校验Hash列表了，然后通过校验后的Hash列表校验数据块。 除了P2P下载外，默克尔树还可以被用来快速比较大量的数据，因为当两个默克尔树根相同时，则意味着所代表的数据必然相同。还有就是可以用来实现零知识证明（零知识证明指的是证明者能够在不向验证者提供任何有用的信息的情况下，使验证者相信某个论断是正确的。举个例子，你要我向你证明我拥有某一把钥匙，这个时候我不需要直接拿钥匙给你看，而是用这个钥匙开锁拿出所在柜子中的某一样东西给你看以此来证明我拥有这把钥匙），关于零知识证明以后有时间再讲，ZCash就是采用零知识证明来达到交易匿名的目的，有兴趣可以去查找资料。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链100篇之第八篇--智能合约]]></title>
    <url>%2F2018%2F10%2F25%2F%E5%8C%BA%E5%9D%97%E9%93%BE100%E7%AF%87%E4%B9%8B%E7%AC%AC%E5%85%AB%E7%AF%87-%E6%99%BA%E8%83%BD%E5%90%88%E7%BA%A6%2F</url>
    <content type="text"><![CDATA[此区块链100篇文章为在博客上看到较好的区块链解释，所以搜集而来，转载博客原地址https://blog.csdn.net/weixin_37504041 一、比特币的转账过程 我在第五篇讲UTXO的时候，有提到比特币的转账过程，现在来回顾一下这个过程。 Bob想要转给Jack一百个比特币，Bob需要先创建一笔交易，因为这笔交易只有被矿工验证并打包进区块的时候才算完成交易，Bob在填写交易信息的时候除了需要提供支付比特币的数额以及双方的地址外，还需要提供自己的公钥以及用私钥生成的数字签名，还有就是上一笔交易的Hash（也就是Bob从哪里得到这些比特币）；信息填完后（其实在现实中没这么复杂，只需要在钱包中填写转入转出地址以及交易金额即可，其他的都是自动的）便可以点击确认，当这笔交易广播到全网中时，矿工开始对这笔交易进行验证，第一步会去找到上一笔交易，确认支付方的比特币来源；接着第二步算出支付方公钥的指纹，确认与支付方的地址一致，从而保证公钥属实；然后第三步是使用公钥去解开数字签名，保证私钥属实。验证通过后矿工便会将这笔交易打包进区块中，如果得到六次以上的确认，则可以认为这笔交易不会再被改变。 讲这个过程就是为了引出今天的知识点–智能合约。 二、比特币与智能合约 在上面我有讲到Bob需要提供的交易信息有一个是上一笔的交易Hash，我们可以来看一下区块链中时如何记录这笔交易的（这其实就是一个UTXO），如下图所示： 可以看出Bob转给Jack的100个比特币是从Alice那得到的，图中还可以看到一个解锁信息，这个解锁信息时候用来证明这100个比特币的归属，因为只有Bob的私钥才能进行解锁，其他人进行解锁是无效的。到这里我们可以将这个UTXO看做是一个简单的智能合约，因为它是一个可自动执行并且自我验证协议，Bob只要输入自己的私钥，解锁脚本就会自动运行校验，解锁成功便说明Bob是这100个比特币的拥有者，反之则说明不是。 因为比特币所支持的脚本语言只跟交易有关，并且不是图灵完备的语言，所能做的事很有限，但是比特币的出现极大地促进了智能合约的发展，现在支持编写智能合约的平台中最有名的当属以太坊。 三、智能合约 上世纪90年代，密码学家尼克萨博从自动贩卖机得到灵感首次提出“智能合约”的概念，看一下智能合约的定义： 智能合约是指一种计算机协议，这类协议一旦制定和部署成功就能实现自我执行（self-executing）和自我验证（self-verifying），而且不再需要人为的干预。 智能合约会对接收到的信息进行回应，它可以接收和储存价值，也可以向外发送信息和价值。看一张图： 从图中可以看出一段代码（智能合约），被部署在分享的、复制的账本上，它可以维持自己的状态，控制自己的资产和对接收到的外界信息或者资产进行回应。举个例子，现在我们对银行账户内存款的操作都需要中心化的银行进行授权，一旦离开了银行的监管，用户就连最基本的存取款操作都无法进行，但是如果是使用智能合约来处理的话情况就不一样了，只要事先正确的通过严谨的逻辑写好代码，便可以不需要人工参与，一切都将按事先写好的逻辑运行，并且结果是公平公正的，因此也有人宣称“代码即法律”，这个观点保留意见，至少以目前的情况来看事情没那么简单。 智能合约的概念虽然很早就被提出来了，但是因为很多技术的不成熟，所以发展很缓慢，但是因为比特币的出现或者说因为区块链技术的出现，智能合约开始成为研究人员与业内人士重点研究的对象，这都极大促进了智能合约的发展。 四、智能合约的优缺点 参考《区块链技术指南》简单列出智能合约的优点。 首先是高效的实时更新，因为智能合约的执行是去中心化的，也就是不需要认为的干预，所以它的执行效率是很高的，你不再需要亲自去相关部门提交排队提交申请资料，然后还需要等几个工作日才能得到结果，现在只需要在网上填好资料点击提交，如果网络通畅，几分钟内便能办完手续，方便快捷。第二便是较低的人为干预风险，因为智能合约是一开始就制定好的，并且是无法更改的，所以一旦出现毁约的情况，那么时间的责任人就会得到相应的惩罚，这保证了公平公正性，也就是说在智能合约面前，人人平等，没有谁有特权。第三就是准确运行， 智能合约是一段执行在计算机上的代码，所以只要运行的计算机没错县错误，那么这个合约的执行结果都是准确无误的，不会出现不可预料的情况，之所以能做到这一点也是得益于密码学的发展和区块链技术的发明。第四是去中心化权威，在区块链网络中一般不存在一个绝对的权威来监督合约的执行，而是由绝大部分的用户来判断合约是否正常执行，这种绝大多数人监督的方式是由POW或POS等共识机制来实现的。 第五便是较低的运行成本，正因为智能合约具有去人为干预的特点，其能够大大减少合约履行、裁决和强制执行所产生的人力成本。 关于智能合约的缺点，我讲一下我的理解，因为智能合约的一段可执行的代码，是代码便不可避免的会存在bug，一旦出现bug或者不可预料的情况，这对于采用智能合约的应用都是极大的挑战（可以参考The DAO事件），这是因为智能合约是去人工干预的。智能合约的优点目前也是它的缺点，但是随着技术的发展，它的这个缺点或许可以被忽略。 五、以太坊与智能合约 说到智能合约就不能不讲到以太坊，与比特币相比，以太坊最大的不同点是它可以支持更加强大的脚本语言（图灵完备的脚本语言），允许开发者在上面开发任意应用，实现任意智能合约，这也是以太坊的最强大之处。作为平台，以太坊可以类比于苹果的应用商店，任何开发者都可以在上面开发应用，并出售给用户。以太坊也是目前支持智能合约的区块链平台中比较成熟的平台。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链100篇之第七篇--比特币的分叉]]></title>
    <url>%2F2018%2F10%2F25%2F%E5%8C%BA%E5%9D%97%E9%93%BE100%E7%AF%87%E4%B9%8B%E7%AC%AC%E4%B8%83%E7%AF%87-%E6%AF%94%E7%89%B9%E5%B8%81%E7%9A%84%E5%88%86%E5%8F%89%2F</url>
    <content type="text"><![CDATA[此区块链100篇文章为在博客上看到较好的区块链解释，所以搜集而来，转载博客原地址https://blog.csdn.net/weixin_37504041 遵循相同机制的分叉 我在第六篇讲到了一种情况就是在比特币网络中，在某一时刻有两个矿工同时算出随机数（即获得记账权），那么这个时候便会出现一种情况，即一条主链叉开变成两条叉链，每条链沿着各自的方向延伸下去，如下图所示： 之所以会出现这种情况是因为每个矿工在开始挖新的区块之前都会先把上一个区块复制过来，然后接上这个区块进行下一个区块的挖矿工作。而现在在全网中的同一时刻有两个符合条件的区块，那么由于距离的远近（P2P网络），矿工找到的区块是不一样的（虽然这两个区块都是符合条件的区块），所以有一些矿工会拿到橙色的区块进行下面的挖矿工作，而有一些矿工会拿到蓝色的区块进行挖矿工作，这就势必导致上图所示的情况，这种情况叫分叉。对于此类情况，比特币有一个很好的机制，就是上文提到的短链服从长链，那么为了达到这种效果比特币是如何做的呢？首先区块是由矿工挖出来的，矿工进行挖矿就需要矿机（以后会专门讲矿机），而不同的矿机具有不同的算力（每秒运算多少次），也就是说每个矿工的计算能力是不一样的，那么，就会出现一种情况，即两条链中会有一条链的算力要比另一条链大，因此这条链的增长速度就会比另一条链要快，也就是这条链会比另一条链长。这时你可能会说那如果两条链的算力一直保持一样呢？这种情况是不可能会出现的，在短时间内是可能的，但一段时间后这种平衡势必会被打破，比如有新的矿机出来（新的矿机算力一般都比较高），那么就会有矿工去购买来替换旧的矿机，这就会导致算力不平衡。 但其中一条链超过另外一条链时，这时全网中就会出现一条最长链，那么矿工在进行新的打包区块工作的时候会把最长链全部复制过来，再在这条链的基础上继续挖矿，当所有矿工都这样做的时候那么这条链就会成为一条主链，而另外一条链就会被抛弃掉，如下图所示： 到这里可能会有一个疑问，就是如果矿工执意要在那条短链上进行挖矿呢？如果这个矿工聪明一点的话他就知道这么做是吃力不讨好的，因为一旦他所在的链最后没有成为主链的话，他在这条连上挖到的比特币都会归零，他之前所做的工作都白费的，所以一般不会去做这种事。好了，到这里或许能明白比特币是如何保证系统中账本唯一性的了，但是需要注意的是这里所有的矿工都是遵循相同的机制，比如每条链的每个区块都是1M大小。那么如果一条链1M大小一条链是8M大小呢？比特币系统还会不会同样的保留最长链丢弃短链呢？继续往下看。 遵循不同机制的分叉 如果是遵循不同的机制也会出现分叉，一般分为软分叉和硬分叉。 1.软分叉软分叉好理解一点，就是当系统进行升级时（比如比特币将一个区块的大小从1M扩容到8M），有一些矿工还没来得及升级，那么就会出现一条链是1M的，一条链是8M。这种情况也好解决，只需要这部分矿工进行升级就行了，那么那条1M的链就会自动消失，而且对全网没有任何的影响。这里可以把软分叉理解为一个人换了一件新的衣服，人还是那个人，只是样子看起来变了。 2.硬分叉当整个区块链网络中，系统版本或协议升级后，且和老版本协议不兼容，未升级的老节点无法接受新节点挖出的全部或者部分区块，导致出现了两条链，假设新节点的算力较大，新节点们在维护一条链，老节点也始终在维护一条他认可的链，如果这时候大多数的节点都开始升级为新版本，那么老节点维护的链能不能存活就看算力有多少了，这就称作硬分叉。直白点就是一条链真的分叉成两条链，且这两条链互不兼容，是两条独立的主链，这两条链唯一的联系就是在分叉前的数据都是一样，而分叉后全都不一样，举个例子就是一块地基上原来有了一座旧房子，这时在这块地基上重新建另一座房子，建成后两者除了在同一块地基上之外就没有任何联系了。 关于硬分叉最有名的当属以太坊的The DAO事件，感兴趣的可以自行查资料，这件事导致了以太坊分叉形成两条不同的主链，一条是以太坊（ETH），一条是以太经典（ETC），现在两条链都同时存在。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链常见问题汇总]]></title>
    <url>%2F2018%2F10%2F24%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[1.问：什么是区块链？ 答：区块链（Blockchain）是指通过去中心化和去信任的方式集体维护一个可靠数据库的技术方案。 2.问：能通俗的解释一下什么是区块链吗？ 答：通俗一点说，区块链技术就指一种全民参与记账的方式。所有的系统背后都有一个数据库，你可以把数据库看成是就是一个大账本。那么谁来记这个账本就变得很重要。目前就是谁的系统谁来记账，微信的账本就是腾讯在记，淘宝的账本就是阿里在记。但现在区块链系统中，系统中的每个人都可以有机会参与记账。在一定时间段内如果有任何数据变化，系统中每个人都可以来进行记账，系统会评判这段时间内记账最快最好的人，把他记录的内容写到账本，并将这段时间内账本内容发给系统内所有的其他人进行备份。这样系统中的每个人都了一本完整的账本。这种方式，我们就称它为区块链技术。 3.问：这样全民记账的区块链有什么好处？ 答：可以发现，这是在牺牲一点效率的情况下，获得了极大的安全性。首先没有一本中央大账本了，所以无法摧毁。每个节点都仅仅是系统的一部分，每个节点权利相等，都有着一模一样的账本。摧毁部分节点对系统一点都没有影响。其次，无法作弊，因为除非你能控制系统内大多数人的电脑都进行修改，否则系统会参照多数人的意见来决定什么才是真实结果，结果会发现修改自己的账本完全没有意义（因为别人不承认）。其次，由于没有中心化的中介机构存在，让所有的东西都通过预先设定的程序自动运行，不仅能够大大降低成本，也能提高效率。而由于每个人都有相同的账本，能确保账本记录过程是公开透明的。 4.问：区块链解决了什么问题吗？ 答：区块链最重要的是解决了中介信用问题。在过去，两个互不认识和信任的人要达成协作是难的，必须要依靠第三方。比如支付行为，在过去任何一种转账，必须要有银行或者支付宝这样的机构存在。但是通过区块链技术，比特币是人类第一次实现在没有任何中介机构参与的情况下，完成双方可以互信的转账行为。这是区块链的重大突破。 5.问：区块链是比特币吗？或者比特币就是区块链吗？ 答：区块链技术是比特币的底层技术，在早期并没有太多人注意到比特币的底层技术。但是当比特币在没有任何中心化机构运营和管理的情况下，在多年里非常稳定的运行，并且没有出现过任何问题。所以很多人注意到，该底层技术技术也许有很大的机制，而且不仅仅可以在比特币中使用，也许可以在许多领域都能够应用这种技术。于是把比特币技术抽象提取出来，称之为区块链技术，或者分布式账本技术。所以从某个角度来看，比特币可以看成是区块链第一个应用，而区块链更类似于TCP/IP这样的底层技术，以后会扩展到越来越多的行业中。 6.问：区块链技术主要可以用在哪些行业？ 答：区块链主要的优势是无需中介参与、过程高效透明且成本很低、数据高度安全。所以如果在这三个方面有任意一个需求的行业都有机会使用区块链技术。 7.问：金融领域为什么要使用区块链技术？有什么实质性的好处？ 答：区块链技术在金融领域中主要的优势去中介化和极大的降低成本。 首先金融行业目前由于防止单点故障和系统性风险，需要进行层层审计来控制金融风险，但由此也造成高昂的内部成本。并且由于不断增加的监管法规出现，特别是2008年金融危机导致对于金融管控门槛不断升高，而反恐战争导致反洗钱和反恐怖主义融资的范围也让监管的广度和深度逐渐扩大，导致整个金融系统的监管成本急剧增加。在这种情况下，区块链技术能够通过防篡改和高透明的方式让真个金融系统极大的降低成本。根据西班牙最大银行桑坦德发布的一份报告显示，2020年左右如果全世界的银行内部都使用区块链技术的话，大概每年能省下200亿美元的成本。这样的数据足以说明“区块链”给传统金融领域带来的巨大变革和突破。 此外由于历史原因，导致传统金融机构在结算和清算时都依靠中央结算所来完成，而由此造成的问题就是效率低下。传统的跨国结算就是因为要通过类似于SWIFT这样的机构，所以跨国电汇往往是按天来计算的。但是比特币在使用区块链技术时，在完全没有中心化运营机构的情况下，完美的运行了七年，不仅能够实现实时结算和清算，而且没有出现过任何一笔账目错误。所以，如果所有的金融系统能够实现去中心化的实时结算和清算，不仅仅将极大的提高全球金融效率，并且由此能够改变全球金融的格局。 8.问：什么是比特币说的“挖矿”？ 答：比特币中的“挖矿”实际上就是记账的过程，比特币的运算采用了一种称为“工作量证明（Proof of Work，PoW）”的机制，系统为了找出谁有更强大的计算能力，每次会出一道数学题，只有最快解出这道题目的计算机才能进行记账。而抢到记账权的计算机会获得25个比特币的奖励。通常把这个行为称为“挖矿”，把获得的比特币视为挖矿成功获得的奖励。 9.问：所有的区块链都需要挖矿吗？ 答：并非所有的区块链项目都会采用类似于比特币这样的“工作量证明”方式，这更多出现在早期的区块链项目中。如果采取其他的证明机制，如“权益证明（Proof of Stake，PoS）”、“股份授权证明机制（DPoS，Delegate Proof of Stake）”都是不需要采取这样的挖矿方式。 10.问：区块链和大数据什么关系？区块链会取代大数据？ 答：区块链和大数据关系并不是很大。大数据主要的是对于海量数据进行管理，而区块链的核心是在没有中心化中介计入的情况下实现数据的高安全性和高可靠性。所以区块链和大数据并不互相冲突，也不会取代，完全是面对不同场景情况下对于数据的不同解决方案。 11.问：区块链和云计算云存储有什么关系？区块链是云计算或云存储吗？ 答：云计算通常定义为通过互联网来提供动态易扩展且经常是虚拟化的资源，但是提供云计算平台的往往是一个中心化机构。而区块链组成的网络一般是没有特定的机构，所以区块链更接近分布式计算系统的定义，属于分布式计算的一种。不过，区块链是能够实现云存储的，不同于目前中心化提供云存储空间，区块链有一些提供去中心化的云存储方案。这样的项目包括Storj，Sia，Maidsafe。 12.问：区块链是软件吗？是用什么程序写的？ 答：区块链不是一种特定的软件，就像“数据库”这个三个字表现的意思一样，它是一种特定技术的设计思想。可以用绝大多数语言来实现它，而且实现的方式也有许多种。而且区块链技术目前还在快速发展中，相对而言，目前区块链技术设计思想还是比较简单的，也许在未来会变得愈加复杂。 13.问：什么是公有链？什么是私有链？什么是联盟链？ 答：公有链是任何节点都是向任何人开放的，每个人都可以参与到这个区块链中参与计算，而且任何人都可以下载获得完整区块链数据（全部账本）。但是有些区块链的应用场景下，并不希望这个系统任何人都可以参与，任何人都可以查看所有数据，只有被许可的节点才可以参与并且查看所有数据。那么这种区块链结构我们称为私有链。 联盟链是指参与每个节点的权限都完全对等，大家在不需要完全互信的情况下就可以实现数据的可信交换，R3组成的银行区块链联盟要构建的就是典型的联盟链。 但是随着区块链技术的快速发展，不排除以后公有链和私有链的界限会变得比较模糊。因为每个节点的可以有较为复杂的读写权限，也许有部分权限的节点会向所有人开发，而部分记账或者核心权限的节点只能向许可的节点开放，那就会不再是纯粹的公有链或者私有链。 14.问：目前区块链技术发展的主要问题？ 答：目前区块链技术还处于一个非常早期的阶段，不仅尚未形成统一的技术标准，而且各种技术方案还在快速发展中。但是过去被认为基于区块链技术的系统会非常耗费资源（类似于比特币），或者区块链技术的系统处理数据有限制之类的问题已经在技术上获得了突破。但是，对于区块链技术的可扩展性，还没有经过大规模的实践考验，而现在主要还停留在原型设计阶段。 如果不能定量分析，使用区块链技术能够为我们带来的实际好处，包括能够节省的资金和创造的价值，那么金融行业短期内还会保持相对谨慎的态度。毕竟，目前全球金融的基础设施投入已经超过数万亿，要建立一套全新的金融架构和底层操作体系是需要有实际数据相支撑的。在现有技术还没有被部署并且获得使用案例的情况下，能节省下的总金额还是很难确定的。这到目前为止，还是一个巨大的疑问存在，就是到底需要多少资金才能建立一个足够强大的区块链来平台处理，资本市场生态系统每天需要面对的万亿数量级的美元。 此外区块链行业极其缺乏人才，缺少大量既了解区块链技术，又了解金融的多方面人才，市场正在拼命寻找可以连接两个世界的人才，需要能够在现实世界中，将区块链技术能够在资本市场中实现，并且实现更好的功能。而需要建立基于区块链技术的全新系统，必然是需要这样的跨界人才。 15.问：什么是智能合约？ 答：智能合约是一种用计算机语言取代法律语言去记录条款的合约。智能合约可以由一个计算系统自动执行。如果区块链是一个数据库，智能合约就是能够使区块链技术应用到现实当中的应用层。传统意义上的合同一般与执行合同内容的计算机代码没有直接联系。纸质合同在大多数情况下是被存档的，而软件会执行用计算机代码形式编写的合同条款。智能合约的潜在好处包括降低签订合约、执行和监管方面的成本；因此，对很多低价值交易相关的合约来说，这是极大降低人力成本。 16.问：智能合约怎么用？ 答：央行如果能够通过区块链来发行法币，那么也可以通过智能合约技术，将代码嵌入到法币发行的行为中，则这部分法币可以被称为“可编程货币”。比如，如果央行指定某一部分资金是发放到农业相关的账户，那么则可以对这部分资金写入相应程序，指定该部分资金只能进入到农业相关的账户中，那么这部分资金在任何情况下也不可能会被挪用到其他的账户中。如果大部分货币都成为“可编程货币”，那么我们则可以想象到，他们组成的金融环境就变成了“可编程金融”。 17.问：区块链和普通人有什么关系？ 答：基本上没什么关系，除非是准备从事这方面的创业。就和TCP/IP协议和普通人之间的关系，普通人完全不需要知道什么是互联网底层的TCP/IP协议，只要享受互联网提供的服务就行。 18.问：区块链项目是否一定需要有某种币出现？ 答：不是。比特币本身是作为一种支付系统，所以它需要有一个价值度量的工具，所以必须要有bitcoin出现。此外，为了奖励有更多人愿意贡献自己的计算机来为系统提供计算，所以需要有bitcoin来进行奖励。而在一些私有链的系统，可以设计专门的资产进行交易，而每个节点都是必须参与计算，这是他们的责任也是他们的权利，所以不用考虑通过奖励的方式来鼓励他们参与，所以在这样的系统里面就可能不再需要设计某种币的存在。 19.问：比特币现在合法了吗？ 答：比特币在主要的世界大国，包括中国在内一直都是完全合法的。由于某些不良媒体的误导，使很多人以为中国曾经宣布过比特币非法。事实上，根据2013年12月5日，中国人民银行等五部委发布的防范比特币风险的通知中明确规定，比特币是一种特定虚拟商品，普通民众在自担风险的前提下拥有参与的自由。而各类金融机构和支付机构不得开展比特币相关的金融服务，或者将比特币作为投资标的。 比特币在德国作为货币单位，在美国定义为大宗商品。欧盟法院认为比特币为一种支付手段，无需征收增值税。 20.问：XX币可以投资吗？是区块链项目吗？是传销吗？ 答：目前包括比特币在内的所有数字货币都具有很高的风险，区块链技术本身在刚刚起步阶段，所有的区块链项目也都具有非常高的风险。不建议任何普通人投资任何数字货币和区块链相关的项目。并且数字货币和区块链具有一定的技术门槛，普通人无法区分哪些是真实的项目，哪些是传销项目。所以普通人建议不要投资任何这类的项目。对于任何你无法分辨是否是传销的项目，请直接视为传销项目。 21.问：如何投资区块链？ 答：大多数区块链都处于起步阶段，而主要都是在海外，国内好的区块链项目非常非常少，所以不建议任何非专业人士投资区块链项目。如果对区块链技术很有兴趣，自己有技术或者金融相关的背景，建议可以考虑在这方面进行创业。 22.问：区块链/比特币到底是谁发明的？中本聪是不是日本人？是不是美国政府的阴谋？ 答：比特币是一个自称为“中本聪”的人或者团队创造的，并且在比特币项目初期就已经完全退出了这个项目。“中本聪”是日本人的可能性非常小，因为他过去的电子邮件中可以推测出，他应该是一个以英语为母语的人。 此外比特币创造者对于目前比特币的项目已经完全没有影响力，所以不太可能是某个阴谋的产物。无论“中本聪”在之后是否会出现，或者在肉体上被消灭都无法影响对比特币产生太多的影响。 23.问：比特币和Q币到底有什么区别？ 答：Q币是一种中心化的电子货币，包括总量，发行方式都是由腾讯公司控制的。而比特币的总量，发行方式都是由程序和加密算法预先设定后，在全世界的多个节点上运行，没有任何人和机构可以修改，不受任何单一人或者机构来控制。一般称Q币为电子货币，或者企业代币。称比特币为数字货币或者加密数字货币。 24.问：比特币总量是有上限的吗？是怎么分配的？ 答：如同前面所说，矿工参与争夺记账权是有机会获得奖励的。在开始的时候是每10分钟系统会奖励记账最快最好的人50个比特币，然后这50个每四年减半，差不多在2140年的时候就不再有新的比特币出现，将会达到2100万个的上限。在这之后，将会使用交易手续费来奖励矿工。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链100篇之第六篇-共识机制]]></title>
    <url>%2F2018%2F10%2F24%2F%E5%8C%BA%E5%9D%97%E9%93%BE100%E7%AF%87%E4%B9%8B%E7%AC%AC%E5%85%AD%E7%AF%87-%E5%85%B1%E8%AF%86%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[此区块链100篇文章为在博客上看到较好的区块链解释，所以搜集而来，转载博客原地址https://blog.csdn.net/weixin_37504041 共识 共识的英文是Consensus，也可以翻译成一致或一致同意。维基百科对于共识的定义是：共识是指分歧双方搁置争议，达成能够被各方所接受的陈述（即使有时只是勉强接受）的社群解决方案。简单的说就是在一个群体中就某一个问题达成共识，一致同意采取某一种策略来解决这个问题，举个例子： 现在有一个班级，大家需要就什么时候拍毕业照进行投票，有两个方案，方案一是在5月1号拍毕业照，方案二是在论文答辩后拍，这时大家进行投票决定，假设结果大多数人选择了方案一，那么最终大家就需要无条件服从这个决定。这里什么时候拍毕业照是一个问题，而在决定五月初拍这是解决这个问题的一个策略，到这里对应上面共识的定义就大概能明白是什么是共识了。 这里有一点要注意的就是并不是全部的人都选择方案一，那么为什么最终结果是选择方案一而不是方案二呢，这是因为大家在此之前就已经达成了另一个共识，就是“少数服从多数”，也就是说我们是在已经有了一个共识的前提下达成另一个共识。 共识机制 上面的共识应该很容易理解，讲完共识，我们来了解什么是共识机制，共识机制又称共识算法，看一下是维基百科对共识机制的解释： 由于加密货币多数采用去中心化的区块链设计，节点是各处分散且平行的，所以必须设计一套制度，来维护系统的运作顺序与公平性，统一区块链的版本，并奖励提供资源维护区块链的使用者，以及惩罚恶意的危害者。这样的制度，必须依赖某种方式来证明，是由谁取得了一个区块链的打包权（或称记账权），并且可以获取打包这一个区块的奖励；又或者是谁意图进行危害，就会获得一定的惩罚，这就是共识机制。 用自己的话说就是一套解决如何在彼此都不信任的基础（因为会存在有人捣乱的情况）上仍然可以就某一个问题达成一致的制度。为了一步步的讲解，先来看下面的一张图： 这张图我们暂且称左边的人为A，右边的人为B，那么思考一个问题，假如在一开始的时候他们彼此都不认识（彼此不信任），那么他们能否就自己带的是什么颜色的帽子达成共识（不能自己摘下自己的帽子来看）？ 从这张图我们可以确定一下几点： A知道B的帽子是什么颜色的； B知道A知道他的帽子是什么颜色的； A也知道B知道自己知道他的帽子是什么颜色的； … 可以看出A其实没办法验证自己的帽子是什么颜色的，你可能会说那A可以直接问B自己是什么颜色的帽子呀，这样不就达成共识了吗，没错，A确实可以通过询问的方式来知道自己帽子的颜色，但是我上面说了他们彼此是不信任的，假如B告诉A他的帽子是绿色的，那么A得到的结论永远都是错的，所以A是永远都不能证实自己的帽子是什么颜色的。这个其实就是两个将军问题的翻版，两个将军问题已经被证实是无解的。 我们继续这个问题往下思考，你可能会想那能不能多添加几个人，通过询问其他人自己的帽子是什么颜色的，然后以少数服从多数的原则就可以初步证实自己的帽子是什么颜色的（注意这里是初步证实不是完全证实），那么有出现一个问题，至少需要添加多少人才能满足可以进行判断的要求？如果是增加一个比如C，那么如果C告诉A他的帽子颜色是黄色的（为了简单，就先假设现在只有黄、绿两个选择），A还是无法证实自己的帽子是什么颜色的，因为他根本就不知道他们中谁在说谎；那么增加两个人呢，比如增加C、D，这个时候他们三个人中就可能会有两个人的答案是一致的，那么A就可以基本知道自己的帽子是什么颜色了。到这里你可能还会问，那如果有两个人说谎呢？这样A得到的结果还是错误的（这就是我上面为什么说是初步而不是完全证实，就是这个原因）。那这个问题要如何解决？先不急我们先捋一捋。 共识机制的本质是解决信任的问题，就是在彼此都不信任的基础上达成一致的意见； 在分布式系统（或者区块链）中要达成共识至少需要四个节点以上，少于四个节点是无法达成共识的。 拜占庭将军问题 在上面我提到就是如果有两个人都说谎了那该如何解决？问题的答案是解决不了，结果就是A会一直被蒙在鼓里，其实提这个问题是为了引出另一个问题，即拜占庭将军问题，看下面的故事： 拜占庭帝国想要进攻一个强大的敌人，为此派出了10支军队去包围这个敌人。这个敌人虽不比拜占庭帝国，但也足以抵御5支常规拜占庭军队的同时袭击。这10支军队在分开的包围状态下同时攻击。他们任一支军队单独进攻都毫无胜算，除非有至少6支军队（一半以上）同时袭击才能攻下敌国。他们分散在敌国的四周，依靠通信兵骑马相互通信来协商进攻意向及进攻时间。困扰这些将军的问题是，他们不确定他们中是否有叛徒，叛徒可能擅自变更进攻意向或者进攻时间。在这种状态下，拜占庭将军们才能保证有多于6支军队在同一时间一起发起进攻，从而赢取战斗？ 为了简化问题，我们先假设所有的将军是通过口头消息来传递信息的，即消息传递的信道绝无问题，因为如果还要考虑信息在传递的过程中被人修改了这种情况，那问题就要更加复杂了，这里先不讨论这个，后面打算单独写一篇关于拜占庭将军问题，再来详细讲解。 问题分析 单从上面的说明可能无法理解这个问题的复杂性，我们来简单分析一下： 先看在没有叛徒情况下，假如一个将军A提一个进攻提议（如：明日下午1点进攻，你愿意加入吗？）由通信兵通信分别告诉其他的将军，如果幸运中的幸运，他收到了其他6位将军以上的同意，发起进攻。如果不幸，其他的将军也在此时发出不同的进攻提议（如：明日下午2点、3点进攻，你愿意加入吗？），由于时间上的差异，不同的将军收到（并认可）的进攻提议可能是不一样的，这是可能出现A提议有3个支持者，B提议有4个支持者，C提议有2个支持者等等。 再加一点复杂性，在有叛徒情况下，一个叛徒会向不同的将军发出不同的进攻提议（通知A明日下午1点进攻，通知B明日下午2点进攻等等），一个叛徒也会可能同意多个进攻提议（即同意下午1点进攻又同意下午2点进攻）。 叛徒发送前后不一致的进攻提议，被称为“拜占庭错误”，而能够处理拜占庭错误的这种容错性称为（Byzantine fault tolerance），简称为BFT。 到这里大概能理解拜占庭将军问题是讲什么的了，对于这个问题如果现在有n个将军和m个叛徒，只要满足n&gt;=3m+1就能达成共识（注意前提是口头消息）。比如有2/3的成员是诚实的，算法就能达到共识。如果叛徒多于1/3，无法达到共识，这些军队无法协调他们的攻击，敌军胜利。 所以当一个分布式系统中有多余三分之一节点出现了故障时，那对这个系统是毁灭性的，当然这些是理论，现实不一定就是毁灭性的，但是一定是很糟糕的事情。 区块链的共识算法 因为区块链是一个分布式的系统，所以它一定存在拜占庭将军问题，那么就需要解决如何保证各个主机状态一致的问题，为了解决这个问题很多人都提出了自己的共识算法，最有名的当属中本聪的POW（工作量证明机制）共识算法，当然还有POS（权益证明机制）、DPOS（股权权益证明机制）、PBFT（拜占庭容错机制）以及RAFT共识算法，对于这些算法在后面都会一一讲到，这里就不讲了，今天主要是理解什么是共识机制以及在区块链中共识机制所要解决的问题。 结尾 在本篇最开始的时候我提到了少数服从多数这个共识，其实在比特币中对应的就是选择最长链，会出现这个问题是因为矿工在打包区块的时候会出现一种情况，即有两个矿工同一时刻计算出随机数获得记账权，这个时候就会出现两条链，这两条链会沿着不同的方向发展下去，那么结果就是比特币网络会变得越来越混乱，为了防止这种事情发生，比特币有一个原则就是选择最长链，即在某一时间段内比较哪条链最长（因为算力的不同就会出现在一段时间后总有一条链的长度超过另一条链的长度这种情况），然后把短的链丢弃掉，这其实也是少数服从多数的原则，准确的说应该是短链服从长链。 这种两个矿工同时挖出一个符合条件的区块的情况也叫作分叉，这就是我下一篇打算讲的比特币分叉。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链100篇之第五篇-UTXO]]></title>
    <url>%2F2018%2F10%2F24%2F%E5%8C%BA%E5%9D%97%E9%93%BE100%E7%AF%87%E4%B9%8B%E7%AC%AC%E4%BA%94%E7%AF%87-UTXO%2F</url>
    <content type="text"><![CDATA[此区块链100篇文章为在博客上看到较好的区块链解释，所以搜集而来，转载博客原地址https://blog.csdn.net/weixin_37504041 第一次看到UTXO是去年10月份的时候在一个微信群上有一个人说了一句话，说：”只要理解了UTXO就理解了比特币“，虽然这话有点夸大了，毕竟比特币并不只有UTXO这个技术，涉及到的东西还是很多的，但可以把这句话改成”只要理解了UTXO就能理解比特币的去中心化的含义“，因为UTXO区别于我们传统上的交易都是基于账户模型的，它是中本聪发明的交易模型，暂且叫UTXO模型，理解了UTXO的交易模型就能明白为什么比特币是去中性化的。 什么是UTXO？ UTXO的全称叫Unspent Transaction Output，因为外国人习惯把Translation叫做TX，所以就简称为UTXO，中文翻译过来就是“未消费的交易输出”，从字面上的意思可以从两点入手来理解，第一是：未消费；第二是：输出。未消费就是还没有花出去的，这就好比你现在手上有一张十块跟一张五块，这时候你去吃个饭花了十块，那么你手上就只剩五块了，而这五块就是未消费的。什么是输出？刚开始看到这两个字时可能会有些疑问，输出意味着拿出去，也就是花出去，那都花出去了怎么能叫未消费呢？按正常的逻辑确实讲不通，但可以换个思路。既然有输出那自然对应的就会有输入，那什么是输入呢？输入就是交易时的给那笔钱，举个例子，本来你打算只吃十块钱的饭，但是你发现只要再加两块钱就可以多加一个鸡腿，于是你改变主意想吃十二块的饭，这时你手里只有一张十块的跟一张五块的，没有两张一块的，于是你就拿着十块跟五块给老板。这里的十块跟五块就是你跟老板之间的交易的输入，然后老板就把饭打给你，接着还找了三张一块钱的给你，这里的三张一块钱便是输出，而且这三张一块钱你还没有花出去，也就是“未消费”，那么就可以把这三张一块钱称为“UTXO”。 这里可以给UTXO下个简单的定义：UTXO = 一笔交易的总输入 - 在这笔交易中花费了多少钱（包括手续费）。为什么说是简单的定义，因为UTXO也有可能是挖矿得到的，比如中本聪挖的第一个区块（也就是创世区块）得到的50个比特币，这50比特币就是UTXO，所以说我刚才的定义并不准确，我下的定义是排除了挖矿这种情况，只在正常的交易过程中适用。到这里你可能会有些疑问，第一，你付给老板的那十二块算不算UTXO？这个问题需要单独写一篇才能说明白，也就是比特币的找零机制，留到第十篇讲，这里先说一下为什么不能简单的说那十二块是不是UTXO，在概念层面上，答案是肯定的，因为这十二块就是“未消费的输出”，但是在实际上我们并不能将你付给老板的那一张十块跟一张五块拆开来，你不能把那张五块钱撕成两半，一半代表两块钱一半代表三块钱，这个在现实世界里是做不到的，但是在比特币的世界里却是很轻松，这个之后再详细讲；第二，既然UTXO可以作为交易的输入，那么交易完成后这个UTXO如何处理，答案是销毁，一旦交易被确认并记录到比特币的区块中，那么这个UTXO就会被永久的销毁掉。 比特币是如何通过UTXO实现交易的？ 大概明白了什么是UTXO，那么就来讲讲UTXO的交易过程是怎样的。为了理解我们可以举个现实交易的例子，现在我要转一百块钱到你的账户去，基本的过程是这样的，银行会先检查我的账户里的余额有没有大于等于一百块，如果有那么就会将我的账户减去100，接着会在你的账户上增加一百，这个过程看起来很容易理解，因为银行确实就是这么干的，但是这里要注意的就是在你我之间存在着一个中心机构，它帮我们进行账户的验证以及转账的操作，但是在比特币的世界里是没有中心机构的，那么它是如何做到即使没有中心机构依然可以让交易顺利的进行？ 假设我现在手里总共有十个比特币，这个十个比特币可能是很多个UTXO组成的，比如0.1BTC，0.5BTC，3BTC，0.0123BTC，6.18BTC，0.2077BTC，这些UTXO总共加起来有10个BTC，这个时候我想转3.55个BTC给你，那么这个时候交易的输入就是0.1BTC，0.5BTC，3BTC，而交易输出就是3.55BTC跟0.05BTC，其中3.55BTC这个UTXO会使用你的公钥进行“锁定”，而0.05BTC的UTXO会使用我的公钥进行“锁定”（这里涉及到非对称加密，可以看我的第二篇博客），一旦被脚本锁定就说明这个比特币是属于谁的，因为只有拥有公钥对应的私钥的人才能进行解锁。到这里我的账户上的比特币总量是6.45个比特币，分别是0.0123BTC，6.18BTC，0.2077BTC跟0.05BTC，注意到没，这里我账户“余额”并不是6.45BTC，而是被我（所有者）锁住的、分散的UTXO，在比特币世界里并没有所谓的“余额”，只有UTXO，我们平常所说的比特币“余额”多少，是通过扫描区块链并聚合所有属于该用户的UTXO来计算该用户的余额。注意到这里你我之间并没有任何的第三方介入，纯粹就是我们两人之间的交易而已，这边是去中心化。 UTXO有什么优点？ 看完上面可能会云里雾里，你可能会说这不就跟我们现实生活中给钱差不多嘛，假如现在我有五张一块钱的，现在我需要给你两块钱，我直接把两张一块钱的给你就行了，这其中也不需要什么人来介入，我们两个人就可以搞定这笔交易。 确实是这样，但是区别就在于UTXO是可分割的，假如现在你要我给你两块五而不是两块，但我现在并没有五毛，我没办法将一块钱撕成两半，所以这笔交易在现实中就比较难达成，但是UTXO就不一样了，它可以切割到小数点后八位，比如可以有0.00023546BTC，所以基本可以满足大部分的交易需求，可以这么说，UTXO使得比特币既解决了传统现金不可分割的问题也避免了传统电子货币的中心化弊端，是去中心化的电子现金。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链100篇之第四篇-矿工及挖矿]]></title>
    <url>%2F2018%2F10%2F24%2F%E5%8C%BA%E5%9D%97%E9%93%BE100%E7%AF%87%E4%B9%8B%E7%AC%AC%E5%9B%9B%E7%AF%87-%E7%9F%BF%E5%B7%A5%E5%8F%8A%E6%8C%96%E7%9F%BF%2F</url>
    <content type="text"><![CDATA[此区块链100篇文章为在博客上看到较好的区块链解释，所以搜集而来，转载博客原地址https://blog.csdn.net/weixin_37504041 在第二篇中我遗留了一个问题，就是虽然收款人没办法验证付款人是否真的有足够的余额进行支付转账（因为现在没有一个中心机构进行认证），所以这时候就需要矿工来进行确认，对于为什么需要矿工这个角色和矿工是干什么的以及挖矿的过程，下面就来一一细说。 为什么需要矿工？ 比特币是一个点对点去中心化的网络，在这个网络里任何人都可以参与进来，成为这个网络中的节点（普通节点或者矿工节点），正因为比特币的世界中没有一个中心机构来确保比特币整个系统的正常运行（这与我们现在中心化系统不一样，是比特币的一大魅力），那么如何确保比特币正常运行呢？这就需要矿工了。 矿工是干什么的？ 因为比特币的交易需要进行确认，并需要将多笔交易打包成区块，矿工便是干这个的。矿工接收到用户广播的账单后，要对账单的合法性和真实性进行验证，这里的合法性，是指矿工会检验支付方的比特币是否充足。矿工们按照交易中支付方的地址，在过往合法的交易中查询该账户“转入”的比特币数量，当大于或者等于本账单中填写的数额时，这笔交易就是合法的，这回答了我们一开头的那个问题；接着矿工需要开始用不同的随机数进行哈希计算，直至找到符合目标值特征的随机数，如果找到了这个随机数，矿工需要将在 10 分钟左右发生的验证过的交易内容打包成一个不超过1MB大小的区块，然后全网广播出去，告诉其他矿工节点已经生成一个新的区块了，你们不需要再进行随机数的计算了，于是其他矿工就会进行验证这个随机数是否是正确的，如果通过了验那么这个新区块就会被添加到最长链的尾部（为什么说最长链，这里涉及到比特币的分叉，在第七篇讲），然后他们就会放弃当前区块的计算，马上继续下一个区块的随机数计算。 举个例子就是现在我和小明、小花、小白以及小李进行记账竞赛（谁赢了谁就有糖吃，所以都很努力的记账），我们都把每一笔有效的交易记录在一个本子上，因为我们的记账速度都差不多，所以我们几乎是同时把本子上的一页纸（相当于一个区块）写满，这个时候并不能以谁记账速度快来分胜负（因为大家几乎同时完成记账），所以我们还需要进行计算一道数学方程题，谁先接触这道题谁就可以获得糖果；于是大家开始埋头计算数学题，假设这时小明最先解除了答案，于是他就大喊一声，“我解出来”，这时大家就开始停下手上的计算工作，开始验证小明的答案是否就是这道题的解，结果大家都认为小明解出了答案，于是这颗糖（比特币）便属于小明的了。 挖矿的过程 挖矿的过程其实就类似于解题，谁先找到答案谁就获得记账权，从而获得比特币奖励。 挖矿的具体过程为：参与者根据上一个区块的hash值，10分钟内的验证过的交易内容，再加上自己猜测的一个随机数X，让新区块的hash值小于比特币网络中给定的一个数。这个数越小，计算出来就越难。系统每隔两周（即经过2016个区块）会根据上一周期的挖矿时间来调整挖矿难度（通过调整限制数的大小），来调节生成区块的时间稳定在10分钟左右。为了避免震荡，每次调整的最大幅度为 4 倍。 比特币如何保证矿工积极挖矿？ 我在第一篇讲比特币的时候有简单提到过比特币的奖励机制，比特币的激励机制就是奖励矿工一些比特币用来激励矿工积极的记账以此来达到整个系统稳定运行的效果，所以矿工节点就需要不断的提高自己的算力来争夺打包区块的权力，因为你的算力越大你就有可能先计算出符合某一个标准的比特币区块头的哈希散列值，这就是工作量证明共识机制，简称POW，除了POW外还有POS、DPOS、RAFT等等（这个留到第六篇讲共识机制的时候再说）。比特币是通过工作量证明的共识机制来决定记账权的，通俗来讲，谁证明了自己的工作量最大，谁就负责记账。 矿工除了可以得到比特币网络奖励的比特币，还可以获得每一笔交易的手续费，奖励手续费可以保证当所有的比特币发行完毕后（总量2100万，预计到2140年发完）比特币网络仍然可以正常运行。 总结 比特币是一个设计很巧妙的系统。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链100篇之第三篇-数字签名]]></title>
    <url>%2F2018%2F10%2F24%2F%E5%8C%BA%E5%9D%97%E9%93%BE100%E7%AF%87%E4%B9%8B%E7%AC%AC%E4%B8%89%E7%AF%87-%E6%95%B0%E5%AD%97%E7%AD%BE%E5%90%8D%2F</url>
    <content type="text"><![CDATA[此区块链100篇文章为在博客上看到较好的区块链解释，所以搜集而来，转载博客原地址https://blog.csdn.net/weixin_37504041 数字签名类似于我们现实世界中的文件签名，我们把名字写在一个文件上，就代表了我们认可了这份文件，并且说明了这份文件是真是可靠的，那么问题是如果文件的署名被伪造了怎么办？在现实世界中，我们一般会找当事人直接验证这份文件是否是亲自签署的而不是被人假冒的，而在计算机世界中，保证数字签名不被伪造是通过数字摘要和非对称加密实现的，举个比特币中交易的例子:假如现在Alice发起一笔比特币转账，需要先将该交易进行数字摘要，缩短成一段字符串，然后用自己的私钥对摘要进行加密，形成数字签名。完成后，需要将交易信息（包括收款方的地址）和数字签名一起广播给记录“账本的人”也就是矿工。矿工用Alice的公钥进行验证，如果验证成功，说明该笔交易确实是Alice发出的，且信息未被更改，是完整的。 下面就来详细讲解这数字摘要和非对称加密技术，讲完之后再回过头来看这个例子就明白了。 数字摘要 数字摘要技术用于对所要传输的数据进行运算生成信息摘要，它并不是一种加密机制，但却能产生信息的数字”指纹”，它的目的是为了确保数据没有被修改或变化，保证信息的完整性不被破坏。说直白点就是将数据进行hash编码。它在数字签名中的作用就是增加被伪造的难度，就好比有些人故意把字要写的龙凤凤舞，目的就是很难让他人伪造。 非对称加密 讲到非对称加密我们需要了解一下什么是对称加密，举个例子：现在你（未成年）跟你朋友想聊一个比较敏感的话题（比如…），但是你怕你老妈会翻看你手机，要是让她知道你跟别人聊这种话题肯定会挨骂，于是你就跟你的朋友约定好你将中文先翻译成英文再发过去，然后他那边可以使用微信翻译将英文翻译成中文，这样即使老妈看到了也无所谓，因为她根本看不懂英语，于是你跟你的朋友便聊了起来： 你：The XiaoHua in the class are so beautiful that I want to chase her.（班里的小花好漂亮，我想追她） 你朋友：I also think she’s pretty.（我也觉得她好漂亮） …… 这里的“班里的小花好漂亮，我想追她”就是明文，而“The XiaoHua in the class are so beautiful that I want to chase her.”就是密文，而密钥就是将“班里的小花好漂亮，我想追她”转化成“The XiaoHua in the class are so beautiful that I want to chase her.”这么一个规则，讲到这大概就清楚了什么是对称加密了吧，可以这么理解： 一方通过密钥将信息加密后，把密文传给另一方，另一方通过这个相同的密钥将密文解密，转换成可以理解的明文。他们之间的关系： 明文 &lt;-&gt; 密钥 &lt;-&gt; 密文 理解了什么是对称加密后你可能有一个疑问，那如果老妈使用了微信翻译将发送出去的英文翻译成中文她不就看懂了吗?对的，这就是对称加密的缺陷，一旦被人拿到了密钥（在对称加密中，密钥就是加解密的规则），那么你发送出去的密文将轻而易举被人破解，那么有什么方法做到即使一个人的密钥被盗窃了，最起码保证你给其他人发送密文不被破解？就是说即使你老妈获取到了你的密钥，但她仍然无法破解你发送出去的信息，只有你的朋友才能查看你发送给他的信息。这个就是我们要讲的非对称加密。 在非对称加密中，不管是信息的发送方还是信息的接收方都有一对属于自己的公钥和密钥，公钥顾名思义就是可以公开，而私钥就是只能自己看，不能给别人拿到。接着发送方会将信息进行数字摘要并使用接收方的公钥进行加密，然后再使用自己的私钥对数字摘要进行加密，接收方收到加密后的信息跟加密串，用 发送方的公钥（因为公钥是公开的）来解密加密串，得到原始的数字摘要，然后对使用自己的私钥进行解密的信息进行摘要后的结果进行比对。如果一致，说明该文件确实是该发送方发过来的，并且文件内容没有被修改过。 还是上面那个例子，你使用你朋友（暂且称为A）的公钥对你的信息进行加密，并且对信息进行数字摘要，然后使用你自己的私钥对摘要进行加密，即数字签名，然后将数字签名和加密后的信息一起发送给A，A接收到信息后，先使用你的公钥对数字签名进行解密，证明该信息确实是你发送过来的而不是别人发送过来的，然后再使用自己的私钥对加密后的信息进行解密（注意这里只有A的私钥才能解密信息，其他人都不可以，即使是你本人也不能解密，所以就算是你老妈拿到了你的私钥也没办法破解该信息），然后对该信息使用同样的算法进行数字摘要，如果得到的摘要跟你发送过来的摘要一致，则说明信息在传递的过程中没有被修改过。 好了，到这里基本上是讲完了，其他的如果有兴趣自行上网搜集材料学习。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链100篇之第二篇-p2p与比特币的支付和交易]]></title>
    <url>%2F2018%2F10%2F24%2F%E5%8C%BA%E5%9D%97%E9%93%BE100%E7%AF%87%E4%B9%8B%E7%AC%AC%E4%BA%8C%E7%AF%87-p2p%E4%B8%8E%E6%AF%94%E7%89%B9%E5%B8%81%E7%9A%84%E6%94%AF%E4%BB%98%E5%92%8C%E4%BA%A4%E6%98%93%2F</url>
    <content type="text"><![CDATA[此区块链100篇文章为在博客上看到较好的区块链解释，所以搜集而来，转载博客原地址https://blog.csdn.net/weixin_37504041 因为比特币网络是一个点对点的网络，也就是peer-to-peer，简称P2P，所以这里需要先讲一下P2P的知识，讲这个主要是更好的理解比特币的支付与交易的过程。 1P2P P2P网络是指位于同一网络中的每台计算机都彼此对等，各个节点共同提供网络服务，不存在任何“特殊”节点，每个网络节点以扁平（flat）的拓扑结构相互连通。对比中心化网络，在P2P网络中不存在任何服务端（server）、中央化的服务。 那么问题来了，节点是如何发现其他节点的呢？ 1.节点会记住它最近成功连接的网络节点，当重新启动后它可以迅速与先前的对等节点网络重新建立连接。 2.节点会在失去已有连接时尝试发现新节点。 3.当建立一个或多个连接后，节点将一条包含自身IP地址消息发送给其相邻节点。相邻节点再将此消息依次转发给它们各自的相邻节点，从而保证节点信息被多个节点所接收、保证连接更稳定。 4.新接入的节点可以向它的相邻节点发送获取地址getaddr消息，要求它们返回其已知对等节点的IP地址列表。节点可以找到需连接到的对等节点。 5.在节点启动时，可以给节点指定一个正活跃节点IP,如果没有，客户端也维持一个列表，列出了那些长期稳定运行的节点。这样的节点也被称为种子节点（其实和BT下载的种子文件道理是一样的），就可以通过种子节点来快速发现网络中的其他节点。 上面的话可能有点难理解，举个生动的例子就是假设你和A是好闺蜜（建立过连接，对应第一点），现在你的一个男性朋友想追A，他想知道关于A的一些信息，这时他可以通过你来得到A的信息（新节点接入，对应第四点），那么这个时候A会把自己的信息告诉你，而你就会将这个信息告诉给你的男性朋友（对应第三点）；如果此时你跟A吵架闹掰了，那么你可能会尝试找新的朋友（对应第二点），至于第五点可以理解为假如你现在有了男朋友，那么你每天醒来的时候第一个想见的可能就是你的男朋友，但如果没有男朋友的话，那么这个时候你的清单里会罗列一些你的“备胎”。这个是对上面那五个点的理解，主要P2P的本质就是节点与节点是平等的。 1比特币的支付与交易 讲完了P2P的知识，现在来讲一下比特币的支付与交易过程。 传统上我们进行支付的过程是这样的：A（填写收款人的银行卡账号及转账金额） –&gt; C（银行，验证A的信息是否正确及账户上余额是否充足，如果条件满足则将A上的账户余额减去相应的额度，并将B的余额增加相应的额度） –&gt; B（B的账户上多了A转过来的钱），但是在比特币网络中是没有C这个节点的，也就是说没有中间机构来验证A的信息是否正确，余额是否充足，那么是如何做到每一笔交易是真是可靠的呢？ 11.首先是付款人签署交易单 付款人需要输入自己的比特币地址以及收款方的比特币地址，还要输入转账比特币的个数，如5个比特币，然后付款方会发送一个请求给收款方（以P2P的形式发送），在发送这个请求之前付款人会先加上收款人的公钥（这里可以先理解为地址，但是两者是有区别的）和交易信息，然后再用自己的私钥加密整个请求，并把自己的公钥标记在这个请求上。这里需要注意的是付款人不仅要将这个请求发给收款方，还需要以P2P的方式广播出去告诉其他网络节点，让所有节点都接收到这个请求。举个例子Alice要给Bob转账5个比特币，那么Alice首先要询问Bob的标识字符串（比特币地址），例如是“ABCDEFG”，同时Alice也有一个标识字符串例如是“HIJKLMN”，然后Alice写一张单子，内容为“HILKLMN支付5比特币给ABCDEFG”，然后用自己的保密印章（私钥）盖一个章，将这张单子交给Bob。 这个过程涉及到非对称加密和数字摘要的知识，在下一篇会详细讲解，看完就会知道为什么要这么做。 12.接着是收款人确认单据签署人 收款人收到这个请求后拿着付款人的公钥进行解密，如果能解密则说明付款人确实是拥有这个私钥的，进而说明这笔账是从付款人转过来的，还可以得到收款人的公钥（因为上面已经讲了付款人会将收款人的公钥跟交易信息进行加密）说明这笔账确实是转给收款人的。 13.确认付款人余额 通过上面的步骤是可以确定这笔账确实是付款人发起的，但是没办法有效的确认付款人是否真的有足够的余额进行转账（毕竟现在没有一个中心机构进行认证），所以这个时候就需要矿工（第四篇会讲）来进行确认，这是因为在比特币的世界里是没有余额这个概念的，我们在比特币钱包里看到的“余额”不是我们所理解的余额，它其实是UTXO（意思是未花费的交易输出，这个打算在第五篇讲），验证付款方是否有余额这个工作通常由矿工来做，而收款方不需要做这个工作，只需要验证这个请求是否是付款人发起的即可。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链100篇之第一篇-比特币]]></title>
    <url>%2F2018%2F10%2F24%2F%E5%8C%BA%E5%9D%97%E9%93%BE100%E7%AF%87%E4%B9%8B%E7%AC%AC%E4%B8%80%E7%AF%87-%E6%AF%94%E7%89%B9%E5%B8%81%2F</url>
    <content type="text"><![CDATA[此区块链100篇文章为在博客上看到较好的区块链解释，所以搜集而来，转载博客原地址https://blog.csdn.net/weixin_37504041 这是区块链100篇的第一篇，自己接触区块链已经有七个月左右了，是去年的九月份才开始接触区块链这个词，一开始以为区块链是一个很高深的技术领域（它的确涉及到了很多高深的知识如密码学、分布式网络、计算机软件、博弈论等等），不过好在这些底层的技术已经有人帮我们封装了，如以太坊，所以入门的门槛就降低了。打算写这个区块链100篇的初衷只是想把自己学到的知识输出出来，顺便让自己从头捋一遍，这样有助于自己对区块链的更深入理解。还有写这个系列的灵感来源于火币出品的区块链100问，个人觉得做得挺好的，所以想用同样的方式来记录自己学区块链的点点滴滴。以后每个星期会更新两到三篇，工作忙的话就只能保证更新一篇，整个系列目标是写到一百篇，主要分三大块，比特币、以太坊以及超级账本（后面或许会加上EOS，这个看情况）。 Bitcoin（比特币） Ethereum（以太坊） Hyperledger Fabric（超级账本） EOS（可能不会更新） 说到区块链就绕不开比特币，毕竟是先有比特币再有区块链的，所以这个系列就从比特币开始说起，后面会对比特币涉及到的各种知识进行逐一讲解（个人能力有限，所以知识点可能会讲错）。 比特币与区块链的历史2008年11月，一个化名中本聪的人发表了一篇名为《比特币：点对点的电子现金系统》的论文，这也被俗称为创世论文，2009年1月用他第一版的软件挖出了第一个区块，即创世区块，之后越来越多的人开始加入挖比特币的行列，一开始参与的人大部分是一些极客，他们对这个数字货币充满好奇，并坚信比特币会颠覆世界金融，随着比特币的影响力越来越大，更多的人加入进来，这其中就不乏一些大神级别的人物，如V神。到了2014年，V神创立了以太坊，正式提出区块链这个概念，到2016年这个概念才开始为公众所知，2017年由于ICO热炒而成为媒体热点，现在依然火热。 比特币的运行机制 简单讲完比特币与区块链的历史（详细的历史可以自行上网查找，这里就不赘述），下面我们讲个故事来讲解比特币的运行机制，之后的会对这其中涉及到的各个知识进行详细的讲解。 1在古代有个村子，这个村庄几乎与世隔绝，过着自给自足的生活，一开始村民进行交易的时候都会把这笔交易记录下来，当做凭证，但是村里的人不是个个都识字，所以这个时候就需要一个既识字又有威望的人来记账，刚好村长早些年读过书，而且他又是一村之长，自然就承担起了这个责任，村长每天的任务就是记录每一笔交易，比如张三使用100块买了王五家的猪，那么村长就在张三名下的存款减去100，在王五的账上加上100。 故事讲到这就基本结束了，下面就是需要思考的时候了。上面讲的例子其实就是我们现实生活中银行干的事，所以你可以把村长看作是银行或者一个比较有权威的第三方支付平台如支付宝、微信。 1问题一、假如随着村长的年龄越来越大，眼睛跟脑子都不太好使了（中心化管理的弊端），记错账怎么办？ 我们可以让大家轮流来记账，这个月张三，下个月李四，大家轮着来，防止账本被一个人拿在手里。于是，账本的记账权发生变化，大家都有权来记账，这样就可以避免中心化个体因记错账给大家带来的损失。 1问题二、如果这时候李四想要挪用村里的公款，虽然他不用直接篡改账本上的信息，但是可以烧掉账本中的一部分内容，这样别人就查不出来了，回头只要告诉大家这是不小心碰到蜡烛，别人也没什么办法。如何防止这种事情的发生？ 解决的办法是每个人都拥有一本自己的账本，任何一个人改动了账本都必须要告知所有其他人，其他人会在自己的账本上同样地记上一笔，如果有人发现新改动的账目不对，可以拒绝接受，到了最后，以大多数人都一致的账目表示为准。所以即便是有人真的不小心损坏了一部分账本的内容，只要找到其他的人去重新复制一份来就行了。 1问题三、如果时间长了，有人就偷懒了，不愿意这么麻烦地记账，就希望别人记好账后，自己拿过来核对一下，没问题就直接抄一遍，那么这下记账记得最勤的人就有意见了，该怎么办？ 办法是每天早上掷骰子，根据点数决定谁来记当天的账，其他人只要核对一下，没问题就复制过来。还可以让被掷到要记账的人，能获得一些奖励，从当天的记账总额中划出一定奖励的比例，这样记账的人就会积极的记账（这就是比特币的激励机制）。 讲到这里实际上这就是比特币的记账雏形，不过关于比特币还有很多细节没讲到，包括非对称加密、UTXO、SHA256算法、双花问题等等，这些后面再一一的讲解，现在先大概了解比特币的运行机制。比特币没有一个中心化的记账机构，记账权分散在各个节点中，使得它被篡改的几率大大降低，提高了安全性及可信任度，这是它的优点，但也是它的缺点，因为每记录一笔账就需要经过各个节点的确认，大大降低了运行效率。 1这里讲一下个人对于区块链的观点，区块链不是万能的，它在某些场景下确实具有无可比拟的优势，比如金融领域或者数据存储，但在某些场景下我们确实是需要中心化来管理的，所以个人觉得具体场景具体运用，均衡取舍，不要为了区块链而区块链，不然最后可能会得不偿失。未来的社会一定是中心化与去中心化并存，没有绝对的中心化也没有绝对的去中心化。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntn网络链接显示设备未托管-解决方法]]></title>
    <url>%2F2018%2F10%2F23%2FUbuntn%E7%BD%91%E7%BB%9C%E9%93%BE%E6%8E%A5%E6%98%BE%E7%A4%BA%E8%AE%BE%E5%A4%87%E6%9C%AA%E6%89%98%E7%AE%A1-%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[转载自：https://blog.csdn.net/wildbison/article/details/6824024 ubuntu11.10 开机进入ubuntu系统后右上角的网络连接显示设备未托管网络不能使用了。 搜集网络资料解决如下： 修改文件： /etc/NetworkManager/NetworkManager.conf[ifupdown]managed=true 重启即可。。。。 原因： Linux里面有两套管理网络连接的方案： 1、/etc/network/interfaces（/etc/init.d/networking）2、Network-Manager 两套方案是冲突的，不能同时共存。第一个方案适用于没有X的环境，如：服务器；或者那些完全不需要改动连接的场合。第二套方案使用于有桌面的环境，特别是笔记本，搬来搬去，网络连接情况随时会变的。 －－－－－－－－－－－－－他们两个为了避免冲突，又能共享配置，就有了下面的解决方案：1、当Network-Manager发现/etc/network/interfaces被改动的时候，则关闭自己（显示为未托管），除非managed设置成真。2、当managed设置成真时，/etc/network/interfaces，则不生效。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-scp命令]]></title>
    <url>%2F2018%2F10%2F23%2FLinux%E5%91%BD%E4%BB%A4-scp%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[scp命令用于在Linux下进行远程拷贝文件的命令，和它类似的命令有cp，不过cp只是在本机进行拷贝不能跨服务器，而且scp传输是加密的。可能会稍微影响一下速度。当你服务器硬盘变为只读read only system时，用scp可以帮你把文件移出来。另外，scp还非常不占资源，不会提高多少系统负荷，在这一点上，rsync就远远不及它了。虽然 rsync比scp会快一点，但当小文件众多的情况下，rsync会导致硬盘I/O非常高，而scp基本不影响系统正常使用。 语法1scp(选项)(参数) 选项12345678910111213-1：使用ssh协议版本1；-2：使用ssh协议版本2；-4：使用ipv4；-6：使用ipv6；-B：以批处理模式运行；-C：使用压缩；-F：指定ssh配置文件；-l：指定宽带限制；-o：指定使用的ssh选项；-P：指定远程主机的端口号；-p：保留文件的最后修改时间，最后访问时间和权限模式；-q：不显示复制进度；-r：以递归方式复制。 参数 源文件：指定要复制的源文件。 目标文件：目标文件。格式为user@host：filename（文件名为目标文件的名称）。 实例从远程复制到本地的scp命令与上面的命令雷同，只要将从本地复制到远程的命令后面2个参数互换顺序就行了。 从远处复制文件到本地目录 1scp root@10.10.10.10:/opt/soft/nginx-0.5.38.tar.gz /opt/soft/ 从10.10.10.10机器上的/opt/soft/的目录中下载nginx-0.5.38.tar.gz 文件到本地/opt/soft/目录中。 从远处复制到本地 1scp -r root@10.10.10.10:/opt/soft/mongodb /opt/soft/ 从10.10.10.10机器上的/opt/soft/中下载mongodb目录到本地的/opt/soft/目录来。 上传本地文件到远程机器指定目录 1scp /opt/soft/nginx-0.5.38.tar.gz root@10.10.10.10:/opt/soft/scptest 复制本地/opt/soft/目录下的文件nginx-0.5.38.tar.gz到远程机器10.10.10.10的opt/soft/scptest目录。 上传本地目录到远程机器指定目录 1scp -r /opt/soft/mongodb root@10.10.10.10:/opt/soft/scptest 上传本地目录/opt/soft/mongodb到远程机器10.10.10.10上/opt/soft/scptest的目录中去。 转载链接： http://man.linuxde.net/scp]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-telnet命令]]></title>
    <url>%2F2018%2F10%2F23%2FLinux%E5%91%BD%E4%BB%A4-telnet%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[telnet命令用于登录远程主机，对远程主机进行管理。telnet因为采用明文传送报文，安全性不好，很多Linux服务器都不开放telnet服务，而改用更安全的ssh方式了。但仍然有很多别的系统可能采用了telnet方式来提供远程登录，因此弄清楚telnet客户端的使用方式仍是很有必要的。 语法1telnet(选项)(参数) 选项123456789101112131415161718-8：允许使用8位字符资料，包括输入与输出；-a：尝试自动登入远端系统；-b&lt;主机别名&gt;：使用别名指定远端主机名称；-c：不读取用户专属目录里的.telnetrc文件；-d：启动排错模式；-e&lt;脱离字符&gt;：设置脱离字符；-E：滤除脱离字符；-f：此参数的效果和指定&quot;-F&quot;参数相同；-F：使用Kerberos V5认证时，加上此参数可把本地主机的认证数据上传到远端主机；-k&lt;域名&gt;：使用Kerberos认证时，加上此参数让远端主机采用指定的领域名，而非该主机的域名；-K：不自动登入远端主机；-l&lt;用户名称&gt;：指定要登入远端主机的用户名称；-L：允许输出8位字符资料；-n&lt;记录文件&gt;：指定文件记录相关信息；-r：使用类似rlogin指令的用户界面；-S&lt;服务类型&gt;：设置telnet连线所需的ip TOS信息；-x：假设主机有支持数据加密的功能，就使用它；-X&lt;认证形态&gt;：关闭指定的认证形态。 参数 远程主机：指定要登录进行管理的远程主机； 端口：指定TELNET协议使用的端口号。 实例12345678910telnet 192.168.2.10Trying 192.168.2.10...Connected to 192.168.2.10 (192.168.2.10).Escape character is &apos;^]&apos;. localhost (Linux release 2.6.18-274.18.1.el5 #1 SMP Thu Feb 9 12:45:44 EST 2012) (1)login: rootPassword: Login incorrect 转载链接： http://man.linuxde.net/telnet]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-wget命令]]></title>
    <url>%2F2018%2F10%2F23%2FLinux%E5%91%BD%E4%BB%A4-wget%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Linux系统中的wget是一个下载文件的工具，它用在命令行下。对于Linux用户是必不可少的工具，我们经常要下载一些软件或从远程服务器恢复备份到本地服务器。wget支持HTTP，HTTPS和FTP协议，可以使用HTTP代理。所谓的自动下载是指，wget可以在用户退出系统的之后在后台执行。这意味这你可以登录系统，启动一个wget下载任务，然后退出系统，wget将在后台执行直到任务完成，相对于其它大部分浏览器在下载大量数据时需要用户一直的参与，这省去了极大的麻烦。 wget 可以跟踪HTML页面上的链接依次下载来创建远程服务器的本地版本，完全重建原始站点的目录结构。这又常被称作”递归下载”。在递归下载的时候，wget 遵循Robot Exclusion标准(/robots.txt). wget可以在下载的同时，将链接转换成指向本地文件，以方便离线浏览。 wget 非常稳定，它在带宽很窄的情况下和不稳定网络中有很强的适应性.如果是由于网络的原因下载失败，wget会不断的尝试，直到整个文件下载完毕。如果是服务器打断下载过程，它会再次联到服务器上从停止的地方继续下载。这对从那些限定了链接时间的服务器上下载大文件非常有用。 语法1wget [参数] [URL地址] 参数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485启动参数：-V, –version 显示wget的版本后退出-h, –help 打印语法帮助-b, –background 启动后转入后台执行-e, –execute=COMMAND 执行`.wgetrc’格式的命令，wgetrc格式参见/etc/wgetrc或~/.wgetrc记录和输入文件参数：-o, –output-file=FILE 把记录写到FILE文件中-a, –append-output=FILE 把记录追加到FILE文件中-d, –debug 打印调试输出-q, –quiet 安静模式(没有输出)-v, –verbose 冗长模式(这是缺省设置)-nv, –non-verbose 关掉冗长模式，但不是安静模式-i, –input-file=FILE 下载在FILE文件中出现的URLs-F, –force-html 把输入文件当作HTML格式文件对待-B, –base=URL 将URL作为在-F -i参数指定的文件中出现的相对链接的前缀–sslcertfile=FILE 可选客户端证书–sslcertkey=KEYFILE 可选客户端证书的KEYFILE–egd-file=FILE 指定EGD socket的文件名下载参数：–bind-address=ADDRESS 指定本地使用地址(主机名或IP，当本地有多个IP或名字时使用)-t, –tries=NUMBER 设定最大尝试链接次数(0 表示无限制).-O –output-document=FILE 把文档写到FILE文件中-nc, –no-clobber 不要覆盖存在的文件或使用.#前缀-c, –continue 接着下载没下载完的文件–progress=TYPE 设定进程条标记-N, –timestamping 不要重新下载文件除非比本地文件新-S, –server-response 打印服务器的回应–spider 不下载任何东西-T, –timeout=SECONDS 设定响应超时的秒数-w, –wait=SECONDS 两次尝试之间间隔SECONDS秒–waitretry=SECONDS 在重新链接之间等待1…SECONDS秒–random-wait 在下载之间等待0…2*WAIT秒-Y, –proxy=on/off 打开或关闭代理-Q, –quota=NUMBER 设置下载的容量限制–limit-rate=RATE 限定下载输率目录参数：-nd –no-directories 不创建目录-x, –force-directories 强制创建目录-nH, –no-host-directories 不创建主机目录-P, –directory-prefix=PREFIX 将文件保存到目录 PREFIX/…–cut-dirs=NUMBER 忽略 NUMBER层远程目录HTTP 选项参数：–http-user=USER 设定HTTP用户名为 USER.–http-passwd=PASS 设定http密码为 PASS-C, –cache=on/off 允许/不允许服务器端的数据缓存 (一般情况下允许)-E, –html-extension 将所有text/html文档以.html扩展名保存–ignore-length 忽略 `Content-Length’头域–header=STRING 在headers中插入字符串 STRING–proxy-user=USER 设定代理的用户名为 USER–proxy-passwd=PASS 设定代理的密码为 PASS–referer=URL 在HTTP请求中包含 `Referer: URL’头-s, –save-headers 保存HTTP头到文件-U, –user-agent=AGENT 设定代理的名称为 AGENT而不是 Wget/VERSION–no-http-keep-alive 关闭 HTTP活动链接 (永远链接)–cookies=off 不使用 cookies–load-cookies=FILE 在开始会话前从文件 FILE中加载cookie–save-cookies=FILE 在会话结束后将 cookies保存到 FILE文件中FTP 选项参数：-nr, –dont-remove-listing 不移走 `.listing’文件-g, –glob=on/off 打开或关闭文件名的 globbing机制–passive-ftp 使用被动传输模式 (缺省值).–active-ftp 使用主动传输模式–retr-symlinks 在递归的时候，将链接指向文件(而不是目录)递归下载参数：-r, –recursive 递归下载－－慎用!-l, –level=NUMBER 最大递归深度 (inf 或 0 代表无穷)–delete-after 在现在完毕后局部删除文件-k, –convert-links 转换非相对链接为相对链接-K, –backup-converted 在转换文件X之前，将之备份为 X.orig-m, –mirror 等价于 -r -N -l inf -nr-p, –page-requisites 下载显示HTML文件的所有图片递归下载中的包含和不包含(accept/reject)：-A, –accept=LIST 分号分隔的被接受扩展名的列表-R, –reject=LIST 分号分隔的不被接受的扩展名的列表-D, –domains=LIST 分号分隔的被接受域的列表–exclude-domains=LIST 分号分隔的不被接受的域的列表–follow-ftp 跟踪HTML文档中的FTP链接–follow-tags=LIST 分号分隔的被跟踪的HTML标签的列表-G, –ignore-tags=LIST 分号分隔的被忽略的HTML标签的列表-H, –span-hosts 当递归时转到外部主机-L, –relative 仅仅跟踪相对链接-I, –include-directories=LIST 允许目录的列表-X, –exclude-directories=LIST 不被包含目录的列表-np, –no-parent 不要追溯到父目录wget -S –spider url 不下载只显示过程 功能用于从网络上下载资源，没有指定目录，下载资源回默认为当前目录。wget虽然功能强大，但是使用起来还是比较简单： 1）支持断点下传功能；这一点，也是网络蚂蚁和FlashGet当年最大的卖点，现在，Wget也可以使用此功能，那些网络不是太好的用户可以放心了； 2）同时支持FTP和HTTP下载方式；尽管现在大部分软件可以使用HTTP方式下载，但是，有些时候，仍然需要使用FTP方式下载软件； 3）支持代理服务器；对安全强度很高的系统而言，一般不会将自己的系统直接暴露在互联网上，所以，支持代理是下载软件必须有的功能； 4）设置方便简单；可能，习惯图形界面的用户已经不是太习惯命令行了，但是，命令行在设置上其实有更多的优点，最少，鼠标可以少点很多次，也不要担心是否错点鼠标； 5）程序小，完全免费；程序小可以考虑不计，因为现在的硬盘实在太大了；完全免费就不得不考虑了，即使网络上有很多所谓的免费软件，但是，这些软件的广告却不是我们喜欢的。 实例使用wget下载单个文件 1wget http://www.linuxde.net/testfile.zip 以下的例子是从网络下载一个文件并保存在当前目录，在下载的过程中会显示进度条，包含（下载完成百分比，已经下载的字节，当前下载速度，剩余下载时间）。 下载并以不同的文件名保存 1wget -O wordpress.zip http://www.linuxde.net/download.aspx?id=1080 wget默认会以最后一个符合/的后面的字符来命令，对于动态链接的下载通常文件名会不正确。 错误：下面的例子会下载一个文件并以名称download.aspx?id=1080保存: 1wget http://www.linuxde.net/download?id=1 即使下载的文件是zip格式，它仍然以download.php?id=1080命令。 正确：为了解决这个问题，我们可以使用参数-O来指定一个文件名： 1wget -O wordpress.zip http://www.linuxde.net/download.aspx?id=1080 wget限速下载 1wget --limit-rate=300k http://www.linuxde.net/testfile.zip 当你执行wget的时候，它默认会占用全部可能的宽带下载。但是当你准备下载一个大文件，而你还需要下载其它文件时就有必要限速了。 使用wget断点续传 1wget -c http://www.linuxde.net/testfile.zip 使用wget -c重新启动下载中断的文件，对于我们下载大文件时突然由于网络等原因中断非常有帮助，我们可以继续接着下载而不是重新下载一个文件。需要继续中断的下载时可以使用-c参数。 使用wget后台下载 1234wget -b http://www.linuxde.net/testfile.zipContinuing in background, pid 1840.Output will be written to `wget-log&apos;. 对于下载非常大的文件的时候，我们可以使用参数-b进行后台下载，你可以使用以下命令来察看下载进度： 1tail -f wget-log 伪装代理名称下载 1wget --user-agent=&quot;Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.16 (KHTML, like Gecko) Chrome/10.0.648.204 Safari/534.16&quot; http://www.linuxde.net/testfile.zip 有些网站能通过根据判断代理名称不是浏览器而拒绝你的下载请求。不过你可以通过--user-agent参数伪装。 测试下载链接 当你打算进行定时下载，你应该在预定时间测试下载链接是否有效。我们可以增加--spider参数进行检查。 1wget --spider URL 如果下载链接正确，将会显示: 12345Spider mode enabled. Check if remote file exists.HTTP request sent, awaiting response... 200 OKLength: unspecified [text/html]Remote file exists and could contain further links,but recursion is disabled -- not retrieving. 这保证了下载能在预定的时间进行，但当你给错了一个链接，将会显示如下错误: 1234wget --spider urlSpider mode enabled. Check if remote file exists.HTTP request sent, awaiting response... 404 Not FoundRemote file does not exist -- broken link!!! 你可以在以下几种情况下使用--spider参数： 定时下载之前进行检查 间隔检测网站是否可用 检查网站页面的死链接 增加重试次数 1wget --tries=40 URL 如果网络有问题或下载一个大文件也有可能失败。wget默认重试20次连接下载文件。如果需要，你可以使用--tries增加重试次数。 下载多个文件 1wget -i filelist.txt 首先，保存一份下载链接文件： 12345cat &gt; filelist.txturl1url2url3url4 接着使用这个文件和参数-i下载。 镜像网站 1wget --mirror -p --convert-links -P ./LOCAL URL 下载整个网站到本地。 --miror开户镜像下载。 -p下载所有为了html页面显示正常的文件。 --convert-links下载后，转换成本地的链接。 -P ./LOCAL保存所有文件和目录到本地指定目录。 过滤指定格式下载 1wget --reject=gif ur 下载一个网站，但你不希望下载图片，可以使用这条命令。 把下载信息存入日志文件 1wget -o download.log URL 不希望下载信息直接显示在终端而是在一个日志文件，可以使用。 限制总下载文件大小 1wget -Q5m -i filelist.txt 当你想要下载的文件超过5M而退出下载，你可以使用。注意：这个参数对单个文件下载不起作用，只能递归下载时才有效。 下载指定格式文件 1wget -r -A.pdf url 可以在以下情况使用该功能： 下载一个网站的所有图片。 下载一个网站的所有视频。 下载一个网站的所有PDF文件。 FTP下载 12wget ftp-urlwget --ftp-user=USERNAME --ftp-password=PASSWORD url 可以使用wget来完成ftp链接的下载。 使用wget匿名ftp下载： 1wget ftp-url 使用wget用户名和密码认证的ftp下载： 1wget --ftp-user=USERNAME --ftp-password=PASSWORD url 参考链接： http://www.cnblogs.com/peida/archive/2013/03/18/2965369.html http://man.linuxde.net/wget]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-netstat命令]]></title>
    <url>%2F2018%2F10%2F23%2FLinux%E5%91%BD%E4%BB%A4-netstat%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[用于显示各种网络相关信息，如网络连接，路由表，接口状态 (Interface Statistics)，masquerade 连接，多播成员 (Multicast Memberships) 等等。 语法netstat [-acCeFghilMnNoprstuvVwx] 选项1234567-a 或–all 显示所有连线中的Socket-c 或–continuous 持续列出网络状态-h 或–help 在线帮助-l 或–listening 显示监控中的服务器的Socket-n 或–numeric 不解析主机名-t 或–tcp 显示TCP 传输协议的连线状况-u或–udp 显示UDP传输协议的连线状况 注意：LISTEN和LISTENING的状态只有用-a或者-l才能看到 网络连接状态详解共有12中可能的状态，前面11种是按照TCP连接建立的三次握手和TCP连接断开的四次挥手过程来描述的。1)、LISTEN:首先服务端需要打开一个socket进行监听，状态为LISTEN./ The socket is listening for incoming connections. 侦听来自远方TCP端口的连接请求 / 2)、 SYN_SENT:客户端通过应用程序调用connect进行active open.于是客户端tcp发送一个SYN以请求建立一个连接.之后状态置为SYN_SENT./The socket is actively attempting to establish a connection. 在发送连接请求后等待匹配的连接请求 / 3)、 SYN_RECV:服务端应发出ACK确认客户端的 SYN,同时自己向客户端发送一个SYN. 之后状态置为SYN_RECV/ A connection request has been received from the network. 在收到和发送一个连接请求后等待对连接请求的确认 / 4)、ESTABLISHED: 代表一个打开的连接，双方可以进行或已经在数据交互了。/ The socket has an established connection. 代表一个打开的连接，数据可以传送给用户 / 5)、 FIN_WAIT1:主动关闭(active close)端应用程序调用close，于是其TCP发出FIN请求主动关闭连接，之后进入FIN_WAIT1状态./ The socket is closed, and the connection is shutting down. 等待远程TCP的连接中断请求，或先前的连接中断请求的确认 / 6)、CLOSE_WAIT:被动关闭(passive close)端TCP接到FIN后，就发出ACK以回应FIN请求(它的接收也作为文件结束符传递给上层应用程序),并进入CLOSE_WAIT./ The remote end has shut down, waiting for the socket to close. 等待从本地用户发来的连接中断请求 / 7)、FIN_WAIT2:主动关闭端接到ACK后，就进入了 FIN-WAIT-2 ./ Connection is closed, and the socket is waiting for a shutdown from the remote end. 从远程TCP等待连接中断请求 / 8)、LAST_ACK:被动关闭端一段时间后，接收到文件结束符的应用程 序将调用CLOSE关闭连接。这导致它的TCP也发送一个 FIN,等待对方的ACK.就进入了LAST-ACK ./ The remote end has shut down, and the socket is closed. Waiting for acknowledgement. 等待原来发向远程TCP的连接中断请求的确认 / 9)、TIME_WAIT:在主动关闭端接收到FIN后，TCP 就发送ACK包，并进入TIME-WAIT状态。/ The socket is waiting after close to handle packets still in the network.等待足够的时间以确保远程TCP接收到连接中断请求的确认 / 10)、CLOSING: 比较少见./ Both sockets are shut down but we still don’t have all our data sent. 等待远程TCP对连接中断的确认 / 11)、CLOSED: 被动关闭端在接受到ACK包后，就进入了closed的状态。连接结束./ The socket is not being used. 没有任何连接状态 / 12)、UNKNOWN: 未知的Socket状态。/ The state of the socket is unknown. / SYN: (同步序列编号,Synchronize Sequence Numbers)该标志仅在三次握手建立TCP连接时有效。表示一个新的TCP连接请求。ACK: (确认编号,Acknowledgement Number)是对TCP请求的确认标志,同时提示对端系统已经成功接收所有数据。FIN: (结束标志,FINish)用来结束一个TCP回话.但对应端口仍处于开放状态,准备接收后续数据。 常用实例1） 查看TCP的连接状态 123netstat -natlp | awk &apos;&#123;print $6&#125;&apos; | sort | uniq -c |sort -rnnetstat -n | awk &apos;/^tcp/ &#123;++S[$NF]&#125;;END &#123;for(a in S) print a,S[a]&#125;&apos;netstat -n | awk &apos;/^tcp/ &#123;print $NF&#125;&apos; | sort |uniq -c | sort -rn 2） 查找请求数较多的前20个IP（常用于查找攻来源） 123netstat -anpl | grep 80 | grep tcp | awk &apos;&#123;print $5&#125;&apos; | awk -F: &apos;&#123;print $1&#125;&apos; | sort | uniq -c | sort -nr | head -20netstat -ant | awk &apos;/:80/&#123;split($5,ip,&quot;:&quot;)&apos;;++A[ip[1]]&#125;END&#123;for(i in A) print A[i],i&#125;&apos; | sort -rn |head -20 tcpdump -i eth0 -tnn dst port 80 -c 1000 | awk -F&quot;.&quot; &apos;&#123;print $1&quot;.&quot;$2&quot;.&quot;$3&quot;.&quot;$4&#125;&apos; | sort | uniq -c | sort -nr | head -20]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DES,AES,RSA之间的区别]]></title>
    <url>%2F2018%2F10%2F19%2FDES-AES-RSA%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[转载：https://blog.csdn.net/m0_37543627/article/details/71244473 BASE64：编码方式（8位字节代码），二进制与字符串相互转换 MD5：Message Algorithm（消息摘要算法第五版），散列函数（哈希算法）_不可逆，压缩性 DES：Data Encrytion Standard（数据加密标准），对应算法是DEA ​ 特点：1. 对称加密 2. 同一个SK AES：Advanced Encrytion Standard（高级加密标准） ​ 特点：1. 对称加密 2. 一个SK扩展成多个子SK，轮加密 RSA：特点： 1. 非对称加密，即：PK与SK不是同一个 ​ 2. PK用于加密，SK用于解密 ​ 3. PK决定SK，但是PK很难算出SK（数学原理：两个大质数相乘，积很难因式分解） ​ 4. 速度慢，只对少量数据加密 相关总结图片： DES加密： AES加密： RSA加密： SSL使用RSA：]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>加密算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DES加密教程详细解读]]></title>
    <url>%2F2018%2F10%2F19%2FDES%E5%8A%A0%E5%AF%86%E6%95%99%E7%A8%8B%E8%AF%A6%E7%BB%86%E8%A7%A3%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[本文来自 ：https://blog.csdn.net/baidu_36856113/article/details/53558795 1997年数据加密标准DES正式公布，其分组长度为64比特，密钥长度为64比特，其中8比特为奇偶校验位，所以实际长度为56比特。现在DES已经被AES所取代。 1）DES的加密过程 明文64位-&gt;初始置换IP-&gt;16轮加密变换-&gt;逆初始置换IP-1-&gt;密文 a.初始置换IP 这里的初始IP置换表的意思为把64位明文按照表中的规则替换，比如第一行，把64位明文的第1位换为其58位，第2位换为50位，第3位换位42位…这表仔细观察有一定的规律，比如我们从右到左一列一列的分别读前4行，后4行。是不是就是2 4 6 8 10 12…和1 3 5 7 9 11… b.16轮加密变换 看似很复杂，其实16轮的过程都是一样的。经过上面说的初始置换IP,接下来就是要把64位分为L0和R0各32位。然后算出f(R0,k1),其结果与L0异或作为R1.而L1直接是和R0相等的。 后面都一样： Li=Ri-1 Ri=Li-1异或f(Ri-1,ki) i=1,2,3,…16, 这时我们就会问那K1~K16从哪来，加密函数f()怎么计算，这些问题我在后面会继续写的。 c.初始逆置换IP-1 方法同初始置换IP。 b1.密钥生成 是不是一下子就能看明白，这里的PC-1为选择置换，可以去掉奇偶校验位。PC-2也是选择置换，它是用于从Ci和Di中选取48位作为密钥Ki。C0和D0是把密钥中实际的56位分成左右28位，LSi是表示对Ci-1和Di-1进行循环左移变换，其中LS1,LS2,LS9,LS16是循环左移1位，其余为2位。选择置换PC-157, 49, 41, 33, 25, 17, 9,1, 58, 50, 42, 34, 26, 18,10, 2, 59, 51, 43,35, 27,19, 11, 3, 60, 52, 44, 36, 63, 55, 47, 39, 31, 23, 15,7, 62, 54, 46, 38, 30, 22,14, 6, 61, 53, 45, 37, 29,21, 13, 5, 28, 20, 12, 4，选择置换PC-214, 17, 11, 24, 1, 5,3, 28, 15, 6, 21, 10,23, 19, 12, 4, 26, 8,16, 7, 27, 20, 13, 2,41, 52, 31, 37, 47, 55,30, 40, 51, 45, 33, 48,44, 49, 39, 56, 34, 53,46, 42, 50, 36, 29, 32b2.加密函数f() 这里的E是把32比特（即上面所提到的64比特分为俩个32比特。）的输入扩展为48比特。将E置换后的结果与48位的Ki异或的结果平均分为8组B1,B2…B8，每组Bi用Si盒处理。这样你会发现进去的是48位，出来的是32位。因为P为32位置换函数，它的输入为32位。把Bi{x1,x2,x3…x6}(48比特平均分为8组，每组就是6比特)中的x1x6和x2x3x4x5分别作为Si盒的行和列（比如x1x2x3…x6=101011,则x1x6=11把它转化为十进制为3。x2x3x4x5=0101转换为十进制为5，所以为3行5列，但是要注意Si盒的行列从0开始算起），找到这个十进制数，把它转化为4位二进制。这4位就是输出。 扩展变换E 这个也是有规律的把第一列和最后一列去掉看看。 S盒 S1 14, 4, 13, 1, 2, 15, 11, 8, 3, 10, 6, 12, 5, 9, 0, 7, 0, 15, 7, 4, 14, 2, 13, 1, 10, 6, 12, 11, 9, 5, 3, 8, 4, 1, 14, 8, 13, 6, 2, 11, 15, 12, 9, 7, 3, 10, 5, 0, 15, 12, 8, 2, 4, 9, 1, 7, 5, 11, 3, 14, 10, 0, 6, 13, S2 15, 1, 8, 14, 6, 11, 3, 4, 9, 7, 2, 13, 12, 0, 5, 10, 3, 13, 4, 7, 15, 2, 8, 14, 12, 0, 1, 10, 6, 9, 11, 5, 0, 14, 7, 11, 10, 4, 13, 1, 5, 8, 12, 6, 9, 3, 2, 15, 13, 8, 10, 1, 3, 15, 4, 2, 11, 6, 7, 12, 0, 5, 14, 9, S3 10, 0, 9, 14, 6, 3, 15, 5, 1, 13, 12, 7, 11, 4, 2, 8, 13, 7, 0, 9, 3, 4, 6, 10, 2, 8, 5, 14, 12, 11, 15, 1, 13, 6, 4, 9, 8, 15, 3, 0, 11, 1, 2, 12, 5, 10, 14, 7, 1, 10, 13, 0, 6, 9, 8, 7, 4, 15, 14, 3, 11, 5, 2, 12, S4 7, 13, 14, 3, 0, 6, 9, 10, 1, 2, 8, 5, 11, 12, 4, 15, 13, 8, 11, 5, 6, 15, 0, 3, 4, 7, 2, 12, 1, 10, 14, 9, 10, 6, 9, 0, 12, 11, 7, 13, 15, 1, 3, 14, 5, 2, 8, 4, 3, 15, 0, 6, 10, 1, 13, 8, 9, 4, 5, 11, 12, 7, 2, 14, S5 2, 12, 4, 1, 7, 10, 11, 6, 8, 5, 3, 15, 13, 0, 14, 9, 14, 11, 2, 12, 4, 7, 13, 1, 5, 0, 15, 10, 3, 9, 8, 6, 4, 2, 1, 11, 10, 13, 7, 8, 15, 9, 12, 5, 6, 3, 0, 14, 11, 8, 12, 7, 1, 14, 2, 13, 6, 15, 0, 9, 10, 4, 5, 3, S6 12, 1, 10, 15, 9, 2, 6, 8, 0, 13, 3, 4, 14, 7, 5, 11, 10, 15, 4, 2, 7, 12, 9, 5, 6, 1, 13, 14, 0, 11, 3, 8, 9, 14, 15, 5, 2, 8, 12, 3, 7, 0, 4, 10, 1, 13, 11, 6, 4, 3, 2, 12, 9, 5, 15, 10, 11, 14, 1, 7, 6, 0, 8, 13, S7 4, 11, 2, 14, 15, 0, 8, 13, 3, 12, 9, 7, 5, 10, 6, 1, 13, 0, 11, 7, 4, 9, 1, 10, 14, 3, 5, 12, 2, 15, 8, 6, 1, 4, 11, 13, 12, 3, 7, 14, 10, 15, 6, 8, 0, 5, 9, 2, 6, 11, 13, 8, 1, 4, 10, 7, 9, 5, 0, 15, 14, 2, 3, 12, S8 13, 2, 8, 4, 6, 15, 11, 1, 10, 9, 3, 14, 5, 0, 12, 7, 1, 15, 13, 8, 10, 3, 7, 4, 12, 5, 6, 11, 0, 14, 9, 2, 7, 11, 4, 1, 9, 12, 14, 2, 0, 6, 10, 13, 15, 3, 5, 8, 2, 1, 14, 7, 4, 10, 8, 13, 15, 12, 9, 0, 3, 5, 6, 11 , p置换 16, 7, 20, 21, 29, 12, 28, 17, 1, 15, 23, 26, 5, 18, 31, 10, 2, 8, 24, 14, 32, 27, 3, 9, 19, 13, 30, 6, 22, 11, 4, 25]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>加密算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GO语言Hash运算]]></title>
    <url>%2F2018%2F10%2F19%2FGO%E8%AF%AD%E8%A8%80Hash%E8%BF%90%E7%AE%97%2F</url>
    <content type="text"><![CDATA[散列函数（散列算法，又称哈希函数）是一种从任何一种数据中创建小的数字“指纹”的方法。散列函数把消息或数据压缩成摘要，使得数据量变小，将数据的格式固定下来。该函数将数据打乱混合，重新创建一个叫做散列值的指纹。 随机生成加密密钥需要尽可能的随机，以便生成的密钥很难再现。加密随机数生成器必须生成无法通过计算方法推算出（低于p&lt;.05的概率）的输出。 散列函数基本特性：如果两个散列值是不相同的（根据同一函数），那么这两个散列值的原始输入也是不相同的。这个特性是散列函数具有确定性的结果，具有这种性质的散列函数称为单向散列函数。但另一方面，散列函数的输入和输出不是唯一对应关系的，如果两个散列值相同，两个输入值很可能是相同的，但也可能不同，这种情况称为“散列碰撞”。 主要应用场景 文件校验 数字签名 鉴权协议 Go语言支持go crypto标准包包含了一些常用的哈希算法，例如md5、sha1、sha256、sha512等。以md5算法为例，了解下go如何生成哈希值。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package main import ( &quot;crypto/md5&quot; &quot;encoding/hex&quot; &quot;fmt&quot;)/* 使用MD5对数据进行哈希运算方法1：使用md5.Sum()方法*/func getMD5String_1(b[]byte) (result string) &#123; //给哈希算法添加数据 res:=md5.Sum(b) //返回值：[Size]byte 数组 /*//方法1： result=fmt.Sprintf(&quot;%x&quot;,res) //通过fmt.Sprintf()方法格式化数据 */ //方法2： result=hex.EncodeToString(res[:]) //对应的参数为：切片，需要将数组转换为切片。 return&#125;/*使用MD5对数据进行哈希运算方法2：使用md5.new()方法*/func getMD5String_2(b[]byte) (result string) &#123; //1、创建Hash接口 myHash:=md5.New() //返回 Hash interface //2、添加数据 myHash.Write(b) //写入数据 //3、计算结果 /* 执行原理为：myHash.Write(b1)写入的数据进行hash运算 + myHash.Sum(b2)写入的数据进行hash运算 结果为：两个hash运算结果的拼接。若myHash.Write()省略或myHash.Write(nil) ，则默认为写入的数据为“”。 根据以上原理，一般不采用两个hash运算的拼接，所以参数为nil */ res:=myHash.Sum(nil) //进行运算 //4、数据格式化 result=hex.EncodeToString(res) //转换为string return&#125;func main() &#123; str:=[]byte(&quot;jiangzhou&quot;) res:=getMD5String_1(str) fmt.Println(&quot;方法1运算结果：&quot;,res) res=getMD5String_2(str) fmt.Println(&quot;方法2运算结果：&quot;,res) &#125;]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>加密算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis搭建主从]]></title>
    <url>%2F2018%2F10%2F18%2Fredis%E6%90%AD%E5%BB%BA%E4%B8%BB%E4%BB%8E%2F</url>
    <content type="text"><![CDATA[1.6搭建主从主从概念 a) 一个master可以拥有多个slave，一个slave可以拥有多个slave，如此下去，形成了强⼤的多级服务器集群架构 b) master用来写数据，slave用来读数据，经统计：网站的读写比率是10:1 c) 通过主从配置可以实现读写分离 d) master和slave都是一个redis实例 主从配置 配置主 a) 查看当前主机的ip地址 1ifconfig a) 修改etc/redis/redis.conf文件” 12sudo vi redis.confbind 192.168.110.36 a) 重启redis服务 1ps aux | grep redis 12Sudo kill -9 12916Sudo redis-server /etc/redis/redis.conf 这里需要注意一下，如果以服务启动的话需要手动停止服务，命令 1sudo service redis stop //这里停止服务 停止服务完然后su一下，然后删除里面/var/lib/redis下面的dump文件 配置从 a) 复制etc/redis/redis.conf文件 1sudo cp redis.conf ./slave.conf a) 修改redis/slave.conf文件 1sudo vi slave.conf a) 编辑内容 123bind 192.168.110.36slaveof 192.168.110.36 6379port 6378 a) 开启redis服务 1sudo redis-server slave.conf a) 查看主从关系 1redis-cli -h 192.168.110.36 info Replication 操作数据 a) 进入主客户端 1redis-cli -h 192.168.110.36 -p 6379 b) 进入从客户端 1redis-cli -h 192.168.110.36 -p 6378 c) 在master上写数据 1mset a1 11 a2 22 a3 33 a) 在slave上读数据 注意：如果出错，可以查看日志文件，su 进入管理员用户 ，密码，然后cd /var/log/redis 然后查看redis.log文件]]></content>
      <categories>
        <category>环境搭建</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu下安装fastDFS使用]]></title>
    <url>%2F2018%2F10%2F18%2FUbuntu%E4%B8%8B%E5%AE%89%E8%A3%85fastDFS%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1.利用fastDFS存储1.1什么是FastDFSFastDFS 是用 c 语言编写的一款开源的分布式文件系统。FastDFS 为互联网量身定制， 充分考虑了冗余备份、负载均衡、线性扩容等机制，并注重高可用、高性能等指标，使用 FastDFS 很容易搭建一套高性能的文件服务器集群提供文件上传、下载等服务。 优点： FastDFS 架构包括 Tracker server 和 Storage server。客户端请求 Tracker server 进行文 件上传、下载，通过 Tracker server 调度最终由 Storage server 完成文件上传和下载。 Tracker server 作用是负载均衡和调度，通过 Tracker server 在文件上传时可以根据一些 方法找到 Storage server 提供文件上传服务。可以将 tracker 称为追踪服务器或调度服务 器。 Storage server 作用是文件存储，客户端上传的文件最终存储在 Storage 服务器上， Storageserver 没有实现自己的文件系统而是利用操作系统 的文件系统来管理文件。可以将 storage 称为存储服务器。 服务端两个角色: Tracker:管理集群，tracker 也可以实现集群。每个 tracker 节点地位平等。收集 Storage 集群的状态。 Storage:实际保存文件 Storage 分为多个组，每个组之间保存的文件是不同的。每 个组内部可以有多个成员，组成员内部保存的内容是一样的，组成员的地位是一致的，没有 主从的概念。 1.2文件上传流程 客户端上传文件后存储服务器将文件 ID 返回给客户端，此文件 ID 用于以后访问该文 件的索引信息。文件索引信息包括:组名，虚拟磁盘路径，数据两级目录，文件名。 组名: 文件上传后所在的 storage 组名称，在文件上传成功后有 storage 服务器返回， 需要客户端自行保存。 虚拟磁盘路径: storage 配置的虚拟路径，与磁盘选项 store_path*对应。如果配置了 store_path0 则是 M00，如果配置了 store_path1 则是 M01，以此类推。 数据两级目录 :storage 服务器在每个虚拟磁盘路径下创建的两级目录，用于存储数据 文件。 文件名 :与文件上传时不同。是由存储服务器根据特定信息生成，文件名包含:源存储 服务器 IP 地址、文件创建时间戳、文件大小、随机数和文件拓展名等信息。 1.3文件下载流程 1.4简易FastDFS架构 1.5FastDFS安装先下载这几个安装包备用 1.5.1安装FastDFS依赖包 解压缩libfastcommon-master.zip 进入到libfastcommon-master的目录中 执行./make.sh 执行sudo ./make.sh install 1.5.2安装FastDFS 解压缩fastdfs-master.zip 进入到 fastdfs-master目录中 执行 ./make.sh 执行 sudo ./make.sh install 1.5.3配置跟踪服务器tracker 1sudo cp /etc/fdfs/tracker.conf.sample /etc/fdfs/tracker.conf 在/home/song/目录中创建目录 fastdfs/tracker 1mkdir –p /home/song/fastdfs/tracker 编辑/etc/fdfs/tracker.conf配置文件 sudo vim /etc/fdfs/tracker.conf ​ 修改 base_path=/home/song/fastdfs/tracker 1.5.4配置存储服务器storage 1sudo cp /etc/fdfs/storage.conf.sample /etc/fdfs/storage.conf 在/home/song/fastdfs/ 目录中创建目录 storage 1mkdir –p /home/song/fastdfs/storage 编辑/etc/fdfs/storage.conf配置文件 sudo vim /etc/fdfs/storage.conf 修改内容： 123base_path=/home/song/fastdfs/storagestore_path0=/home/song/fastdfs/storagetracker_server=自己ubuntu虚拟机的ip地址:22122 1.5.5启动tracker和storage进入到/etc/fdfs/下面执行以下两条指令 12sudo fdfs_trackerd /etc/fdfs/tracker.confsudo fdfs_storaged /etc/fdfs/storage.conf 1.5.6测试是否安装成功 sudo cp /etc/fdfs/client.conf.sample /etc/fdfs/client.conf 编辑/etc/fdfs/client.conf配置文件 sudo vim /etc/fdfs/client.conf 修改内容： 12base_path=/home/song/fastdfs/trackertracker_server=自己ubuntu虚拟机的ip地址:22122 上传文件测试(fastDHT) sudo fdfs_upload_file /etc/fdfs/client.conf 要上传的图片文件 如果返回类似group1/M00/00/00/rBIK6VcaP0aARXXvAAHrUgHEviQ394.jpg 的文件id则说明文件上传成功 1.5.7安装fastdfs-nginx-module 解压缩 nginx-1.8.1.tar.gz 解压缩 fastdfs-nginx-module-master.zip 进入nginx-1.8.1目录中 执行 1sudo ./configure --prefix=/usr/local/nginx/ --add-module=fastdfs-nginx-module-master解压后的目录的绝对路径/src 注意：这时候会报一个错，说没有PCRE库 下载缺少的库 1sudo apt-get install libpcre3 libpcre3-dev 首先你需要去更换源，因为ubuntu自带的源没有这个库 更换下载源为阿里的源 先把原来的源文件备份 1sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak 编辑源文件 1sudo vim /etc/apt/sources.list 把原来的内容全部删掉，粘贴一下内容： 1234567891011121314deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiverse deb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse 更换完源之后执行 12sudo apt-get updatesudo apt-get install libpcre3 libpcre3-dev 然后进入nginx-1.8.1目录中，再次执行： 1sudo ./configure --prefix=/usr/local/nginx/ --add-module=fastdfs-nginx-module-master解压后的目录的绝对路径/src 这时候还会报一个错（错误还真多），错误原因是因为nginx编译的时候把警告当错误处理，事实上这个警告并不影响（程序员忽略警告） 解决方法： 找到objs目录下的Makefile vim Makefile 删掉里面的-Werror(如果没有修改权限，修改一下这个文件的权限,chmod 777 Makefile) 然后回到nginx-1.8.1目录中，再次执行： 执行完成后执行sudo make 执行sudo make install 1sudo ./configure --prefix=/usr/local/nginx/ --add-module=fastdfs-nginx-module-master解压后的目录的绝对路径/src sudo cp fastdfs-nginx-module-master解压后的目录中src下mod_fastdfs.conf /etc/fdfs/mod_fastdfs.conf sudo vim /etc/fdfs/mod_fastdfs.conf 修改内容： 1234connect_timeout=10tracker_server=自己ubuntu虚拟机的ip地址:22122url_have_group_name=truestore_path0=/home/song/fastdfs/storage sudo cp 解压缩的fastdfs-master目录中的conf中的http.conf /etc/fdfs/http.conf sudo cp 解压缩的fastdfs-master目录中conf中的mime.types /etc/fdfs/mime.types sudo vim /usr/local/nginx/conf/nginx.conf 在http部分中添加配置信息如下： 1234567891011server &#123; listen 8888; server_name localhost; location ~/group[0-9]/ &#123; ngx_fastdfs_module; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; 启动nginx sudo /usr/local/nginx/sbin/nginx 到这里我们就已经安装完成了下面可以用go客户端来测试一下 1.6使用go客户端上传文件测试 下载包 1go get github.com/weilaihui/fdfs_client 这时候会报一个错： 这是因为我们的网络有防火墙，不能直接去google下载相应的包，所以就失败啦 解决办法： 在~/workspace/go/src目录下面创建一个golang.org/x目录 12cd ~/workspace/go/srcmkdir -p golang.org/x 进入golang.org/x下载两个包 123cd golang.org/xgit clone https://github.com/golang/crypto.gitgit clone https://github.com/golang/sys.git 然后再执行最初的下载命令 1go get github.com/weilaihui/fdfs_client go操作fastDFS的方法 先导包，把我们下载的包导入 1import "github.com/weilaihui/fdfs_client" 导包之后,我们需要指定配置文件生成客户端对象 1client,_:=fdfs_client.NewFdfsClient("/etc/fdfs/client.conf") 接着我们就可以通过client对象执行文件上传，上传有两种方法，一种是通过文件名，一种是通过字节流 通过文件名上传UploadByFilename ,参数是文件名（必须通过文件名能找到要上传的文件），返回值是fastDFS定义的一个结构体，包含组名和文件ID两部分内容 1fdfsresponse,err := client.UploadByFilename("flieName") 通过字节流上传UploadByBuffer,参数是字节数组和文件后缀，返回值和通过文件名上传一样。 1fdfsresponse,err := client.UploadByBuffer(fileBuffer,ext) 测试代码123456789101112131415161718192021222324252627282930//先导包import &quot;github.com/weilaihui/fdfs_client&quot;//通过GetFile获取文件信息f,h,err := this.GetFile(filePath)defer f.Close()//然后对上传的文件进行格式和大小判断//1.判断文件格式ext := path.Ext(h.Filename)if ext != &quot;.jpg&quot; &amp;&amp; ext != &quot;.png&quot;&amp;&amp;ext != &quot;.jpeg&quot;&#123; beego.Info(&quot;上传文件格式不正确&quot;) return &quot;&quot;&#125;//2.文件大小if h.Size&gt;5000000&#123; beego.Info(&quot;文件太大，不允许上传&quot;) return &quot;&quot;&#125;//3.上传文件//先获取一个[]bytefileBuffer := make([]byte,h.Size)//把文件数据读入到fileBuffer中f.Read(fileBuffer)//获取client对象client := fdfs_client.NewFdfsClient(&quot;/etc/fdfs/client.conf&quot;)//上传fdfsresponse,_:=client.UploadByBuffer(fileBuffer,ext)//返回文件IDreturn fdfsresponse.RemoteFileId]]></content>
      <categories>
        <category>环境搭建</category>
      </categories>
      <tags>
        <tag>fastDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GO语言RSA加密解密]]></title>
    <url>%2F2018%2F10%2F18%2FGO%E8%AF%AD%E8%A8%80RSA%E5%8A%A0%E5%AF%86%E8%A7%A3%E5%AF%86%2F</url>
    <content type="text"><![CDATA[对称加密中，加密和解密使用相同的密钥，因此必须向解密者配送密钥，即密钥配送问题。而非对称加密中，由于加密和解密分别使用公钥和私钥，而公钥是公开的，因此可以规避密钥配送问题。非对称加密算法，也称公钥加密算法。 1977年，Ron Rivest、Adi Shamir、Leonard Adleman三人在美国公布了一种公钥加密算法，即RSA公钥加密算法。RSA是目前最有影响力和最常用的公钥加密算法，可以说是公钥加密算法的事实标准。 一、RSA加密原理 使用M和C分别表示明文和密文，则RSA加密、解密过程如下： 其中e、n的组合(e, n)即为公钥，d、n的组合(d, n)即为私钥。当然e、d、n并非任意取值，需要符合一定条件，如下即为e、d、n的求解过程。 生成密钥对 e、d、n的求解过程，也即生成密钥对的过程。涉及如下步骤： 1、取两个大质数（也称素数）p、q，n = pq。 2、取正整数e、d，使得ed mod (p-1)(q-1) = 1，也即：ed ≡ 1 mod (p-1)(q-1)。 e和d是模(p-1)(q-1)的乘法逆元，仅当e与(p-1)(q-1)互质时，存在d。 举例验证： 1、取p、q分别为13、17，n = pq = 221。 2、而(p-1)(q-1) = 12x16 = 192，取e、d分别为13、133，有13x133 mod 192 = 1 取明文M = 60，公钥加密、私钥解密，加密和解密过程分别如下： RSA加密原理证明过程 手动求解密钥对中的d ed mod (p-1)(q-1) = 1，已知e和(p-1)(q-1)求d，即求e对模(p-1)(q-1)的乘法逆元。 如上面例子中，p、q为13、17，(p-1)(q-1)=192，取e=13，求13d mod 192 = 1中的d。 13d ≡ 1 (mod 192)，在右侧添加192的倍数，使计算结果可以被13整除。 13d ≡ 1 + 192x9 ≡ 13x133 (mod 192)，因此d = 133 其他计算方法有：费马小定律、扩展欧几里得算法、欧拉定理。 RSA安全性 由于公钥公开，即e、n公开。 因此破解RSA私钥，即为已知e、n情况下求d。 因ed mod (p-1)(q-1) = 1，且n=pq，因此该问题演变为：对n质因数分解求p、q。 目前已被证明，已知e、n求d和对n质因数分解求p、q两者是等价的。实际中n长度为2048位以上，而当n&gt;200位时分解n是非常困难的，因此RSA算法目前仍被认为是安全实用的。 RSA计时*和防范 RSA解密的本质是模幂运算，即： 其中C为密文，(d,n)为私钥，均为超过1024位的大数运算，直接计算并不可行，因此最经典的算法为蒙哥马利算法。而这种计算是比较是耗时的，因此者可以观察不同的输入对应的解密时间，通过分析推断私钥，称为计时。而防范RSA计时的办法，即在解密时加入随机因素，使得*者无法准确获取解密时间。 具体实现步骤如下： 二、Go RSA加密解密1、rsa加解密，必然会去查crypto/ras这个包 Package rsa implements RSA encryption as specified in PKCS#1. 这是该包的说明：实现RSA加密技术，基于PKCS#1规范。 对于什么是PKCS#1，可以查阅相关资料。PKCS（公钥密码标准），而#1就是RSA的标准。可以查看：PKCS系列简介 从该包中函数的名称，可以看到有两对加解密的函数。 EncryptOAEP和DecryptOAEPEncryptPKCS1v15和DecryptPKCS1v15 这称作加密方案，详细可以查看，PKCS #1 v2.1 RSA 算法标准 可见，当与其他语言交互时，需要确定好使用哪种方案。 PublicKey和PrivateKey两个类型分别代表公钥和私钥，关于这两个类型中成员该怎么设置，这涉及到RSA加密算法，本文中，这两个类型的实例通过解析文章开头生成的密钥得到。 2、解析密钥得到PublicKey和PrivateKey的实例这个过程，我也是花了好些时间（主要对各种加密的各种东东不熟）：怎么将openssl生成的密钥文件解析到公钥和私钥实例呢？ 在encoding/pem包中，看到了—–BEGIN Type—–这样的字样，这正好和openssl生成的密钥形式差不多，那就试试。 在该包中，一个block代表的是PEM编码的结构，关于PEM，请查阅相关资料。我们要解析密钥，当然用Decode方法： func Decode(data []byte) (p *Block, rest []byte) 这样便得到了一个Block的实例（指针）。 解析来看crypto/x509。为什么是x509呢？这又涉及到一堆概念。先不管这些，我也是看encoding和crypto这两个包的子包摸索出来的。在x509包中，有一个函数： func ParsePKIXPublicKey(derBytes []byte) (pub interface{}, err error) 从该函数的说明：ParsePKIXPublicKey parses a DER encoded public key. These values are typically found in PEM blocks with “BEGIN PUBLIC KEY”。可见这就是解析PublicKey的。另外，这里说到了PEM，可以上面的encoding/pem对了。（PKIX是啥东东，查看这里 ） 而解析私钥的，有好几个方法，从上面的介绍，我们知道，RSA是PKCS#1，刚好有一个方法： func ParsePKCS1PrivateKey(der []byte) (key *rsa.PrivateKey, err error) 返回的就是rsa.PrivateKey。 代码实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112package main import ( &quot;crypto/rsa&quot; &quot;crypto/rand&quot; &quot;crypto/x509&quot; &quot;encoding/pem&quot; &quot;os&quot; &quot;fmt&quot;) func RSAGenKey(bits int) error &#123; /* 生成私钥 */ //1、使用RSA中的GenerateKey方法生成私钥 privateKey, err := rsa.GenerateKey(rand.Reader, bits) if err != nil &#123; return err &#125; //2、通过X509标准将得到的RAS私钥序列化为：ASN.1 的DER编码字符串 privateStream := x509.MarshalPKCS1PrivateKey(privateKey) //3、将私钥字符串设置到pem格式块中 block1 := pem.Block&#123; Type: &quot;private key&quot;, Bytes: privateStream, &#125; //4、通过pem将设置的数据进行编码，并写入磁盘文件 fPrivate, err := os.Create(&quot;privateKey.pem&quot;) if err != nil &#123; return err &#125; defer fPrivate.Close() err = pem.Encode(fPrivate, &amp;block1) if err != nil &#123; return err &#125; /* 生成公钥 */ publicKey:=privateKey.PublicKey publicStream,err:=x509.MarshalPKIXPublicKey(&amp;publicKey) //publicStream:=x509.MarshalPKCS1PublicKey(&amp;publicKey) block2:=pem.Block&#123; Type:&quot;public key&quot;, Bytes:publicStream, &#125; fPublic,err:=os.Create(&quot;publicKey.pem&quot;) if err!=nil &#123; return err &#125; defer fPublic.Close() pem.Encode(fPublic,&amp;block2) return nil&#125;//对数据进行加密操作func EncyptogRSA(src []byte,path string) (res []byte,err error) &#123; //1.获取秘钥（从本地磁盘读取） f,err:=os.Open(path) if err!=nil &#123; return &#125; defer f.Close() fileInfo,_:=f.Stat() b:=make([]byte,fileInfo.Size()) f.Read(b) // 2、将得到的字符串解码 block,_:=pem.Decode(b) // 使用X509将解码之后的数据 解析出来 //x509.MarshalPKCS1PublicKey(block):解析之后无法用，所以采用以下方法：ParsePKIXPublicKey keyInit,err:=x509.ParsePKIXPublicKey(block.Bytes) //对应于生成秘钥的x509.MarshalPKIXPublicKey(&amp;publicKey) //keyInit1,err:=x509.ParsePKCS1PublicKey(block.Bytes) if err!=nil &#123; return &#125; //4.使用公钥加密数据 pubKey:=keyInit.(*rsa.PublicKey) res,err=rsa.EncryptPKCS1v15(rand.Reader,pubKey,src) return&#125;//对数据进行解密操作func DecrptogRSA(src []byte,path string)(res []byte,err error) &#123; //1.获取秘钥（从本地磁盘读取） f,err:=os.Open(path) if err!=nil &#123; return &#125; defer f.Close() fileInfo,_:=f.Stat() b:=make([]byte,fileInfo.Size()) f.Read(b) block,_:=pem.Decode(b)//解码 privateKey,err:=x509.ParsePKCS1PrivateKey(block.Bytes)//还原数据 res,err=rsa.DecryptPKCS1v15(rand.Reader,privateKey,src) return&#125;func main() &#123; //rsa.GenerateKey() err:=RSAGenKey(4096) if err!=nil &#123; fmt.Println(err) return &#125; fmt.Println(&quot;秘钥生成成功！&quot;) str:=&quot;山重水复疑无路，柳暗花明又一村！&quot; fmt.Println(&quot;加密之前的数据为：&quot;,string(str)) data,err:=EncyptogRSA([]byte(str),&quot;publicKey.pem&quot;) data,err=DecrptogRSA(data,&quot;privateKey.pem&quot;) fmt.Println(&quot;加密之后的数据为：&quot;,string(data))&#125;]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>加密算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GO语言AES加密解密]]></title>
    <url>%2F2018%2F10%2F18%2FGO%E8%AF%AD%E8%A8%80AES%E5%8A%A0%E5%AF%86%E8%A7%A3%E5%AF%86%2F</url>
    <content type="text"><![CDATA[基本概念 密码学中，块密码的工作模式（英语：mode of operation）允许使用同一个块密码密钥对多于一块的数据进行加密，并保证其安全性。 块密码自身只能加密长度等于密码块长度的单块数据，若要加密变长数据，则数据必须先被划分为一些单独的密码块。通常而言，最后一块数据也需要使用合适填充方式将数据扩展到符合密码块大小的长度。 一种工作模式描述了加密每一数据块的过程，并常常使用基于一个通常称为初始化向量的附加输入值以进行随机化，以保证安全。 常见的模式有ECB，CBC，OFB，CFB，CTR和XTS等 加密模式仅仅保证 机密性 ，对于保证 完整性 或未篡改，需要采用分离的消息验证码，例如CBC-MAC。密码学社群认识到了对专用的保证完整性的方法的需求，NIST因此提出了HMAC，CMAC和GMAC。 在发现将认证模式与加密模式联合起来的难度之后，密码学社区开始研究结合了加密和认证的单一模式，这种模式被称为认证加密模式（AE，Authenticated Encryption），或称为authenc。AE模式的例子包括CCM，GCM[11]，CWC，EAX，IAPM和OCB。 初始化向量(IV)初始化向量（IV，Initialization Vector）是许多工作模式中用于随机化加密的一块数据，因此可以由相同的明文，相同的密钥产生不同的密文，而无需重新产生密钥，避免了通常相当复杂的这一过程。 初始化向量与密钥相比有不同的安全性需求，因此IV通常无须保密，然而在大多数情况中，不应当在使用同一密钥的情况下两次使用同一个IV。对于CBC和CFB，重用IV会导致泄露明文首个块的某些信息，亦包括两个不同消息中相同的前缀。对于OFB和CTR而言，重用IV会导致完全失去安全性。另外，在CBC模式中，IV在加密时必须是无法预测的；特别的，在许多实现中使用的产生IV的方法，例如SSL2.0使用的，即采用上一个消息的最后一块密文作为下一个消息的IV，是不安全的。 填充 部分模式(ECB和CBC)需要最后一块在加密前进行填充 CFB，OFB和CTR模式不需要对长度不为密码块大小整数倍的消息进行特别的处理。因为这些模式是通过对块密码的输出与平文进行异或工作的。最后一个平文块（可能是不完整的）与密钥流块的前几个字节异或后，产生了与该平文块大小相同的密文块。流密码的这个特性使得它们可以应用在需要密文和平文数据长度严格相等的场合，也可以应用在以流形式传输数据而不便于进行填充的场合。 举例两个填充代码 1234567891011121314151617181920212223242526func PKCS5Padding(ciphertext []byte, blockSize int) []byte &#123; padding := blockSize - len(ciphertext)%blockSize//需要padding的数目 //只要少于256就能放到一个byte中，默认的blockSize=16(即采用16*8=128, AES-128长的密钥) //最少填充1个byte，如果原文刚好是blocksize的整数倍，则再填充一个blocksize padtext := bytes.Repeat([]byte&#123;byte(padding)&#125;, padding)//生成填充的文本 return append(ciphertext, padtext...)&#125; func PKCS5UnPadding(origData []byte) []byte &#123; length := len(origData) unpadding := int(origData[length-1]) return origData[:(length - unpadding)]&#125; func ZeroPadding(ciphertext []byte, blockSize int) []byte &#123; padding := blockSize - len(ciphertext)%blockSize padtext := bytes.Repeat([]byte&#123;0&#125;, padding)//用0去填充 return append(ciphertext, padtext...)&#125; func ZeroUnPadding(origData []byte) []byte &#123; return bytes.TrimFunc(origData, func(r rune) bool &#123; return r == rune(0) &#125;)&#125; 常见模式ECB（Electronic codebook）模式加密： 对每个密码块应用秘钥，缺点在于同样的平文块会被加密成相同的密文块；因此，它不能很好的隐藏数据模式。在某些场合，这种方法不能提供严格的数据保密性，因此并不推荐用于密码协议中。下面的例子显示了ECB在密文中显示平文的模式的程度：该图像的一个位图版本（左图）通过ECB模式可能会被加密成中图，而非ECB模式通常会将其加密成下图 而且由于每个块分别加密，用它的协议本身不能提供数据完整性保护，易收到重放攻击的影响。 CBC（Cipher-block chaining）模式加密： 解密： 在CBC模式中，每个平文块先与前一个密文块进行异或后，再进行加密。在这种方法中，每个密文块都依赖于它前面的所有平文块。同时，为了保证每条消息的唯一性，在第一个块中需要使用初始化向量。 CBC是最为常用的工作模式。它的主要缺点在于加密过程是串行的，无法被并行化，而且消息必须被填充到块大小的整数倍。解决后一个问题的一种方法是利用密文窃取。 注意在加密时，平文中的微小改变会导致其后的全部密文块发生改变，而在解密时，从两个邻接的密文块中即可得到一个平文块。因此，解密过程可以被并行化，而解密时，密文中一位的改变只会导致其对应的平文块完全改变和下一个平文块中对应位发生改变，不会影响到其它平文的内容。 代码实例：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package main import ( &quot;bytes&quot; &quot;crypto/cipher&quot; &quot;crypto/aes&quot; &quot;fmt&quot;) //填充字符串（末尾）func PaddingText1(str []byte, blockSize int) []byte &#123; //需要填充的数据长度 paddingCount := blockSize - len(str)%blockSize //填充数据为：paddingCount ,填充的值为：paddingCount paddingStr := bytes.Repeat([]byte&#123;byte(paddingCount)&#125;, paddingCount) newPaddingStr := append(str, paddingStr...) //fmt.Println(newPaddingStr) return newPaddingStr&#125; //去掉字符（末尾）func UnPaddingText1(str []byte) []byte &#123; n := len(str) count := int(str[n-1]) newPaddingText := str[:n-count] return newPaddingText&#125;//---------------DES加密 解密--------------------func EncyptogAES(src, key []byte) []byte &#123; block,err:=aes.NewCipher(key) if err!= nil&#123; fmt.Println(nil) return nil &#125; src=PaddingText1(src,block.BlockSize()) blockMode:=cipher.NewCBCEncrypter(block,key) blockMode.CryptBlocks(src,src) return src &#125;func DecrptogAES(src,key[]byte) []byte &#123; block,err:=aes.NewCipher(key) if err!= nil&#123; fmt.Println(nil) return nil &#125; blockMode:=cipher.NewCBCDecrypter(block,key) blockMode.CryptBlocks(src,src) src=UnPaddingText1(src) return src&#125; func main() &#123; str:=&quot;山重水复疑无路，柳暗花明又一村！&quot; fmt.Println(&quot;编码的数据为：&quot;,str) key:=[]byte(&quot;12345678abcdefgh&quot;) src:=EncyptogAES([]byte(str),key) DecrptogAES(src,key) fmt.Println(&quot;解码之后的数据为：&quot;,string(src)) &#125;]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>加密算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Beego框架开发之常见错误01之session设置]]></title>
    <url>%2F2018%2F10%2F16%2FBeego%E6%A1%86%E6%9E%B6%E5%BC%80%E5%8F%91%E4%B9%8B%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF01%E4%B9%8Bsession%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Session 今天我们来讲解一下beego框架开发里面的第一种常见的错误，而且有时候这些错误隐藏的不好找，有些地方语法根本没有错误，所有找错误的时候如果没有注意到，那么 就会很不好找bug,特别对于新手来说，如果这些错误不注意那么很容易犯错。 先来看一下介绍 what is cookie?cookie是存储在客户端的，用于标识客户身份的！ what is sessionsession 是存储在服务端，也是用于客户身份标识，用于跟踪用户会话。 BeeGo session保存方式Beego内置了session模块，目前session模块支持的后端引擎包括memory，cookie，file，mysql，redis，couchbase，memcache、postgres，用户也可以根据相应的interface实现自己的引擎。 我们先来看一下设置session的语法1this.SetSession(&quot;userName&quot;,username) 在beego里设置session的只需要这一句话就行了， 通过这种方式就可以开启session 1234567891011func (this *MainController) Get() &#123; v := this.GetSession(&quot;asta&quot;) if v == nil &#123; this.SetSession(&quot;asta&quot;, int(1)) this.Data[&quot;num&quot;] = 0 &#125; else &#123; this.SetSession(&quot;asta&quot;, v.(int)+1) this.Data[&quot;num&quot;] = v.(int) &#125; this.TplName = &quot;index.tpl&quot;&#125; 着这里我们验证登录的时候看一下如下代码 123456789func (this *Index)ShowIndex()&#123; userName:=this.GetSession(&quot;userName&quot;) //获取session if userName==nil&#123; this.Redirect(&quot;/login&quot;,302) return &#125; this.Data[&quot;userName&quot;]=userName //给后台传数据 this.TplName=&quot;index.html&quot;&#125; 在这里需要注意一下，if userName==nil 这句话，有可能新手会判断为””空字符串，这里的小错不要犯，应该判断为空。 常犯错误有些新手可能经常犯无效的内存地址或空指针异常的错误，如图 报错是这样的，但是到文件中149行查看语句就是 1this.SetSession(&quot;userName&quot;,username) 这句语句，这句语句有什么错呢，语法上完全没有错，这时候很多新手可能就会郁闷找不到错误， 这是因为session没有初始化的原因，beego里面session是默认没有初始化的，在开发文档上有两种方式初始化session beego 中使用 session 相当方便，只要在 main 入口函数中设置如下：1beego.BConfig.WebConfig.Session.SessionOn = true 或者通过配置文件配置如下：1sessionon = true 这样就不会报错了。]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>beego框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go语言DES加密解密]]></title>
    <url>%2F2018%2F10%2F14%2FGo%E8%AF%AD%E8%A8%80DES%E5%8A%A0%E5%AF%86%E8%A7%A3%E5%AF%86%2F</url>
    <content type="text"><![CDATA[概念理解DES是以64比特的明文为一个单位来进行加密，并生成64比特的密文。由于它每次只能处理特定长度的一块数据，所以DES属于分组密码算法。cypto/des包提供了有关des加密的功能。 模式由于分组密码算法只能加密固定长度的分组，所以当加密的明文超过分组密码的长度时，就需要对分组密码算法进行迭代，而迭代的方法就称为分组密码的模式。模式主要有ECB(电子密码本)、CBC(密码分组链接模式)、CTR(计数器模式)、OFB(输出反馈模式)、CFB(密码反馈模式)五种。下面简单介绍下前两种： ECB(electronic code book)是最简单的方式，它将明文分组加密后的结果直接成为密文分组。 优缺点：模式操作简单；明文中的重复内容将在密文中表现出来，特别对于图像数据和明文变化较少的数据；适于短报文的加密传递。 CBC(cipher block chaining)的原理是加密算法的输入是当前的明文分组和前一密文分组的异或，第一个明文分组和一个初始向量进行异或，这样同一个明文分组重复出现时会产生不同的密文分组。 特点：同一个明文分组重复出现时产生不同的密文分组；加密函数的输入是当前的明文分组和前一个密文分组的异或；每个明文分组的加密函数的输入与明文分组之间不再有固定的关系；适合加密长消息。 填充方式在按8个字节对DES进行加密或解密时，如果最后一段字节不足8位，就需要对数据进行补位。即使加密或解密的数据刚好是8的倍数时，也会再补8位。举个栗子，如果末尾刚好出现1，这时你就无法判断这个1是原来数据，还是经过补位得到的1。因此，可以再补8位进行标识。填充方式主要有以下几种：pkcs7padding、pkcs5padding、zeropadding、iso10126、ansix923。 pkcs7padding和pkcs5padding的填充方式相同，填充字节的值都等于填充字节的个数。例如需要填充4个字节，则填充的值为”4 4 4 4”。 zeropadding填充字节的值都为0。 密码DES的密钥长度是64比特，但由于每隔7个比特会设置一个用于错误检测的比特，因此其实质密钥长度为56比特。 偏移量上面模式中，例如CBC，再加密第一个明文分组时，由于不存在“前一个密文分组”，因此需要事先准备一个长度为一个分组的比特序列来代替“前一个密文分组”，这个比特序列成为初始化向量，也称偏移量，通常缩写为IV。一般来说，每次加密时都会随机产生一个不同的比特序列来作为初始化向量。偏移量的长度必须和块的大小相同。 输出加密后的字节在显示时可以进行hex和base64编码，hex是十六进制编码，base64是一种基于64个可打印字符来标识二进制数据的方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788package main import ( "bytes" "crypto/des" "crypto/cipher" "fmt") //填充字符串（末尾）func PaddingText(str []byte, blockSize int) []byte &#123; //需要填充的数据长度 paddingCount := blockSize - len(str)%blockSize //填充数据为：paddingCount ,填充的值为：paddingCount paddingStr := bytes.Repeat([]byte&#123;byte(paddingCount)&#125;, paddingCount) newPaddingStr := append(str, paddingStr...) //fmt.Println(newPaddingStr) return newPaddingStr&#125; //去掉字符（末尾）func UnPaddingText(str []byte) []byte &#123; n := len(str) count := int(str[n-1]) newPaddingText := str[:n-count] return newPaddingText&#125;//---------------DES加密 解密--------------------func EncyptogDES(src, key []byte) []byte &#123; //1、创建并返回一个使用DES算法的cipher.Block接口 block, _ := des.NewCipher(key) //2、对数据进行填充 src1 := PaddingText(src, block.BlockSize()) //3.创建一个密码分组为链接模式，底层使用des加密的blockmode接口 iv := []byte("aaaabbbb") blockMode := cipher.NewCBCEncrypter(block, iv) //4加密连续的数据块 desc := make([]byte, len(src1)) blockMode.CryptBlocks(desc, src1) return desc&#125;func DecrptogDES(src,key[]byte) []byte &#123; //创建一个block的接口 block,_:=des.NewCipher(key) iv := []byte("aaaabbbb") //链接模式，创建blockMode接口 blockeMode:=cipher.NewCBCDecrypter(block,iv) //解密 blockeMode.CryptBlocks(src,src) //去掉填充 newText:=UnPaddingText(src) return newText&#125;//---------------DES加密 解密--------------------func Encyptog3DES(src, key []byte) []byte &#123; //des包下的三次加密接口 block,_:=des.NewTripleDESCipher(key) src=PaddingText(src,block.BlockSize()) blockMode:=cipher.NewCBCEncrypter(block,key[:block.BlockSize()]) blockMode.CryptBlocks(src,src) return src&#125;func Decrptog3DES(src,key[]byte) []byte &#123; block,_:=des.NewTripleDESCipher(key) blockMode:=cipher.NewCBCDecrypter(block,key[:block.BlockSize()]) blockMode.CryptBlocks(src,src) src=UnPaddingText(src) return src&#125;func main() &#123; fmt.Println("---------------DES加解密----------------------") str:=[]byte("乌慢。。。") fmt.Println("加密之前的数据为：",string(str)) key:=[]byte("12345678") src:=EncyptogDES(str,key) src=DecrptogDES(src,key) fmt.Println("解密之后的数据为：",string(src)) fmt.Println("---------------3DES加解密----------------------") str1:=[]byte("海内存知己天涯若比邻") fmt.Println("加密之前的数据为：",string(str1)) key1:=[]byte("12345678abcdefgh98765432") src1:=Encyptog3DES(str1,key1) src1=Decrptog3DES(src1,key1) fmt.Println("解密之后的数据为：",string(src1)) &#125; 之后还有讲解AES加密算法，以及非对称加密算法，椭圆曲线加密算法的详解等等]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>加密算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go语言常用加密算法]]></title>
    <url>%2F2018%2F10%2F14%2FGo%E8%AF%AD%E8%A8%80%E5%B8%B8%E7%94%A8%E5%8A%A0%E5%AF%86%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[常见的加密算法原文链接：https://studygolang.com/articles/10134 在项目开发过程中，当我们利用数据库存储一些关于用户的隐私信息，诸如密码、帐户密钥等数据时，需要加密后才向数据库写入。这时，我们需要一些高效地、简单易用的加密算法，当我们向数据库写数据时加密数据，然后把加密后的数据存入数据库；当需要读取数据时，从数据库把加密后的数据取出来，再通过算法解密。 常用的加密算法有Base64、MD5、AES和DES。 Base64Base64是一种任意二进制到文本字符串的编码方法，常用于在URL、Cookie、网页中传输少量二进制数据。 首先使用Base64编码需要一个含有64个字符的表，这个表由大小写字母、数字、+和/组成。采用Base64编码处理数据时，会把每三个字节共24位作为一个处理单元，再分为四组，每组6位，查表后获得相应的字符即编码后的字符串。编码后的字符串长32位，这样，经Base64编码后，原字符串增长1/3。如果要编码的数据不是3的倍数，最后会剩下一到两个字节，Base64编码中会采用\x00在处理单元后补全，编码后的字符串最后会加上一到两个 = 表示补了几个字节。 Base64表 1BASE64Table = &quot;IJjkKLMNO567PQX12RVW3YZaDEFGbcdefghiABCHlSTUmnopqrxyz04stuvw89+/&quot; 加密。函数里的第二行的代码可以把一个输入的字符串转换成一个字节数组。 12345func Encode(data string) string &#123; content := *(*[]byte)(unsafe.Pointer((*reflect.SliceHeader)(unsafe.Pointer(&amp;data)))) coder := base64.NewEncoding(BASE64Table) return coder.EncodeToString(content)&#125; 解密。函数返回处的代码可以把一个 字节数组转换成一个字符串。 12345func Decode(data string) string &#123; coder := base64.NewEncoding(BASE64Table) result, _ := coder.DecodeString(data) return *(*string)(unsafe.Pointer(&amp;result))&#125; 测试。 12345678910 func main()&#123; strTest := &quot;I love this beautiful world!&quot; strEncrypt := Encode(strTest) strDecrypt := Decode(strEncrypt) fmt.Println(&quot;Encrypted:&quot;,strEncrypt) fmt.Println(&quot;Decrypted:&quot;,strDecrypt) &#125;//Output://Encrypted: VVJmGsEBONRlFaPfDCYgcaRSEHYmONcpbCrAO2==//Decrypted: I love this beautiful world! MD5MD5的全称是Message-DigestAlgorithm 5，它可以把一个任意长度的字节数组转换成一个定长的整数，并且这种转换是不可逆的。对于任意长度的数据，转换后的MD5值长度是固定的，而且MD5的转换操作很容易，只要原数据有一点点改动，转换后结果就会有很大的差异。正是由于MD5算法的这些特性，它经常用于对于一段信息产生信息摘要，以防止其被篡改。其还广泛就于操作系统的登录过程中的安全验证，比如Unix操作系统的密码就是经过MD5加密后存储到文件系统中，当用户登录时输入密码后， 对用户输入的数据经过MD5加密后与原来存储的密文信息比对，如果相同说明密码正确，否则输入的密码就是错误的。 MD5以512位为一个计算单位对数据进行分组，每一分组又被划分为16个32位的小组，经过一系列处理后，输出4个32位的小组，最后组成一个128位的哈希值。对处理的数据进行512求余得到N和一个余数，如果余数不为448,填充1和若干个0直到448位为止，最后再加上一个64位用来保存数据的长度，这样经过预处理后，数据变成（N+1）x 512位。 加密。Encode 函数用来加密数据，Check函数传入一个未加密的字符串和与加密后的数据，进行对比，如果正确就返回true。 123456789func Check(content, encrypted string) bool &#123; return strings.EqualFold(Encode(content), encrypted)&#125;func Encode(data string) string &#123; h := md5.New() h.Write([]byte(data)) return hex.EncodeToString(h.Sum(nil))&#125; 测试。 1234567func main() &#123; strTest := &quot;I love this beautiful world!&quot; strEncrypted := &quot;98b4fc4538115c4980a8b859ff3d27e1&quot; fmt.Println(Check(strTest, strEncrypted))&#125;//Output://true DESDES是一种对称加密算法，又称为美国数据加密标准。DES加密时以64位分组对数据进行加密，加密和解密都使用的是同一个长度为64位的密钥，实际上只用到了其中的56位，密钥中的第8、16…64位用来作奇偶校验。DES有ECB（电子密码本）和CBC（加密块）等加密模式。 DES算法的安全性很高，目前除了穷举搜索破解外， 尚无更好的的办法来破解。其密钥长度越长，破解难度就越大。 填充和去填充函数。 123456789101112func ZeroPadding(ciphertext []byte, blockSize int) []byte &#123; padding := blockSize - len(ciphertext)%blockSize padtext := bytes.Repeat([]byte&#123;0&#125;, padding) return append(ciphertext, padtext...)&#125; func ZeroUnPadding(origData []byte) []byte &#123; return bytes.TrimFunc(origData, func(r rune) bool &#123; return r == rune(0) &#125;)&#125; 加密。 1234567891011121314151617181920func Encrypt(text string, key []byte) (string, error) &#123; src := []byte(text) block, err := des.NewCipher(key) if err != nil &#123; return &quot;&quot;, err &#125; bs := block.BlockSize() src = ZeroPadding(src, bs) if len(src)%bs != 0 &#123; return &quot;&quot;, errors.New(&quot;Need a multiple of the blocksize&quot;) &#125; out := make([]byte, len(src)) dst := out for len(src) &gt; 0 &#123; block.Encrypt(dst, src[:bs]) src = src[bs:] dst = dst[bs:] &#125; return hex.EncodeToString(out), nil&#125; 解密。 123456789101112131415161718192021222324func Decrypt(decrypted string , key []byte) (string, error) &#123; src, err := hex.DecodeString(decrypted) if err != nil &#123; return &quot;&quot;, err &#125; block, err := des.NewCipher(key) if err != nil &#123; return &quot;&quot;, err &#125; out := make([]byte, len(src)) dst := out bs := block.BlockSize() if len(src)%bs != 0 &#123; return &quot;&quot;, errors.New(&quot;crypto/cipher: input not full blocks&quot;) &#125; for len(src) &gt; 0 &#123; block.Decrypt(dst, src[:bs]) src = src[bs:] dst = dst[bs:] &#125; out = ZeroUnPadding(out) return string(out), nil&#125; 测试。在这里，DES中使用的密钥key只能为8位。 1234567891011121314151617func main() &#123; key := []byte(&quot;2fa6c1e9&quot;) str :=&quot;I love this beautiful world!&quot; strEncrypted, err := Encrypt(str, key) if err != nil &#123; log.Fatal(err) &#125; fmt.Println(&quot;Encrypted:&quot;, strEncrypted) strDecrypted, err := Decrypt(strEncrypted, key) if err != nil &#123; log.Fatal(err) &#125; fmt.Println(&quot;Decrypted:&quot;, strDecrypted)&#125;//Output://Encrypted: 5d2333b9fbbe5892379e6bcc25ffd1f3a51b6ffe4dc7af62beb28e1270d5daa1//Decrypted: I love this beautiful world! AESAES，即高级加密标准（Advanced Encryption Standard），是一个对称分组密码算法，旨在取代DES成为广泛使用的标准。AES中常见的有三种解决方案，分别为AES-128、AES-192和AES-256。 AES加密过程涉及到4种操作：字节替代（SubBytes）、行移位（ShiftRows）、列混淆（MixColumns）和轮密钥加（AddRoundKey）。解密过程分别为对应的逆操作。由于每一步操作都是可逆的，按照相反的顺序进行解密即可恢复明文。加解密中每轮的密钥分别由初始密钥扩展得到。算法中16字节的明文、密文和轮密钥都以一个4x4的矩阵表示。 AES 有五种加密模式：电码本模式（Electronic Codebook Book (ECB)）、密码分组链接模式（Cipher Block Chaining (CBC)）、计算器模式（Counter (CTR)）、密码反馈模式（Cipher FeedBack (CFB)）和输出反馈模式（Output FeedBack (OFB)）。下面以CFB为例。 加密。iv即初始向量，这里取密钥的前16位作为初始向量。 1234567891011func Encrypt(text string, key []byte) (string, error) &#123; var iv = key[:aes.BlockSize] encrypted := make([]byte, len(text)) block, err := aes.NewCipher(key) if err != nil &#123; return &quot;&quot;, err &#125; encrypter := cipher.NewCFBEncrypter(block, iv) encrypter.XORKeyStream(encrypted, []byte(text)) return hex.EncodeToString(encrypted), nil&#125; 解密。 12345678910111213141516171819202122func Decrypt(encrypted string, key []byte) (string, error) &#123; var err error defer func() &#123; if e := recover(); e != nil &#123; err = e.(error) &#125; &#125;() src, err := hex.DecodeString(encrypted) if err != nil &#123; return &quot;&quot;, err &#125; var iv = key[:aes.BlockSize] decrypted := make([]byte, len(src)) var block cipher.Block block, err = aes.NewCipher([]byte(key)) if err != nil &#123; return &quot;&quot;, err &#125; decrypter := cipher.NewCFBDecrypter(block, iv) decrypter.XORKeyStream(decrypted, src) return string(decrypted), nil&#125; 测试。密钥key只能为16位、24位或32位，分别对应AES-128, AES-192和 AES-256。 123456789101112131415161718192021func main()&#123; str := &quot;I love this beautiful world!&quot; key := []byte&#123;0xBA, 0x37, 0x2F, 0x02, 0xC3, 0x92, 0x1F, 0x7D, 0x7A, 0x3D, 0x5F, 0x06, 0x41, 0x9B, 0x3F, 0x2D, 0xBA, 0x37, 0x2F, 0x02, 0xC3, 0x92, 0x1F, 0x7D, 0x7A, 0x3D, 0x5F, 0x06, 0x41, 0x9B, 0x3F, 0x2D, &#125; strEncrypted,err := Encrypt(str, key) if err != nil &#123; log.Error(&quot;Encrypted err:&quot;,err) &#125; fmt.Println(&quot;Encrypted:&quot;,strEncrypted) strDecrypted,err := Decrypt(strEncrypted, key) if err != nil &#123; log.Error(&quot;Decrypted err:&quot;,err) &#125; fmt.Println(&quot;Decrypted:&quot;,strDecrypted)&#125;//Output://Encrypted: f866bfe2a36d5a43186a790b41dc2396234dd51241f8f2d4a08fa5dc//Decrypted: I love this beautiful world! 参考 Golang中的DES加密ECB模式 AES五种加密模式（CBC、ECB、CTR、OCF、CFB)]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>加密算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[beego框架图文简介讲解05]]></title>
    <url>%2F2018%2F10%2F12%2Fbeego%E6%A1%86%E6%9E%B6%E5%9B%BE%E6%96%87%E7%AE%80%E4%BB%8B%E8%AE%B2%E8%A7%A305%2F</url>
    <content type="text"><![CDATA[3.小Demo讲解我们今天以注册和登陆作为我们今天的demo讲解，把前面讲到的内容都串起来。先注册，然后登陆。 在开始具体的业务之前我们要做数据库设计，在正式开发中这一步是非常重要，也比较复杂的，但是今天我们只实现登陆和注册，就简单有个用户名和密码即可，model.go内容如下: 123456789101112131415161718192021222324package modelsimport ( "github.com/astaxie/beego/orm" _ "github.com/go-sql-driver/mysql")type User struct &#123; Id int Name string Passwd string&#125;func init()&#123; //1.连接数据库 orm.RegisterDataBase("default","mysql","root:123456@tcp(127.0.0.1:3306)/test?charset=utf8") //2.注册表 orm.RegisterModel(new(User)) //3.生成表 //1.数据库别名 //2.是否强制更新 //3.创建表过程是否可见 orm.RunSyncdb("default",false,true)&#125; 3.1注册确定注册请求路径,修改路由文件 我们这里以/register作为注册的请求路径。所以这里我们需要修改router.go文件的内容。 在router.go文件的init()函数中加下面这行代码: 1beego.Router("/register", &amp;controllers.UserController&#123;&#125;,"get:ShowRegister") 根据上面路由的指定，我们需要添加注册控制器 在controllers文件夹下创建一个user.go，然后在这个文件里面定义一个结构体UserController当控制器。 123type UserController struct&#123; beego.Controller&#125; 注意这里面添加的beego.Controller,是为了继承自beego自带的控制器。 添加显示注册页面函数 添加函数的时候需要注意，这个函数必须是UserController的函数才可以，不然在路由里面调用不到。那如何把函数设置成UserController的成员函数呢？是在函数名前面加上括号，然后放上UserController的指针。这里我们先指定注册的视图。代码如下： 123func (this*UserController)ShowRegister()&#123; this.TplName = "register.html"&#125; 注意：这里如果函数名首字母小写，路由同意找不到函数，所以函数名首字母必须大写 添加视图页面 在views文件夹下面创建一个名字为register.html的文件。然后实现成类似界面： 我们做后台的不关注样式，明天直接拿现成的样式来用即可，我们重在实现功能。 form标签里面需要添加两个属性，一个是action,一个是method，action其实就是请求路径，这里处理的还是注册业务，所以我们还用register请求，action = “/register”,因为是上传数据，所以我们把method设置为post,即method=&quot;post&quot;,代码如下: 12345678910111213141516171819&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;注册页面&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div style="position:absolute;left:50%; top:50%;"&gt; &lt;form action="/register" method="post"&gt; 用户名:&lt;input type="text" name="userName"&gt; &lt;p&gt; &lt;/p&gt; 密码：&lt;input type="password" name="passwd"&gt; &lt;p&gt; &lt;/p&gt; &lt;input type="submit" value="注册"&gt; &lt;/form&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 让项目运行起来，然后我们在浏览器里面输入相应的地址就能看见我们的注册页面了。 显示完注册页面之后，接着我们来处理注册的post请求。因为action=”/register”，method=”post”,所以我们可以去router.go界面给post请求指定相应的方法。修改如下： 1beego.Router("/register", &amp;controllers.UserController&#123;&#125;,"get:ShowRegister;post:HandleRegister") 指定方法名之后我们就需要去控制器中实现他。 注册业务处理 首先在user.go中添加这个函数： 12func (this*UserController)HandleRegister()&#123;&#125; 接着开始处理注册业务 首先要获取数据。这里给大家介绍一类方法，这类方法将会在我们项目一中高频率的出现，因为他的作用太强大了。 this.GetString()：获取字符串类型值 this.GetInt()：获取整型值 this.GetFloat：获取浮点型值 … this.GetFile()：获取上传的文件 作用：接收前端传递过来的数据，不管是get请求还是post请求，都能接收。 参数: 是传递数据的key值，一般情况下是form表单中标签的name属性值 返回值：根据返回类型不同，返回值也不一样，最常用的GetString()只有一个返回值，如果没有取到值就返回空字符串，其他几个函数会返回一个错误类型。获取的值一般是标签里面的value属性值。至于比较特殊的，我们用到的时候给大家做介绍。 知道了获取数据函数，我们就可以获取前端传递过来的数据啦。 获取注册的用户名和密码 12userName := this.GetString("userName")passwd := this.GetString("passwd") 对数据进行校验 一般情况下，我们做服务器开发，从前端拿过来数据都要进行一系列的校验，然后才会用数据对数据库进行操作。不做校验的服务器很容易被黑掉。这里我们只做简单的判空校验。 12345if userName == "" || passwd == ""&#123; beego.Info("数据数据不完整，请重新输入！") this.TplName = "register.html" return&#125; 思考：如何把那句错误提示传递给视图？ 把数据插入数据库 如果数据校验没有问题，那我们就需要把数据插入到数据库中。数据库插入操作前面刚讲过，这里就不一步一步的分开介绍了，代码如下： 12345678910111213//获取orm对象 o := orm.NewOrm() //获取要插入的数据对象 var user models.User //给对象赋值 user.Name = userName user.Passwd = passwd //把数据插入到数据库 if _,err := o.Insert(&amp;user);err != nil&#123; beego.Info("注册失败，请更换用户名再次注册!") this.TplName = "register.html" return &#125; 因为我们现在还没有其他界面，如果跳转成功就返回一句话注册成功,等我们实现了登陆界面之后再实现注册之后跳转登陆界面的操作。 给浏览器返回一句化的代码如下： 1this.Ctx.WriteString("注册成功!") 完整后台代码如下 12345678910111213141516171819202122232425262728293031323334//显示注册页面func(this*UserController)ShowRegister()&#123; this.TplName = "register.html"&#125;//处理注册业务func(this*UserController)HandleRegister()&#123; //获取前端传递的数据 userName := this.GetString("userName") passwd := this.GetString("passwd") //对数据进行校验 if userName == "" || passwd == ""&#123; beego.Info("数据数据不完整，请重新输入！") this.TplName = "register.html" return &#125; //把数据插入到数据库 //获取orm对象 o := orm.NewOrm() //获取要插入的数据对象 var user models.User //给对象赋值 user.Name = userName user.Passwd = passwd //把数据插入到数据库 if _,err := o.Insert(&amp;user);err != nil&#123; beego.Info("注册失败，请更换用户名再次注册!") this.TplName = "register.html" return &#125; //返回提示信息 this.Ctx.WriteString("注册成功!")&#125; 3.2登陆登陆和注册业务流程差不多，差别也就体现在一个是对数据的查询一个是数据的插入，讲义里面就不做详细分析，直接贴代码。 路由文件修改 添加下面一行代码： 1beego.Router("/login", &amp;controllers.UserController&#123;&#125;,"get:ShowLogin;post:HandleLogin") 后台代码修改 在控制器中添加展示登录页的函数ShowLogin和处理登陆数据的函数HandleLogin。完整代码如下: 12345678910111213141516171819202122232425262728293031323334353637//显示登陆界面func(this*UserController)ShowLogin()&#123; this.TplName = "login.html"&#125;//处理登陆业务func(this*UserController)HandleLogin()&#123; //获取前端传递的数据 userName := this.GetString("userName") passwd := this.GetString("passwd") //对数据进行校验 if userName == "" || passwd == ""&#123; beego.Info("数据数据不完整，请重新输入！") this.TplName = "login.html" return &#125; //查询数据库，判断用户名和密码是否正确 //获取orm对象 o := orm.NewOrm() //获取要插入的数据对象 var user models.User //给对象赋值 user.Name = userName //根据用户名查询 if err := o.Read(&amp;user,"Name");err != nil&#123; beego.Info("用户名错误，请重新输入！") this.TplName = "login.html" return &#125; if user.Passwd != passwd&#123; beego.Info("密码错误，请重新输入！") this.TplName = "login.html" return &#125; //返回提示信息 this.Ctx.WriteString("登陆成功!")&#125; 添加视图文件 登陆界面和注册界面很相似，拷贝过来简单修改一下即可，代码如下： 12345678910111213141516171819&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;登陆页面&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div style="position:absolute;left:50%; top:50%;"&gt; &lt;form action="/login" method="post"&gt; 用户名:&lt;input type="text" name="userName"&gt; &lt;p&gt; &lt;/p&gt; 密码：&lt;input type="password" name="passwd"&gt; &lt;p&gt; &lt;/p&gt; &lt;input type="submit" value="登陆"&gt; &lt;/form&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 这样我们的登陆和注册就算完成了，但是有一个问题，我们的登陆注册还是各干各的，没有关联起来，我们前面等登陆页面实现完之后，注册成功就跳转到登陆页面。现在我们来实现一下跳转。 3.3页面之间的跳转beego里面页面跳转的方式有两种，一种是重定向，一种是渲染，都能够在浏览器上显示新的页面 3.3.1重定向重定向用到的方法是 this.Redirect() 函数，有两个参数，第一个参数是请求路径，第二个参数是http状态码。 请求路径就不说了，就是和超链接一样的路径。 我们重点介绍一下状态码： 状态码一共分为五类： 1xx : 服务端已经接收到了客户端请求，客户端应当继续发送请求 。常见的请求：100 2xx :请求已成功 （已实现）常见的请求：200 3xx :请求的资源转换路径了，请求被跳转。常见的请求：300，302 4xx :客户端请求失败。常见的请求：404 5xx :服务器端错误。常见的请求：500 状态码详解:http://tool.oschina.net/commons?type=5 重定向的工作流程是： 1234567891：当服务端向客户端响应 redirect后，并没有提供任何view数据进行渲染，仅仅是告诉浏览器响应为 redirect，以及重定向的目标地址2：浏览器收到服务端 redirect 过来的响应，会再次发起一个 http 请求3：由于是浏览器再次发起了一个新的 http 请求，所以浏览器地址栏中的 url 会发生变化4：浏览中最终得到的页面是最后这个 重定向的url 请求后的页面5：所以redirect(&quot;/register&quot;,302) 相当于你在浏览器中手动输入 localhost/register 3.3.2渲染渲染就是控制期把一些数据传递给视图，然后视图用这些输出组织成html界面。所以不会再给浏览器发请求，是服务器自己的行为，所以浏览器的地址栏不会改变，但是显示的页面可能会发生变化。用的函数是:this.TplName = &quot;login.html&quot; 3.3.3两者之间的区别 区别 重定向 渲染 响应方式 告诉浏览器相应是redirect，然后返回一个新的地址给浏览器，让浏览器重新发起请求 直接给浏览器返回视图 地址栏显示 浏览器地址栏显示的是新的地址 浏览器地址栏显示的还是原来的地址 作用 不能给视图传递数据，但是能够获取到加载页面时的数据 能够给视图传递数据，但如果要获取加载页面时的数据，需要再次写相关代码 使用场景 页面跳转的时候 页面加载或者是登陆注册失败的时候]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>beego框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[beego框架图文简介讲解04]]></title>
    <url>%2F2018%2F10%2F12%2Fbeego%E6%A1%86%E6%9E%B6%E5%9B%BE%E6%96%87%E7%AE%80%E4%BB%8B%E8%AE%B2%E8%A7%A304%2F</url>
    <content type="text"><![CDATA[2.10 ORM框架Beego中内嵌了ORM框架，用来操作数据库。那么ORM框架是什么呢？ORM框架是Object-RelationShip-Mapping的缩写，中文叫对象关系映射，他们之间的关系，我们用图来表示： 2.10.1 ORM初始化 首先要导包 1import "github.com/astaxie/beego/orm" 然后要定义一个结构体 12345type User struct&#123; Id int Name string PassWord string&#125; 思考:如果表名和字段名为小写会发生什么结果？ 注意观察数据库表中的字段和结构体中的字段是否一样？ 然后向数据库中注册表，这一步又分为三步： 连接数据库 用RegisterDataBase()函数，第一个参数为数据库别名，也可以理解为数据库的key值，项目中必须有且只能有一个别名为default的连接，第二个参数是数据库驱动，这里我们用的是MySQL数据库，所以以MySQL驱动为例，第三个参数是连接字符串，和传统操作数据库连接字符串一样，格式为：用户名:密码@tcp(ip:port)/数据库名称?编码方式，代码如下： 1orm.RegisterDataBase("default","mysql","root:123456@tcp(127.0.0.1:3306)/class1?charset=utf8") 注意：ORM只能操作表，不能操作数据库，所以我们连接的数据库要提前在MySQL终端创建好。 注册数据库表 用orm.RegisterModel()函数，参数是结构体对象，如果有多个表，可以用 ,隔开，多new几个对象： 1orm.RegisterModel(new(User)) 生成表 用orm.RunSyncdb()函数，这个函数有三个参数， 第一个参数是数据库的别名和连接数据库的第一个参数相对应。 第二个参数是是否强制更新，一般我们写的都是false，如果写true的话，每次项目编译一次数据库就会被清空一次，fasle的话会在数据库发生重大改变（比如添加字段）的时候更新数据库。 第三个参数是用来说，生成表过程是否可见，如果我们写成课件，那么生成表的时候执行的SQL语句就会在终端看到。反之看不见。代码如下: 1orm.RunSyncdb("default",false,true) 完整代码如下: 12345678910111213141516171819import "github.com/astaxie/beego/orm"type User struct &#123; Id int Name string Passwd string&#125;func init()&#123; //1.连接数据库 orm.RegisterDataBase("default","mysql","root:123456@tcp(127.0.0.1:3306)/test?charset=utf8") //2.注册表 orm.RegisterModel(new(User)) //3.生成表 //1.数据库别名 //2.是否强制更新 //3.创建表过程是否可见 orm.RunSyncdb("default",false,true)&#125; 因为这里我们把ORM初始化的代码放到了 models包的init()函数里面，所以如果我们想让他执行的话就需要在main.go里面加入这么一句代码: 1import _ "classOne/models" 2.10.2 简单的ORM增删改查操作在执行ORM的操作之前需要先把ORM包导入，但是GoLand会自动帮我们导包，也可以手动导包 1import "github.com/astaxie/beego/orm" 插入 先获取一个ORM对象,用orm.NewOrm()即可获得 1o := orm.NewOrm() 定义一个要插入数据库的结构体对象 1var user User 给定义的对象赋值 12user.Name = "itcast"user.Passwd = "heima" 这里不用给Id赋值，因为建表的时候我们没有指定主键，ORM默认会以变量名为Id，类型为int的字段当主键，至于如何指定主键，我们明天详细介绍。 执行插入操作，o.Insert()插入，参数是结构体对象，返回值是插入的id和错误信息。 1234id, err := o.Insert(&amp;user)if err == nil &#123; fmt.Println(id)&#125; 查询 也是要先获得一个ORM对象 1o := orm.NewOrm() 定义一个要获取数据的结构体对象 1var user User 给结构体对象赋值，相当于给查询条件赋值 1user.Name = "itcast" 查询,用o.Read()，第一个参数是对象地址，第二个参数是指定查询字段，返回值只有错误信息。 12345err := o.Read(&amp;user,"Name")if err != nil&#123; beego.Info("查询数据错误",err) return &#125; 如果查询字段是查询对象的主键的话，可以不用指定查询字段 更新 一样的套路，先获得ORM对象 1o := orm.NewOrm() 定义一个要更新的结构体对象 1var user User 给结构体对象赋值，相当于给查询条件赋值 1user.Name = "itcast" 查询要更新的对象是否存在 12345err := o.Read(&amp;user)if err != nil&#123; beego.Info("查询数据错误",err) return&#125; 如果查找到了要更新的对象,就给这个对象赋新值 1user.Passwd = "itheima" 执行更新操作,用o.Update()函数，参数是结构体对象指针，返回值是更新的条目数和错误信息 12345count,err=o.Update(&amp;user)if err != nil&#123; beego.Info("更新数据错误",err) return&#125; 删除 同样的，获取ORM对象，获取要删除的对象 12o := orm.NewOrm()var user User 给删除对象赋值，删除的对象主键必须有值，如果主键没值，就查询一下。我们这里直接给主键赋值。 1user.Id = 1 执行删除操作，用的方法是o.Delete()，参数是删除的结构体对象,返回值是删除的条目数和错误信息 1234num, err := o.Delete(&amp;User&#123;Id: 1&#125;)if err == nil &#123; fmt.Println(num)&#125;]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>beego框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-ldd命令]]></title>
    <url>%2F2018%2F10%2F10%2FLinux%E5%91%BD%E4%BB%A4-ldd%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[ldd用于打印程序或者库文件所依赖的共享库列表。 语法ldd(选项)(参数) 选项123456--version：打印指令版本号；-v：详细信息模式，打印所有相关信息；-u：打印未使用的直接依赖；-d：执行重定位和报告任何丢失的对象；-r：执行数据对象和函数的重定位，并且报告任何丢失的对象和函数；--help：显示帮助信息。 参数文件：指定可执行程序或者文库。 其他1) ldd不是一个可执行程序，而只是一个shell脚本 ldd能够显示可执行模块的dependency(所属)(所属)，其原理是通过设置一系列的环境变量，如下：LD_TRACE_LOADED_OBJECTS、LD_WARN、LD_BIND_NOW、LD_LIBRARY_VERSION、LD_VERBOSE等。当LD_TRACE_LOADED_OBJECTS环境变量不为空时，任何可执行程序在运行时，它都会只显示模块的dependency(所属)，而程序并不真正执行。要不你可以在shell终端测试一下，如下： export LD_TRACE_LOADED_OBJECTS=1 再执行任何的程序，如ls等，看看程序的运行结果。 2) ldd显示可执行模块的dependency(所属)的工作原理，其实质是通过ld-linux.so（elf动态库的装载器）来实现的。我们知道，ld-linux.so模块会先于executable模块程序工作，并获得控制权，因此当上述的那些环境变量被设置时，ld-linux.so选择了显示可执行模块的dependency(所属)。 实际上可以直接执行ld-linux.so模块，如：/lib/ld-linux.so.2 –list program（这相当于ldd program）。 转载链接： https://www.cnblogs.com/Spiro-K/p/6378576.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-ifconfig命令]]></title>
    <url>%2F2018%2F10%2F10%2FLinux%E5%91%BD%E4%BB%A4-ifconfig%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[许多windows非常熟悉ipconfig命令行工具，它被用来获取网络接口配置信息并对此进行修改。Linux系统拥有一个类似的工具，也就是ifconfig(interfaces config)。通常需要以root身份登录或使用sudo以便在Linux机器上使用ifconfig工具。依赖于ifconfig命令中使用一些选项属性，ifconfig工具不仅可以被用来简单地获取网络接口配置信息，还可以修改这些配置。 语法ifconfig(参数) 参数123456789101112131415161718add&lt;地址&gt;：设置网络设备IPv6的ip地址；del&lt;地址&gt;：删除网络设备IPv6的IP地址；down：关闭指定的网络设备；&lt;hw&lt;网络设备类型&gt;&lt;硬件地址&gt;：设置网络设备的类型与硬件地址；io_addr&lt;I/O地址&gt;：设置网络设备的I/O地址；irq&lt;IRQ地址&gt;：设置网络设备的IRQ；media&lt;网络媒介类型&gt;：设置网络设备的媒介类型；mem_start&lt;内存地址&gt;：设置网络设备在主内存所占用的起始地址；metric&lt;数目&gt;：指定在计算数据包的转送次数时，所要加上的数目；mtu&lt;字节&gt;：设置网络设备的MTU；netmask&lt;子网掩码&gt;：设置网络设备的子网掩码；tunnel&lt;地址&gt;：建立IPv4与IPv6之间的隧道通信地址；up：启动指定的网络设备；-broadcast&lt;地址&gt;：将要送往指定地址的数据包当成广播数据包来处理；-pointopoint&lt;地址&gt;：与指定地址的网络设备建立直接连线，此模式具有保密功能；-promisc：关闭或启动指定网络设备的promiscuous模式；IP地址：指定网络设备的IP地址；网络设备：指定网络设备的名称。 常用实例1）显示网络设备信息（激活状态的） 12345678910111213141516# ifconfigeth0 Link encap:Ethernet HWaddr 00:50:56:BF:26:20 inet addr:192.168.120.204 Bcast:192.168.120.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:8700857 errors:0 dropped:0 overruns:0 frame:0 TX packets:31533 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:596390239 (568.7 MiB) TX bytes:2886956 (2.7 MiB)lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:16436 Metric:1 RX packets:68 errors:0 dropped:0 overruns:0 frame:0 TX packets:68 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:2856 (2.7 KiB) TX bytes:2856 (2.7 KiB) 说明： eth0 表示第一块网卡， 其中 HWaddr 表示网卡的物理地址，可以看到目前这个网卡的物理地址(MAC地址）是 00:50:56:BF:26:20 inet addr 用来表示网卡的IP地址，此网卡的 IP地址是 192.168.120.204，广播地址， Bcast:192.168.120.255，掩码地址Mask:255.255.255.0 lo 是表示主机的回坏地址，这个一般是用来测试一个网络程序，但又不想让局域网或外网的用户能够查看，只能在此台主机上运行和查看所用的网络接口。比如把 HTTPD服务器的指定到回坏地址，在浏览器输入 127.0.0.1 就能看到你所架WEB网站了。但只是您能看得到，局域网的其它主机或用户无从知道。 第一行：连接类型：Ethernet（以太网）HWaddr（硬件mac地址） 第二行：网卡的IP地址、子网、掩码 第三行：UP（代表网卡开启状态）RUNNING（代表网卡的网线被接上）MULTICAST（支持组播）MTU:1500（最大传输单元）：1500字节 第四、五行：接收、发送数据包情况统计 第七行：接收、发送数据字节数统计信息。 2）启动关闭指定网卡 12# ifconfig eth0 up# ifconfig eth0 down 说明： ifconfig eth0 up 为启动网卡eth0 ；ifconfig eth0 down 为关闭网卡eth0。ssh登陆linux服务器操作要小心，关闭了就不能开启了，除非你有多网卡。 3） 为网卡配置和删除IPv6地址： 12# ifconfig eth0 add 33ffe:3240:800:1005::2/64 #为网卡eth0配置IPv6地址# ifconfig eth0 del 33ffe:3240:800:1005::2/64 #为网卡eth0删除IPv6地址 4） 用ifconfig修改MAC地址： 1ifconfig eth0 hw ether 00:AA:BB:CC:dd:EE 5）配置IP地址 123# ifconfig eth0 192.168.120.56 # ifconfig eth0 192.168.120.56 netmask 255.255.255.0 # ifconfig eth0 192.168.120.56 netmask 255.255.255.0 broadcast 192.168.120.255 说明： ifconfig eth0 192.168.120.56 给eth0网卡配置IP地：192.168.120.56 ifconfig eth0 192.168.120.56 netmask 255.255.255.0 给eth0网卡配置IP地址：192.168.120.56 ，并加上子掩码：255.255.255.0 ifconfig eth0 192.168.120.56 netmask 255.255.255.0 broadcast 192.168.120.255 /给eth0网卡配置IP地址：192.168.120.56，加上子掩码：255.255.255.0，加上个广播地址： 192.168.120.255 6）启用和关闭ARP协议 12# ifconfig eth0 arp # ifconfig eth0 -arp 7）设置最大传输单元 1# ifconfig eth0 mtu 1500 备注：用ifconfig命令配置的网卡信息，在网卡重启后机器重启后，配置就不存在。要想将上述的配置信息永远的存的电脑里，那就要修改网卡的配置文件了。 参考链接： http://www.cnblogs.com/peida/archive/2013/02/27/2934525.html https://man.linuxde.net/]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-ping命令]]></title>
    <url>%2F2018%2F10%2F10%2FLinux%E5%91%BD%E4%BB%A4-ping%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Linux系统的ping命令是常用的网络命令，它通常用来测试与目标主机的连通性，我们经常会说“ping一下某机器，看是不是开着”、不能打开网页时会说“你先ping网关地址192.168.1.1试试”。它通过发送ICMP ECHO_REQUEST数据包到网络主机（send ICMP ECHO_REQUEST to network hosts），并显示响应情况，这样我们就可以根据它输出的信息来确定目标主机是否可访问（但这不是绝对的）。有些服务器为了防止通过ping探测到，通过防火墙设置了禁止ping或者在内核参数中禁止ping，这样就不能通过ping确定该主机是否还处于开启状态。 linux下的ping和windows下的ping稍有区别,linux下ping不会自动终止,需要按ctrl+c终止或者用参数-c指定要求完成的回应次数。 语法ping(选项)(参数) 选项1234567891011121314-d：使用Socket的SO_DEBUG功能；-c&lt;完成次数&gt;：设置完成要求回应的次数；-f：极限检测；-i&lt;间隔秒数&gt;：指定收发信息的间隔时间；-I&lt;网络界面&gt;：使用指定的网络界面送出数据包；-l&lt;前置载入&gt;：设置在送出要求信息之前，先行发出的数据包；-n：只输出数值；-p&lt;范本样式&gt;：设置填满数据包的范本样式；-q：不显示指令执行过程，开头和结尾的相关信息除外；-r：忽略普通的Routing Table，直接将数据包送到远端主机上；-R：记录路由过程；-s&lt;数据包大小&gt;：设置数据包的大小；-t&lt;存活数值&gt;：设置存活数值TTL的大小；-v：详细显示指令的执行过程。 参数目的主机：指定发送ICMP报文的目的主机。 功能ping命令用于：确定网络和各外部主机的状态；跟踪和隔离硬件和软件问题；测试、评估和管理网络。如果主机正在运行并连在网上，它就对回送信号进行响应。每个回送信号请求包含一个网际协议（IP）和 ICMP 头，后面紧跟一个 tim 结构，以及来填写这个信息包的足够的字节。缺省情况是连续发送回送信号请求直到接收到中断信号（Ctrl-C）。 ping 命令每秒发送一个数据报并且为每个接收到的响应打印一行输出。ping 命令计算信号往返时间和(信息)包丢失情况的统计信息，并且在完成之后显示一个简要总结。ping 命令在程序超时或当接收到 SIGINT 信号时结束。Host 参数或者是一个有效的主机名或者是因特网地址。 常用实例1）ping的通的情况 1234567# ping 192.168.120.205PING 192.168.120.205 (192.168.120.205) 56(84) bytes of data.64 bytes from 192.168.120.205: icmp_seq=1 ttl=64 time=0.720 ms64 bytes from 192.168.120.205: icmp_seq=2 ttl=64 time=0.181 ms--- 192.168.120.205 ping statistics ---5 packets transmitted, 5 received, 0% packet loss, time 4000msrtt min/avg/max/mdev = 0.181/0.293/0.720/0.214 ms 2）ping不同的情况 1234567# ping 192.168.120.202PING 192.168.120.202 (192.168.120.202) 56(84) bytes of data.From 192.168.120.204 icmp_seq=1 Destination Host UnreachableFrom 192.168.120.204 icmp_seq=2 Destination Host Unreachable-- 192.168.120.202 ping statistics ---8 packets transmitted, 0 received, +6 errors, 100% packet loss, time 7005ms, pipe 4 3）ping指定次数 12345678910111213141516# ping -c 10 192.168.120.206PING 192.168.120.206 (192.168.120.206) 56(84) bytes of data.64 bytes from 192.168.120.206: icmp_seq=1 ttl=64 time=1.25 ms64 bytes from 192.168.120.206: icmp_seq=2 ttl=64 time=0.260 ms64 bytes from 192.168.120.206: icmp_seq=3 ttl=64 time=0.242 ms64 bytes from 192.168.120.206: icmp_seq=4 ttl=64 time=0.271 ms64 bytes from 192.168.120.206: icmp_seq=5 ttl=64 time=0.274 ms64 bytes from 192.168.120.206: icmp_seq=6 ttl=64 time=0.295 ms64 bytes from 192.168.120.206: icmp_seq=7 ttl=64 time=0.269 ms64 bytes from 192.168.120.206: icmp_seq=8 ttl=64 time=0.270 ms64 bytes from 192.168.120.206: icmp_seq=9 ttl=64 time=0.253 ms64 bytes from 192.168.120.206: icmp_seq=10 ttl=64 time=0.289 ms--- 192.168.120.206 ping statistics ---10 packets transmitted, 10 received, 0% packet loss, time 9000msrtt min/avg/max/mdev = 0.242/0.367/1.251/0.295 ms 4）时间间隔和次数限制的ping 12345678910111213141516# ping -c 10 -i 0.5 192.168.120.206PING 192.168.120.206 (192.168.120.206) 56(84) bytes of data.64 bytes from 192.168.120.206: icmp_seq=1 ttl=64 time=1.24 ms64 bytes from 192.168.120.206: icmp_seq=2 ttl=64 time=0.235 ms64 bytes from 192.168.120.206: icmp_seq=3 ttl=64 time=0.244 ms64 bytes from 192.168.120.206: icmp_seq=4 ttl=64 time=0.300 ms64 bytes from 192.168.120.206: icmp_seq=5 ttl=64 time=0.255 ms64 bytes from 192.168.120.206: icmp_seq=6 ttl=64 time=0.264 ms64 bytes from 192.168.120.206: icmp_seq=7 ttl=64 time=0.263 ms64 bytes from 192.168.120.206: icmp_seq=8 ttl=64 time=0.331 ms64 bytes from 192.168.120.206: icmp_seq=9 ttl=64 time=0.247 ms64 bytes from 192.168.120.206: icmp_seq=10 ttl=64 time=0.244 ms--- 192.168.120.206 ping statistics ---10 packets transmitted, 10 received, 0% packet loss, time 4499msrtt min/avg/max/mdev = 0.235/0.362/1.241/0.294 ms 5）多参数使用 123456789101112131415161718192021# ping -i 3 -s 1024 -t 255 192.168.120.206PING 192.168.120.206 (192.168.120.206) 1024(1052) bytes of data.1032 bytes from 192.168.120.206: icmp_seq=1 ttl=64 time=1.99 ms1032 bytes from 192.168.120.206: icmp_seq=2 ttl=64 time=0.694 ms1032 bytes from 192.168.120.206: icmp_seq=3 ttl=64 time=0.300 ms1032 bytes from 192.168.120.206: icmp_seq=4 ttl=64 time=0.481 ms1032 bytes from 192.168.120.206: icmp_seq=5 ttl=64 time=0.415 ms1032 bytes from 192.168.120.206: icmp_seq=6 ttl=64 time=0.600 ms1032 bytes from 192.168.120.206: icmp_seq=7 ttl=64 time=0.411 ms1032 bytes from 192.168.120.206: icmp_seq=8 ttl=64 time=0.281 ms1032 bytes from 192.168.120.206: icmp_seq=9 ttl=64 time=0.318 ms1032 bytes from 192.168.120.206: icmp_seq=10 ttl=64 time=0.362 ms1032 bytes from 192.168.120.206: icmp_seq=11 ttl=64 time=0.408 ms1032 bytes from 192.168.120.206: icmp_seq=12 ttl=64 time=0.445 ms1032 bytes from 192.168.120.206: icmp_seq=13 ttl=64 time=0.397 ms1032 bytes from 192.168.120.206: icmp_seq=14 ttl=64 time=0.406 ms1032 bytes from 192.168.120.206: icmp_seq=15 ttl=64 time=0.458 ms--- 192.168.120.206 ping statistics ---15 packets transmitted, 15 received, 0% packet loss, time 41999msrtt min/avg/max/mdev = 0.281/0.531/1.993/0.404 ms 说明： -i 3 发送周期为 3秒 -s 设置发送包的大小为1024 -t 设置TTL值为 255 参考链接： http://man.linuxde.net/ping http://www.cnblogs.com/peida/archive/2013/03/06/2945407.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[beego框架图文简介讲解03]]></title>
    <url>%2F2018%2F10%2F10%2Fbeego%E6%A1%86%E6%9E%B6%E5%9B%BE%E6%96%87%E7%AE%80%E4%BB%8B%E8%AE%B2%E8%A7%A303%2F</url>
    <content type="text"><![CDATA[2.8Go操作MySQL数据库（简单方法） 安装go操作MySQL的驱动 1go get -u -v github.com/go-sql-driver/mysql go简单操作MySQL数据库 导包 1import "github.com/go-sql-driver/mysql" 连接数据库，用sql.Open()方法,open()方法的第一个参数是驱动名称,第二个参数是用户名:密码@tcp(ip:port)/数据库名称?编码方式,返回值是连接对象和错误信息，例如： 12conn,err := sql.Open("mysql","root:123456@tcp(127.0.0.1:3306)/test?charset=utf8")defer conn.Close()//随手关闭数据库是个好习惯 执行数据库操作，这一步分为两种情况，一种是增删改，一种是查询，因为增删改不返回数据，只返回执行结果，查询要返回数据，所以这两块的操作函数不一样。 创建表 创建表的方法也是Exec(),参数是SQL语句，返回值是结果集和错误信息： 12res ,err:= conn.Exec("create table user(name VARCHAR(40),pwd VARCHAR(40))")beego.Info("create table result=",res,err) 增删改操作 执行增删改操作语句的是Exec()，参数是SQL语句，返回值是结果集和错误信息，通过对结果集的判断，得到执行结果的信息。以插入数据为例代码如下： 123res,_:=conn.Exec("insert into user(name,pwd) values (?,?)","tony","tony")count,_:=res.RowsAffected()this.Ctx.WriteString(strconv.Itoa(int(count))) 查询操作 用的函数是Query(),参数是SQL语句，返回值是查询结果集和错误信息，然后循环结果集取出其中的数据。代码如下： 12345678data ,err :=conn.Query("SELECT name from user") var userName string if err == nil&#123; for data.Next()&#123; data.Scan(&amp;userName) beego.Info(userName) &#125; &#125; 全部代码 123456789101112131415161718192021//连接数据库conn,err := sql.Open("mysql","root:123456@tcp(127.0.0.1:3306)/test?charset=utf8") if err != nil&#123; beego.Info("链接失败") &#125; defer conn.Close()//建表 res ,err:= conn.Exec("create table user(userName VARCHAR(40),passwd VARCHAR(40))") beego.Info("create table result=",res,err)//插入数据 res,err =conn.Exec("insert user(userName,passwd) values(?,?)","itcast","heima") beego.Info(res,err)//查询数据 data ,err :=conn.Query("SELECT userName from user") var userName string if err == nil&#123; for data.Next()&#123; data.Scan(&amp;userName) beego.Error(userName) &#125; &#125;]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>beego框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[beego框架图文简介讲解02]]></title>
    <url>%2F2018%2F10%2F10%2Fbeego%E6%A1%86%E6%9E%B6%E5%9B%BE%E6%96%87%E7%AE%80%E4%BB%8B%E8%AE%B2%E8%A7%A302%2F</url>
    <content type="text"><![CDATA[2.5Beego运行流程分析 浏览器发出请求 路由拿到请求，并给相应的请求指定相应的控制器 找到指定的控制器之后，控制器看是否需要查询数据库 如果需要查询数据库就找model取数据 如果不需要数据库，直接找view要视图 控制器拿到视图页面之后，把页面返回给浏览器 根据文字流程分析代码流程 从项目的入口main.go开始 找到router.go文件的Init函数 找到路由指定的控制器文件default.go的Get方法 然后找到指定视图的语法，整个项目就串起来啦。 2.6Post案例实现刚才我们分析了beego项目的整个运行流程，最终是如何调到Get方法的呢？beego通过内部语法给不同的http请求指定了不同的方法，因为我们是从浏览器地址栏发送的请求，属于get请求，所以调用的是Get方法。为了检验老师说的对不对，我们可以实现一个post请求，看看效果。 2.6.1前端修改前端代码如下: 修改我们刚才创建的新的视图，为了能够发送post请求，我们在视图中添加一个能发送post请求的控件form 123&lt;form method="post" action="/index"&gt; &lt;input type="submit"&gt;&lt;/form&gt; 然后设置一个能接收后台传递过来的数据的标签 1&lt;h1&gt;hello &#123;&#123;.data&#125;&#125;&lt;/h1&gt; 全部代码： 123456789101112131415&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;form method="post" action="/index"&gt; &lt;input type="submit"&gt;&lt;/form&gt;&lt;h1&gt;hello &#123;&#123;.data&#125;&#125;&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; 2.6.2后台代码修改后台代码 先设置我们Get请求要传递的数据和要显示的视图页面 1234func (c *MainController) Get() &#123; c.Data["data"] = "world" c.TplName = "test.html" //渲染&#125; 再设置我们post请求要传递的数据和要显示的视图页面 1234func (c *MainController) Post() &#123; c.Data["data"] = "我是最棒的" c.TplName = "test.html" //渲染&#125; 操作 先在浏览器输入网址，然后点击页面上的按钮，看一下页面的变化，有没有出现我是最棒的几个字 2.7Beego中路由的快速体验2.7.1路由的简单设置路由的作用：根据不同的请求指定不同的控制器 1路由函数：beego.Router(&quot;/path&quot;,&amp;controller.MainController&#123;&#125;) 函数参数： 先分析一下Url地址由哪几部分组成？ 同一资源定位符 http://192.168.110.71:8080/index http://地址:端口/资源路径 第一个参数：资源路径，也就是/后面的内容 第二个参数：需要指定的控制器指针 了解上面的内容之后我们来看几个简单的例子： 123beego.Router("/", &amp;controllers.MainController&#123;&#125;)beego.Router("/index", &amp;controllers.IndexController&#123;&#125;)beego.Router("/login", &amp;controllers.LoginController&#123;&#125;) 2.7.2高级路由设置一般在开发过程中，我们基本不使用beego提供的默认请求访问方法，都是自定义相应的方法。那我们来看一下如何来自定义请求方法。 自定义请求方法需要用到Router的第三个参数。这个参数是用来给不同的请求指定不同的方法。具体有如下几种情况。 一个请求访问一个方法(也是最常用的)，请求和方法之间用 : 隔开，不同的请求用 ; 隔开: 1beego.Router("/simple",&amp;SimpleController&#123;&#125;,"get:GetFunc;post:PostFunc") 可以多个请求，访问一个方法 ，请求之间用,隔开，请求与方法之间用:隔开： 1beego.Router("/api",&amp;RestController&#123;&#125;,"get,post:ApiFunc") 所有的请求访问同一个方法，用*号代表所有的请求，和方法之间用:隔开： 1beego.Router("/api/list",&amp;RestController&#123;&#125;,"*:ListFood") 如果同时存在 * 和对应的 HTTP请求，那么优先执行 HTTP请求所对应的方法，例如同时注册了如下所示的路由： 1beego.Router("/simple",&amp;SimpleController&#123;&#125;,"*:AllFunc;post:PostFunc") 那么当遇到Post请求的时候，执行PostFunc而不是AllFunc。 如果用了自定义方法之后，默认请求将不能访问。]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>beego框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[beego框架图文简介讲解01]]></title>
    <url>%2F2018%2F10%2F10%2Fbeego%E6%A1%86%E6%9E%B6%E5%9B%BE%E6%96%87%E7%AE%80%E4%BB%8B%E8%AE%B2%E8%A7%A301%2F</url>
    <content type="text"><![CDATA[beego框架1.Beego框架了解2.beego框架快速入门1.1beego框架了解 ​ Beego作者：谢孟军 Beego框架是go语言开发的web框架。 那什么是框架呢？就是别人写好的代码，我们可以直接使用！这个代码是专门针对某一个开发方向定制的，例如：我们要做一个网站，利用 beego 框架就能非常快的完成网站的开发，如果没有框架，每一个细节都需要我们处理，开发速度会大大降低。 go语言的web框架：beego,gin,echo等等，那为什么我们选择beego呢？ 第一，beego是中国人开发的，开发文档比较详细，beego官网网址: https://beego.me/ 。第二，现在公司里面用beego的也比较多，比如今日头条，百度云盘，腾讯，阿里等。 2.1MVC架构Beego是MVC架构。MVC 是一种应用非常广泛的体系架构，几乎所有的编程语言都会使用到，而且所有的程序员在工作中都会遇到！用 MVC 的方式开发程序，可以让程序的结构更加合理和清晰。 我们画图说明 beego具体是如何内嵌MVC呢？我们搭起环境通过代码分析。 2.2环境搭建这里默认大家已经搭建好了go语言的开发环境。 需要安装Beego源码和Bee开发工具//sudo apt-get install 12$ go get -u -v github.com/astaxie/beego$ go get -u -v github.com/beego/bee beego源码大家都了解，就是框架的源码。 Bee开发工具带有很多Bee命令。比如bee new创建项目，bee run运行项目等。 用bee运行项目，项目自带热更新：是现在后台程序常用的一种技术，即在服务器运行期间，可以不停服替换静态资源。替换go文件时会自动重新编译。 安装完之后，bee可执行文件默认存放在$GOPATH/bin里面，所以需要把$GOPATH/bin添加到环境变量中才可以进行下一步 123456$ cd ~$ vim .bashrc//在最后一行插入export PATH="$GOPATH/bin:$PATH"//然后保存退出$ source .bashrc 安装好之后，运行bee new preojectName来创建一个项目，注意：通过bee创建的项目代码都是在$GOPATH/src目录下面的 生成的项目目录结构如下: 1234567891011121314151617quickstart|-- conf| `-- app.conf|-- controllers| `-- default.go|-- main.go|-- models|-- routers| `-- router.go|-- static| |-- css| |-- img| `-- js|-- tests| `-- default_test.go|-- views `-- index.tpl 进入项目目录 执行bee run命令，在浏览器输入网址：127.0.0.1：8080，显示如下： 2.3beego的项目结构分析1234567891011121314151617quickstart|-- conf| `-- app.conf|-- controllers| `-- default.go|-- main.go|-- models|-- routers| `-- router.go|-- static| |-- css| |-- img| `-- js|-- tests| `-- default_test.go|-- views `-- index.tpl conf文件夹:放的是项目有关的配置文件 controllers:存放主要的业务代码 main.go:项目的入口文件 models:存放的是数据库有关内容 routers:存放路由文件，路由作用是根据不同的请求指定不同的控制器 static：存放静态资源，包括图片，html页面，css样式，js文件等 tests:测试文件 views：存放视图有关内容 后面我们重点需要操作的是MVC文件夹，routers文件夹。 2.4Beego快速体验 前面我们简单了解了 beego初始化的内容，那么就来个beego的快速体验吧！ 根据上图所示的步骤，对自己创建的项目进行三步修改，然后在浏览器是否能看到修改之后的效果。 如果把你们前面做的静态网页放到views文件夹下呢？一个静态网站是不是就出现啦！有没有感受到beego开发网站的快捷！ 代码分析 123c.Data[&quot;Email&quot;] = &quot;astaxie@gmail.com&quot;是给视图传递数据，在视图界面里面需要用&#123;&#123; &#125;&#125;加上 . 才能获取到，比如这行代码的意思就是，给视图传递，Key为Email，value为astaxie@gmail.com 的数据。在视图中要通过&#123;&#123;.Email&#125;&#125;就能获取到value值。c.TplName = &quot;index.tpl&quot;的作用是指定视图。这里面需要注意的是，默认指定的界面是tpl结尾，但是打开这个文件分析，发现还是一个html界面。所以我们也可以用html文件当视图文件。 通过我们对Beego的快速体验能够得出如下结论： 控制器(Controller)的作用 1.能够给视图传递数据 2.能够指定视图 视图(View)的作用 1.view本质就是个html。所以能在浏览器显示 2.能够接收控制器传递过来的数据]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>beego框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-lsof命令]]></title>
    <url>%2F2018%2F10%2F10%2FLinux%E5%91%BD%E4%BB%A4-lsof%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[lsof（list open files）是一个列出当前系统打开文件的工具。在linux环境下，任何事物都以文件的形式存在，通过文件不仅仅可以访问常规数据，还可以访问网络连接和硬件。所以如传输控制协议 (TCP) 和用户数据报协议 (UDP) 套接字等，系统在后台都为该应用程序分配了一个文件描述符，无论这个文件的本质如何，该文件描述符为应用程序与基础操作系统之间的交互提供了通用接口。因为应用程序打开文件的描述符列表提供了大量关于这个应用程序本身的信息，因此通过lsof工具能够查看这个列表对系统监测以及排错将是很有帮助的。 语法lsof(选项) 选项123456789101112-a：列出打开文件存在的进程；-c&lt;进程名&gt;：列出指定进程所打开的文件；-g：列出GID号进程详情；-d&lt;文件号&gt;：列出占用该文件号的进程；+d&lt;目录&gt;：列出目录下被打开的文件；+D&lt;目录&gt;：递归列出目录下被打开的文件；-n&lt;目录&gt;：列出使用NFS的文件；-i&lt;条件&gt;：列出符合条件的进程。（4、6、协议、:端口、 @ip ）-p&lt;进程号&gt;：列出指定进程号所打开的文件；-u：列出UID号进程详情；-h：显示帮助信息；-v：显示版本信息。 命令功能用于查看你进程开打的文件，打开文件的进程，进程打开的端口(TCP、UDP)。找回/恢复删除的文件。是十分方便的系统监视工具，因为 lsof 需要访问核心内存和各种文件，所以需要root用户执行。 lsof打开的文件可以是： 1.普通文件 2.目录 3.网络文件系统的文件 4.字符或设备文件 5.(函数)共享库 6.管道，命名管道 7.符号链接 8.网络文件（例如：NFS file、网络socket，unix域名socket） 9.还有其它类型的文件，等等 常用实例1）无任何参数 123456789101112# lsofcommand PID USER FD type DEVICE SIZE NODE NAMEinit 1 root cwd DIR 8,2 4096 2 /init 1 root rtd DIR 8,2 4096 2 /init 1 root txt REG 8,2 43496 6121706 /sbin/initinit 1 root mem REG 8,2 143600 7823908 /lib64/ld-2.5.soinit 1 root mem REG 8,2 1722304 7823915 /lib64/libc-2.5.soinit 1 root mem REG 8,2 23360 7823919 /lib64/libdl-2.5.soinit 1 root mem REG 8,2 95464 7824116 /lib64/libselinux.so.1init 1 root mem REG 8,2 247496 7823947 /lib64/libsepol.so.1init 1 root 10u FIFO 0,17 1233 /dev/initctlmigration 2 root cwd DIR 8,2 4096 2 / 说明： lsof输出各列信息的意义如下： COMMAND：进程的名称 PID：进程标识符 PPID：父进程标识符（需要指定-R参数） USER：进程所有者 PGID：进程所属组 FD：文件描述符，应用程序通过文件描述符识别该文件。 文件描述符列表： cwd：表示current work dirctory，即：应用程序的当前工作目录，这是该应用程序启动的目录，除非它本身对这个目录进行更改 txt：该类型的文件是程序代码，如应用程序二进制文件本身或共享库，如上列表中显示的 /sbin/init 程序 lnn：library references (AIX); er：FD information error (see NAME column); jld：jail directory (FreeBSD); ltx：shared library text (code and data); mxx ：hex memory-mapped type number xx. m86：DOS Merge mapped file; mem：memory-mapped file; mmap：memory-mapped device; pd：parent directory; rtd：root directory; tr：kernel trace file (OpenBSD); v86 VP/ix mapped file; 0：表示标准输出 1：表示标准输入 2：表示标准错误 一般在标准输出、标准错误、标准输入后还跟着文件状态模式： u：表示该文件被打开并处于读取/写入模式。 r：表示该文件被打开并处于只读模式。 w：表示该文件被打开并处于。 空格：表示该文件的状态模式为unknow，且没有锁定。 -：表示该文件的状态模式为unknow，且被锁定。 同时在文件状态模式后面，还跟着相关的锁： N：for a Solaris NFS lock of unknown type; r：for read lock on part of the file; R：for a read lock on the entire file; w：for a write lock on part of the file;（文件的部分写锁） W：for a write lock on the entire file;（整个文件的写锁） u：for a read and write lock of any length; U：for a lock of unknown type; x：for an SCO OpenServer Xenix lock on part of the file; X：for an SCO OpenServer Xenix lock on the entire file; space：if there is no lock. 文件类型： DIR：表示目录。 CHR：表示字符类型。 BLK：块设备类型。 UNIX： UNIX 域套接字。 FIFO：先进先出 (FIFO) 队列。 IPv4：网际协议 (IP) 套接字。 DEVICE：指定磁盘的名称 SIZE：文件的大小 NODE：索引节点（文件在磁盘上的标识） NAME：打开文件的确切名称 2）查看谁正在使用某个文件，也就是说查找某个文件相关的进程 1234# lsof /bin/bashCOMMAND PID USER FD TYPE DEVICE SIZE NODE NAMEbash 24159 root txt REG 8,2 801528 5368780 /bin/bashbash 24909 root txt REG 8,2 801528 5368780 /bin/bash 3）递归查看某个目录的文件信息 1234# lsof test/test3COMMAND PID USER FD TYPE DEVICE SIZE NODE NAMEbash 24941 root cwd DIR 8,2 4096 2258872 test/test3vi 24976 root cwd DIR 8,2 4096 2258872 test/test3 说明： 使用了+D，对应目录下的所有子目录和文件都会被列出 4）不使用+D选项，遍历查看某个目录的所有文件信息的方法 1234# lsof |grep &apos;test/test3&apos;bash 24941 root cwd DIR 8,2 4096 2258872 /opt/soft/test/test3vi 24976 root cwd DIR 8,2 4096 2258872 /opt/soft/test/test3vi 24976 root 4u REG 8,2 12288 2258882 /opt/soft/test/test3/.log2013.log.swp 5）列出某个用户打开的文件信息 1# lsof -u username 说明: -u 选项，u其实是user的缩写 6）列出某个程序进程所打开的文件信息 1# lsof -c mysql 说明: -c 选项将会列出所有以mysql这个进程开头的程序的文件，其实你也可以写成 lsof | grep mysql, 但是第一种方法明显比第二种方法要少打几个字符了 7）列出多个进程多个打开的文件信息 1# lsof -c mysql -c apache 8）列出某个用户以及某个进程所打开的文件信息 1# lsof -u test -c mysql 9）列出除了某个用户外的被打开的文件信息 1# lsof -u ^root 说明： ^这个符号在用户名之前，将会把是root用户打开的进程不让显示 10）通过某个进程号显示该进行打开的文件 1# lsof -p 1 11）列出多个进程号对应的文件信息 1# lsof -p 1,2,3 12）列出除了某个进程号，其他进程号所打开的文件信息 1# lsof -p ^1 13）列出所有的网络连接 1# lsof -i 14）列出所有tcp 网络连接信息 1# lsof -i tcp 15）列出所有udp网络连接信息 1# lsof -i udp 16）列出谁在使用某个端口 1# lsof -i :3306 17）列出谁在使用某个特定的udp端口 1# lsof -i udp:55 18）特定的tcp端口 1# lsof -i tcp:80 19）列出某个用户的所有活跃的网络端口 1# lsof -a -u test -i 20）列出所有网络文件系统 1# lsof -N 21）域名socket文件 1# lsof -u 22）某个用户组所打开的文件信息 1# lsof -g 5555 23）根据文件描述列出对应的文件信息 1234# lsof -d description(like 2)# lsof -d txt# lsof -d 1# lsof -d 2 说明： 0表示标准输入，1表示标准输出，2表示标准错误，从而可知：所以大多数应用程序所打开的文件的 FD 都是从 3 开始 24）根据文件描述范围列出文件信息 1# lsof -d 2-3 25）列出COMMAND列中包含字符串” sshd”，且文件描符的类型为txt的文件信息 1234# lsof -c sshd -a -d txtCOMMAND PID USER FD TYPE DEVICE SIZE NODE NAMEsshd 2756 root txt REG 8,2 409488 1027867 /usr/sbin/sshdsshd 24155 root txt REG 8,2 409488 1027867 /usr/sbin/sshd 26）列出被进程号为1234的进程所打开的所有IPV4 network files 1# lsof -i 4 -a -p 1234 27）列出目前连接主机peida.linux上端口为：20，21，22，25，53，80相关的所有文件信息，且每隔3秒不断的执行lsof指令 1# lsof -i @peida.linux:20,21,22,25,53,80 -r 3 参考链接： http://www.cnblogs.com/peida/archive/2013/02/26/2932972.html https://man.linuxde.net/lsof]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-crontab命令]]></title>
    <url>%2F2018%2F10%2F10%2FLinux%E5%91%BD%E4%BB%A4-crontab%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[crontab命令被用来提交和管理用户的需要周期性执行的任务，与windows下的计划任务类似，当安装完成操作系统后，默认会安装此服务工具，并且会自动启动crond进程，crond进程每分钟会定期检查是否有要执行的任务，如果有要执行的任务，则自动执行该任务。 语法crontab(选项)(参数) 选项12345-e：编辑该用户的计时器设置;-l：列出该用户的计时器设置;-r：删除该用户的计时器设置;-u&lt;用户名称&gt;：指定要设定计时器的用户名称;-i：在删除用户的crontab文件时给确认提示。 参数crontab文件：指定包含待执行任务的crontab文件。 知识扩展Linux下的任务调度分为两类：系统任务调度和用户任务调度 系统任务调度：系统周期性所要执行的工作，比如写缓存数据到硬盘、日志清理等。在/etc目录下有一个crontab文件，这个就是系统任务调度的配置文件。 /etc/crontab文件包括下面几行： 123456789SHELL=/bin/bashPATH=/sbin:/bin:/usr/sbin:/usr/binMAILTO=&quot;&quot;HOME=/# run-parts51 * * * * root run-parts /etc/cron.hourly24 7 * * * root run-parts /etc/cron.daily22 4 * * 0 root run-parts /etc/cron.weekly42 4 1 * * root run-parts /etc/cron.monthly 前四行是用来配置crond任务运行的环境变量，第一行SHELL变量指定了系统要使用哪个shell，这里是bash，第二行PATH变量指定了系统执行命令的路径，第三行MAILTO变量指定了crond的任务执行信息将通过电子邮件发送给root用户，如果MAILTO变量的值为空，则表示不发送任务执行信息给用户，第四行的HOME变量指定了在执行命令或者脚本时使用的主目录。 用户任务调度：用户定期要执行的工作，比如用户数据备份、定时邮件提醒等。用户可以使用 crontab 工具来定制自己的计划任务。所有用户定义的crontab文件都被保存在/var/spool/cron目录中。其文件名与用户名一致，使用者权限文件如下： 123/etc/cron.deny 该文件中所列用户不允许使用crontab命令/etc/cron.allow 该文件中所列用户允许使用crontab命令/var/spool/cron/ 所有用户crontab文件存放的目录,以用户名命名 crontab文件的含义：用户所建立的crontab文件中，每一行都代表一项任务，每行的每个字段代表一项设置，它的格式共分为六个字段，前五段是时间设定段，第六段是要执行的命令段，格式如下： 1minute hour day month week command 顺序：分 时 日 月 周 minute： 表示分钟，可以是从0到59之间的任何整数。 hour：表示小时，可以是从0到23之间的任何整数。 day：表示日期，可以是从1到31之间的任何整数。 month：表示月份，可以是从1到12之间的任何整数。 week：表示星期几，可以是从0到7之间的任何整数，这里的0或7代表星期日。 command：要执行的命令，可以是系统命令，也可以是自己编写的脚本文件。 在以上各个字段中，还可以使用以下特殊字符： 星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作。 逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9” 中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6” 正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如*/10，如果用在minute字段，表示每十分钟执行一次。 查看crontab服务状态： 1service crond status 手动启动crontab服务： 1service crond start 查看crontab服务是否已设置为开机启动，执行命令： 1ntsysv 加入开机自动启动： 1chkconfig –level 35 crond on 常用方法1). 创建一个新的crontab文件 在考虑向cron进程提交一个crontab文件之前，首先要做的一件事情就是设置环境变量EDITOR。cron进程根据它来确定使用哪个编辑器编辑crontab文件。9 9 %的UNIX和LINUX用户都使用vi，如果你也是这样，那么你就编辑$ HOME目录下的. profile文件，在其中加入这样一行： EDITOR=vi; export EDITOR 然后保存并退出。不妨创建一个名为\ cron的文件，其中\是用户名，例如， davecron。在该文件中加入如下的内容。 (put your own initials here)echo the date to the console every15minutes between 6pm and 6am 0,15,30,45 18-06 * /bin/echo ‘date’ &gt; /dev/console 保存并退出。确信前面5个域用空格分隔。 在上面的例子中，系统将每隔1 5分钟向控制台输出一次当前时间。如果系统崩溃或挂起，从最后所显示的时间就可以一眼看出系统是什么时间停止工作的。在有些系统中，用tty1来表示控制台，可以根据实际情况对上面的例子进行相应的修改。为了提交你刚刚创建的crontab文件，可以把这个新创建的文件作为cron命令的参数： $ crontab davecron 现在该文件已经提交给cron进程，它将每隔1 5分钟运行一次。 同时，新创建文件的一个副本已经被放在/var/spool/cron目录中，文件名就是用户名(即dave)。 2). 列出crontab文件 为了列出crontab文件，可以用： $ crontab -l 0,15,30,45,18-06 * /bin/echo date &gt; dev/tty1 你将会看到和上面类似的内容。可以使用这种方法在$ H O M E目录中对crontab文件做一备份： $ crontab -l &gt; $HOME/mycron 这样，一旦不小心误删了crontab文件，可以用上一节所讲述的方法迅速恢复。 3). 编辑crontab文件 如果希望添加、删除或编辑crontab文件中的条目，而E D I TO R环境变量又设置为v i，那么就可以用v i来编辑crontab文件，相应的命令为： $ crontab -e 可以像使用v i编辑其他任何文件那样修改crontab文件并退出。如果修改了某些条目或添加了新的条目，那么在保存该文件时， c r o n会对其进行必要的完整性检查。如果其中的某个域出现了超出允许范围的值，它会提示你。 我们在编辑crontab文件时，没准会加入新的条目。例如，加入下面的一条： 1# DT:delete core files,at 3.30am on 1,7,14,21,26,26 days of each month 30 3 1,7,14,21,26 /bin/find -name “core’ -exec rm {} \; 现在保存并退出。最好在crontab文件的每一个条目之上加入一条注释，这样就可以知道它的功能、运行时间，更为重要的是，知道这是哪位用户的作业。 现在让我们使用前面讲过的crontab -l命令列出它的全部信息： 1234567891011 $ crontab -l # (crondave installed on Tue May 4 13:07:43 1999) # DT:ech the date to the console every 30 minites0,15,30,45 18-06 * * * /bin/echo `date` &gt; /dev/tty1 # DT:delete core files,at 3.30am on 1,7,14,21,26,26 days of each month 30 3 1,7,14,21,26 * * /bin/find -name &quot;core&apos; -exec rm &#123;&#125; \; 4). 删除crontab文件 要删除crontab文件，可以用： 1$ crontab -r 5). 恢复丢失的crontab文件 如果不小心误删了crontab文件，假设你在自己的$ H O M E目录下还有一个备份，那么可以将其拷贝到/var/spool/cron/\，其中\是用户名。如果由于权限问题无法完成拷贝，可以用： $ crontab \ 其中，\是你在$ H O M E目录中副本的文件名。 我建议你在自己的$ H O M E目录中保存一个该文件的副本。我就有过类似的经历，有数次误删了crontab文件（因为r键紧挨在e键的右边）。这就是为什么有些系统文档建议不要直接编辑crontab文件，而是编辑该文件的一个副本，然后重新提交新的文件。 有些crontab的变体有些怪异，所以在使用crontab命令时要格外小心。如果遗漏了任何选项，crontab可能会打开一个空文件，或者看起来像是个空文件。这时敲delete键退出，不要按，否则你将丢失crontab文件。 常用实例1）每1分钟执行一次command 1* * * * * command 2）每小时的第3和第15分钟执行 13,15 * * * * command 3）在上午8点到11点的第3和第15分钟执行 13,15 8-11 * * * command 4）每隔两天的上午8点到11点的第3和第15分钟执行 13,15 8-11 */2 * * command 5）每个星期一的上午8点到11点的第3和第15分钟执行 13,15 8-11 * * 1 command 6）每晚的21:30重启smb 130 21 * * * /etc/init.d/smb restart 7）每月1、10、22日的4 : 45重启smb 145 4 1,10,22 * * /etc/init.d/smb restart 8）每周六、周日的1 : 10重启smb 110 1 * * 6,0 /etc/init.d/smb restart 9）每天18 : 00至23 : 00之间每隔30分钟重启smb 10,30 18-23 * * * /etc/init.d/smb restart 10）每星期六的晚上11 : 00 pm重启smb 10 23 * * 6 /etc/init.d/smb restart 11）每一小时重启smb 1* */1 * * * /etc/init.d/smb restart 12）晚上11点到早上7点之间，每隔一小时重启smb 1* 23-7/1 * * * /etc/init.d/smb restart 13）每月的4号与每周一到周三的11点重启smb 10 11 4 * mon-wed /etc/init.d/smb restart 14）一月一号的4点重启smb 10 4 1 jan * /etc/init.d/smb restart 15）每小时执行/etc/cron.hourly目录内的脚本 101 * * * * root run-parts /etc/cron.hourly 说明： run-parts这个参数了，如果去掉这个参数的话，后面就可以写要运行的某个脚本名，而不是目录名了 使用注意事项有时我们创建了一个crontab，但是这个任务却无法自动执行，而手动执行这个任务却没有问题，这种情况一般是由于在crontab文件中没有配置环境变量引起的。 在crontab文件中定义多个调度任务时，需要特别注意的一个问题就是环境变量的设置，因为我们手动执行某个任务时，是在当前shell环境下进行的，程序当然能找到环境变量，而系统自动执行任务调度时，是不会加载任何环境变量的，因此，就需要在crontab文件中指定任务运行所需的所有环境变量，这样，系统执行任务调度时就没有问题了。 不要假定cron知道所需要的特殊环境，它其实并不知道。所以你要保证在shelll脚本中提供所有必要的路径和环境变量，除了一些自动设置的全局变量。所以注意如下3点： 1）脚本中涉及文件路径时写全局路径； 2）脚本执行要用到java或其他环境变量时，通过source命令引入环境变量，如： cat start_cbp.sh #!/bin/sh source /etc/profile export RUN_CONF=/home/d139/conf/platform/cbp/cbp_jboss.conf /usr/local/jboss-4.0.5/bin/run.sh -c mev &amp; 3）当手动执行脚本OK，但是crontab死活不执行时。这时必须大胆怀疑是环境变量惹的祸，并可以尝试在crontab中直接引入环境变量解决问题。如： 0 . /etc/profile;/bin/sh /var/www/java/audit_no_count/bin/restart_audit.sh 注意清理系统用户的邮件日志 每条任务调度执行完毕，系统都会将任务输出信息通过电子邮件的形式发送给当前系统用户，这样日积月累，日志信息会非常大，可能会影响系统的正常运行，因此，将每条任务进行重定向处理非常重要。 例如，可以在crontab文件中设置如下形式，忽略日志输出： 0 /3 /usr/local/apache2/apachectl restart &gt;/dev/null 2&gt;&amp;1 “/dev/null 2&gt;&amp;1”表示先将标准输出重定向到/dev/null，然后将标准错误重定向到标准输出，由于标准输出已经重定向到了/dev/null，因此标准错误也会重定向到/dev/null，这样日志输出问题就解决了。 . 系统级任务调度与用户级任务调度 系统级任务调度主要完成系统的一些维护操作，用户级任务调度主要完成用户自定义的一些任务，可以将用户级任务调度放到系统级任务调度来完成（不建议这么做），但是反过来却不行，root用户的任务调度操作可以通过“crontab –uroot –e”来设置，也可以将调度任务直接写入/etc/crontab文件，需要注意的是，如果要定义一个定时重启系统的任务，就必须将任务放到/etc/crontab文件，即使在root用户下创建一个定时重启系统的任务也是无效的。 其他注意事项 新创建的cron job，不会马上执行，至少要过2分钟才执行。如果重启cron则马上执行。 当crontab突然失效时，可以尝试/etc/init.d/crond restart解决问题。或者查看日志看某个job有没有执行/报错tail -f /var/log/cron。 千万别乱运行crontab -r。它从Crontab目录（/var/spool/cron）中删除用户的Crontab文件。删除了该用户的所有crontab都没了。 在crontab中%是有特殊含义的，表示换行的意思。如果要用的话必须进行转义\%，如经常用的date ‘+%Y%m%d’在crontab里是不会执行的，应该换成date ‘+\%Y\%m\%d’。 参考链接： http://www.cnblogs.com/peida/archive/2013/01/08/2850483.html http://man.linuxde.net/crontab]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-watch命令]]></title>
    <url>%2F2018%2F10%2F08%2FLinux%E5%91%BD%E4%BB%A4-watch%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[watch是一个非常实用的命令，基本所有的Linux发行版都带有这个小工具，如同名字一样，watch可以帮你监测一个命令的运行结果，省得你一遍遍的手动运行。在Linux下，watch是周期性的执行下个程序，并全屏显示执行结果。你可以拿他来监测你想要的一切命令的结果变化，比如 tail 一个 log 文件，ls 监测某个文件的大小变化，看你的想象力了！ 语法watch(选项)(参数) 选项123-n：指定指令执行的间隔时间（秒）；-d：高亮显示指令输出信息不同之处；-t：不显示标题。 参数指令：需要周期性执行的指令。 常用实例1）每隔一秒高亮显示网络链接数的变化情况 1# watch -n 1 -d netstat -ant 说明： 其它操作： 中断watch操作：Ctrl+c 2）每隔一秒高亮显示http链接数的变化情况 1watch -n 1 -d &apos;pstree|grep http&apos; 说明： 每隔一秒高亮显示http链接数的变化情况。 后面接的命令若带有管道符，需要加’’将命令区域归整。 3）实时查看模拟攻击客户机建立起来的连接数 1watch &apos;netstat -an | grep:21 | \ grep&lt;模拟攻击客户机的IP&gt;| wc -l&apos; 4）监测当前目录中 scf’ 的文件的变化 1watch -d &apos;ls -l|grep scf&apos; 5）10秒一次输出系统的平均负载 1watch -n 10 &apos;cat /proc/loadavg&apos; 6）监测磁盘inode和block数目变化情况 1#watch -n 1 &quot;df -i;df&quot; 参考链接： http://www.cnblogs.com/peida/archive/2012/12/31/2840241.html http://man.linuxde.net/watch]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-iostat命令]]></title>
    <url>%2F2018%2F10%2F08%2FLinux%E5%91%BD%E4%BB%A4-iostat%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Linux系统中的 iostat是I/O statistics（输入/输出统计）的缩写，iostat工具将对系统的磁盘操作活动进行监视。它的特点是汇报磁盘活动统计情况，同时也会汇报出CPU使用情况。同vmstat一样，iostat也有一个弱点，就是它不能对某个进程进行深入分析，仅对系统的整体情况进行分析。iostat属于sysstat软件包。可以用yum install sysstat 直接安装。 语法iostat(选项)(参数) 选项12345678-c：仅显示CPU使用情况；-d：仅显示设备利用率；-k：显示状态以千字节每秒为单位，而不使用块每秒；-m：显示状态以兆字节每秒为单位；-p：仅显示块设备和所有被使用的其他分区的状态；-t：显示每个报告产生时的时间；-V：显示版号并退出；-x：显示扩展状态。 参数 间隔时间：每次报告的间隔时间（秒）； 次数：显示报告的次数。 常用实例 1）显示所有设备负载情况 12345678910# iostat Linux 2.6.32-696.10.2.el6.x86_64 (dzh-hw-bj3219) 04/04/2018 _x86_64_ (8 CPU)avg-cpu: %user %nice %system %iowait %steal %idle 0.81 0.03 0.16 0.04 0.07 98.90Device: tps Blk_read/s Blk_wrtn/s Blk_read Blk_wrtnxvda 0.38 0.10 5.71 729274 43157304xvdb 30.95 7.62 686.84 57636578 5194927160dm-0 85.98 7.62 686.84 57635962 5194927160 说明： cpu属性值说明： %user：CPU处在用户模式下的时间百分比。 %nice：CPU处在带NICE值的用户模式下的时间百分比。 %system：CPU处在系统模式下的时间百分比。 %iowait：CPU等待输入输出完成时间的百分比。 %steal：管理程序维护另一个虚拟处理器时，虚拟CPU的无意识等待时间百分比。 %idle：CPU空闲时间百分比。 备注：如果%iowait的值过高，表示硬盘存在I/O瓶颈，%idle值高，表示CPU较空闲，如果%idle值高但系统响应慢时，有可能是CPU等待分配内存，此时应加大内存容量。%idle值如果持续低于10，那么系统的CPU处理能力相对较低，表明系统中最需要解决的资源是CPU。 2）定时显示所有信息 1iostat 2 3 说明： 每隔 2秒刷新显示，且显示3次 3）显示指定磁盘信息 12345# iostat -d xvdaLinux 2.6.32-696.10.2.el6.x86_64 (dzh-hw-bj3219) 04/04/2018 _x86_64_ (8 CPU)Device: tps Blk_read/s Blk_wrtn/s Blk_read Blk_wrtnxvda 0.38 0.10 5.71 729290 43160264 4）显示tty和cpu信息 1234567891011# iostat -tLinux 2.6.32-696.10.2.el6.x86_64 (dzh-hw-bj3219) 04/04/2018 _x86_64_ (8 CPU)04/04/2018 02:25:52 PMavg-cpu: %user %nice %system %iowait %steal %idle 0.81 0.03 0.16 0.04 0.07 98.90Device: tps Blk_read/s Blk_wrtn/s Blk_read Blk_wrtnxvda 0.38 0.10 5.71 729290 43160960xvdb 30.95 7.62 686.87 57637978 5195407104dm-0 85.99 7.62 686.87 57637362 5195407104 5）以m为单位显示所有信息 12345678910# iostat -mLinux 2.6.32-696.10.2.el6.x86_64 (dzh-hw-bj3219) 04/04/2018 _x86_64_ (8 CPU)avg-cpu: %user %nice %system %iowait %steal %idle 0.81 0.03 0.16 0.04 0.07 98.90Device: tps MB_read/s MB_wrtn/s MB_read MB_wrtnxvda 0.38 0.00 0.00 356 21075xvdb 30.95 0.00 0.34 28143 2536858dm-0 85.99 0.00 0.34 28143 2536858 6）查看TPS和吞吐量信息 1234567# iostat -d -k 1 1 Linux 2.6.32-696.10.2.el6.x86_64 (dzh-hw-bj3219) 04/04/2018 _x86_64_ (8 CPU)Device: tps kB_read/s kB_wrtn/s kB_read kB_wrtnxvda 0.38 0.05 2.85 364653 21581388xvdb 30.95 3.81 343.44 28819121 2597806052dm-0 85.99 3.81 343.44 28818813 2597806052 说明： tps：该设备每秒的传输次数（Indicate the number of transfers per second that were issued to the device.）。“一次传输”意思是“一次I/O请求”。多个逻辑请求可能会被合并为“一次I/O请求”。“一次传输”请求的大小是未知的。 kB_read/s：每秒从设备（drive expressed）读取的数据量； kB_wrtn/s：每秒向设备（drive expressed）写入的数据量； kB_read：读取的总数据量；kB_wrtn：写入的总数量数据量； 这些单位都为Kilobytes。 7）查看设备使用率（%util）、响应时间（await） 1234567# iostat -d -x -k 1 1 Linux 2.6.32-696.10.2.el6.x86_64 (dzh-hw-bj3219) 04/04/2018 _x86_64_ (8 CPU)Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilxvda 0.00 0.33 0.00 0.38 0.05 2.85 15.16 0.00 3.11 1.94 3.12 1.06 0.04xvdb 0.00 55.03 0.13 30.83 3.81 343.45 22.44 0.33 10.74 3.87 10.76 0.12 0.37dm-0 0.00 0.00 0.13 85.86 3.81 343.45 8.08 0.33 3.89 3.88 3.89 0.04 0.37 说明： rrqm/s： 每秒进行 merge 的读操作数目.即 delta(rmerge)/s wrqm/s： 每秒进行 merge 的写操作数目.即 delta(wmerge)/s r/s： 每秒完成的读 I/O 设备次数.即 delta(rio)/s w/s： 每秒完成的写 I/O 设备次数.即 delta(wio)/s rsec/s： 每秒读扇区数.即 delta(rsect)/s wsec/s： 每秒写扇区数.即 delta(wsect)/s rkB/s： 每秒读K字节数.是 rsect/s 的一半,因为每扇区大小为512字节.(需要计算) wkB/s： 每秒写K字节数.是 wsect/s 的一半.(需要计算) avgrq-sz：平均每次设备I/O操作的数据大小 (扇区).delta(rsect+wsect)/delta(rio+wio) avgqu-sz：平均I/O队列长度.即 delta(aveq)/s/1000 (因为aveq的单位为毫秒). await： 平均每次设备I/O操作的等待时间 (毫秒).即 delta(ruse+wuse)/delta(rio+wio) svctm： 平均每次设备I/O操作的服务时间 (毫秒).即 delta(use)/delta(rio+wio) %util： 一秒中有百分之多少的时间用于 I/O 操作,或者说一秒中有多少时间 I/O 队列是非空的，即 delta(use)/s/1000 (因为use的单位为毫秒) 如果 %util 接近 100%，说明产生的I/O请求太多，I/O系统已经满负荷，该磁盘可能存在瓶颈。 idle小于70% IO压力就较大了，一般读取速度有较多的wait。 同时可以结合vmstat 查看查看b参数(等待资源的进程数)和wa参数(IO等待所占用的CPU时间的百分比，高过30%时IO压力高)。 另外 await 的参数也要多和 svctm 来参考。差的过高就一定有 IO 的问题。 avgqu-sz 也是个做 IO 调优时需要注意的地方，这个就是直接每次操作的数据的大小，如果次数多，但数据拿的小的话，其实 IO 也会很小。如果数据拿的大，才IO 的数据会高。也可以通过 avgqu-sz × ( r/s or w/s ) = rsec/s or wsec/s。也就是讲，读定速度是这个来决定的。 svctm 一般要小于 await (因为同时等待的请求的等待时间被重复计算了)，svctm 的大小一般和磁盘性能有关，CPU/内存的负荷也会对其有影响，请求过多也会间接导致 svctm 的增加。await 的大小一般取决于服务时间(svctm) 以及 I/O 队列的长度和 I/O 请求的发出模式。如果 svctm 比较接近 await，说明 I/O 几乎没有等待时间；如果 await 远大于 svctm，说明 I/O 队列太长，应用得到的响应时间变慢，如果响应时间超过了用户可以容许的范围，这时可以考虑更换更快的磁盘，调整内核 elevator 算法，优化应用，或者升级 CPU。 队列长度(avgqu-sz)也可作为衡量系统 I/O 负荷的指标，但由于 avgqu-sz 是按照单位时间的平均值，所以不能反映瞬间的 I/O 洪水。 形象的比喻： r/s+w/s 类似于交款人的总数 平均队列长度(avgqu-sz)类似于单位时间里平均排队人的个数 平均服务时间(svctm)类似于收银员的收款速度 平均等待时间(await)类似于平均每人的等待时间 平均I/O数据(avgrq-sz)类似于平均每人所买的东西多少 I/O 操作率 (%util)类似于收款台前有人排队的时间比例 设备IO操作:总IO(io)/s = r/s(读) +w/s(写) =1.46 + 25.28=26.74 平均每次设备I/O操作只需要0.36毫秒完成,现在却需要10.57毫秒完成，因为发出的 请求太多(每秒26.74个)，假如请求时同时发出的，可以这样计算平均等待时间: 平均等待时间=单个I/O服务器时间*(1+2+…+请求总数-1)/请求总数 每秒发出的I/0请求很多,但是平均队列就4,表示这些请求比较均匀,大部分处理还是比较及时。 8）查看cpu状态 1234567891011# iostat -c 1 3Linux 2.6.32-696.10.2.el6.x86_64 (dzh-hw-bj3219) 04/04/2018 _x86_64_ (8 CPU)avg-cpu: %user %nice %system %iowait %steal %idle 0.81 0.03 0.16 0.04 0.07 98.90avg-cpu: %user %nice %system %iowait %steal %idle 2.29 0.13 0.38 0.00 0.13 97.07avg-cpu: %user %nice %system %iowait %steal %idle 4.32 0.00 0.51 0.13 0.00 95.04 参考链接： http://www.cnblogs.com/peida/archive/2012/12/28/2837345.html http://man.linuxde.net/iostat]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-vmstat命令]]></title>
    <url>%2F2018%2F10%2F08%2FLinux%E5%91%BD%E4%BB%A4-vmstat%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[vmstat是Virtual Meomory Statistics（虚拟内存统计）的缩写，可对操作系统的虚拟内存、进程、CPU活动进行监控。他是对系统整体情况进行统计，不足之处是无法对某个进程今次那个深入分析。vmstat工具提供了一种低开销的系统性能观察方式。因为vmstat本身就是低开销工具，在非常高负荷的服务器上，你需要查看并监控系统的健康情况，在控制窗口还是能够使用vmstat命令前，我们先了解下Linux系统中关于物理内存和虚拟内存相关信息。 物理内存和虚拟内存区别： 我们知道，直接从物理内存读取数据要比从硬盘读写数据要快的多，因此，我们希望所有数据的读取和写入都在内存完成，而内存是有限的，这样就引出了物理内存与虚拟内存的概念。 物理内存就是系统硬件提供的内存大小，是真正的内存，在linux下还有一个虚拟内存的概念，虚拟内存就是为了满足物理内存的不足而提出的策略，它是利用磁盘空间虚拟出的一块逻辑内存，用作虚拟内存的磁盘空间被称为交换空间（Swap Space）。 作为物理内存的扩展，linux会在物理内存不足时，使用交换分区的虚拟内存，更详细的说，就是内核会将暂时不用的内存块信息写到交换空间，这样以来，物理内存得到了释放，这块内存就是用于其目的，当需要用到原始的内容时，这些信息会被重新从交换空间读入物理内存。 linux的内存管理采取的是分页存取机制，为了保证物理内存能得到充分的利用，内核会在适当的时候讲物理内存中不经常使用的数据块自动交换到虚拟内存中，而将经常使用的信息保留到物理内存。 要深入了解linux内存运行机制，需要知道下面提到的几个方面： 首先，linux系统会不时的进行页面，以保持尽可能多的空闲物理内存，即使并没有什么事情需要内存，linux也会交换出暂时不用的内存页面。这可以避免等待交互所需的时间。 其次，linux进行页面交换是有条件的，不时所有页面在不用时都交换到虚拟内存，linux内核根据“最近最经常使用”算法，仅仅将一些不经常使用的页面文件交换到虚拟内存，有时我们会看到这么一个现象：linux物理内存还有很多，但是交换空间也使用了很多。其实，这并不奇怪，例如，一个占用很大内存的进程运行时，需要耗费很多内存资源，此时就会有一些不常用页面文件被交换到虚拟内存中，但后来这个占用很多内存资源的进程结束并释放了很多内存是，刚才被交换出去的页面文件并不会自动的交换进物理内存，除非有这个必要，那么此刻系统物理内存就会空闲很多，同时交换空间也在被使用，就出现了刚才所说的现象了。关于这点，不用担心什么，只要知道是怎么一回事就可以了。 最后，交换空间的页面在使用时会首先被交换到物理内存，如果此时没有足够的物理内存来容纳这些页面，他们又会被马上交换出去，如此以来，虚拟内存中可能没有足够空间来存储这些交换页面，最终会导致linux出现假死机、服务异常等问题，linux虽然可以在一段时间内自行恢复，但是恢复后的系统已经基本不可用了。 因此，合理规划和设计linux内存的使用，是非常重要的。 虚拟内存原理： 在系统中运行的每个进程都需要使用到内存，但不是每个进程都需要每时每刻使用系统分配的内存空间。当系统运行所需内存超过实际的物理内存，内核会释放某些进程所占用但未使用的部分或所有物理内存，将这部分资料存储在磁盘上直到进程下一次调用，并将释放出的内存提供给有需要的进程使用。 在linux内存管理中，主要是通过“调页Paging”和“交换Swapping”来完成上述的内存调度。调页算法是将内存中最近不常使用的页面换到磁盘上，把活动页面保留在内存中供进程使用。交换技术是将整个进程，而不是部分页面，全部交换到磁盘上。 分页（Page）写入磁盘的过程被称作Page-Out，分页（Page）从磁盘重新回到内存的过程被称作Page-In。当内核需要一个分页时，但发现此分页不在物理内存中（因为已经被Page-Out了），此时就发生了分页错误（Page Fault）。 当系统内核发现可运行内存变少时，就会通过Page-Out来释放一部分物理内存。尽管Page-Out不是经常发生，但是如果Page-out频繁不断的发生，直到当内核管理分页的时间超过运行程式的时间时，系统效能会急剧下降。这时的系统已经运行非常慢或进入暂停状态，这种状态也被称作thrashing（颠簸）。 语法vmstat(选项)(参数) 选项12345678-a：显示活动内页；-f：显示启动后创建的进程总数；-m：显示slab信息；-n：头信息仅显示一次；-s：以表格方式显示事件计数器和内存状态；-d：报告磁盘状态；-p：显示指定的硬盘分区状态；-S：输出信息的单位。 参数 事件间隔：状态信息刷新的时间间隔； 次数：显示报告的次数。 常用实例1）显示虚拟内存使用情况 12345678# vmstat 5 6procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------ r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 3029876 199616 690980 0 0 0 2 3 2 0 0 100 0 0 0 0 3029752 199616 690980 0 0 0 41 1009 39 0 0 100 0 0 0 0 3029752 199616 690980 0 0 0 3 1004 36 0 0 100 0 0 0 0 3029752 199616 690980 0 0 0 4 1004 36 0 0 100 0 0 0 0 3029752 199616 690980 0 0 0 6 1003 33 0 0 100 0 0 说明： 字段说明： Procs（进程）： r: 运行队列中进程数量 b: 等待IO的进程数量 Memory（内存）： swpd: 使用虚拟内存大小 free: 可用内存大小 buff: 用作缓冲的内存大小 cache: 用作缓存的内存大小 Swap： si: 每秒从交换区写到内存的大小 so: 每秒写入交换区的内存大小 IO：（现在的Linux版本块的大小为1024bytes） bi: 每秒读取的块数 bo: 每秒写入的块数 系统： in: 每秒中断数，包括时钟中断。 cs: 每秒上下文切换数。 CPU（以百分比表示）： us: 用户进程执行时间(user time) sy: 系统进程执行时间(system time) id: 空闲时间(包括IO等待时间),中央处理器的空闲时间 。以百分比表示。 wa: 等待IO时间 备注： 如果 r经常大于 4 ，且id经常少于40，表示cpu的负荷很重。如果pi，po 长期不等于0，表示内存不足。如果disk 经常不等于0， 且在 b中的队列 大于3， 表示 io性能不好。Linux在具有高稳定性、可靠性的同时，具有很好的可伸缩性和扩展性，能够针对不同的应用和硬件环境调整，优化出满足当前应用需要的最佳性能。因此企业在维护Linux系统、进行系统调优时，了解系统性能分析工具是至关重要的。 命令： vmstat 5 5 表示在5秒时间内进行5次采样。将得到一个数据汇总他能够反映真正的系统情况。 2）显示活跃和非活跃内存 12345678# vmstat -a 2 5procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------ r b swpd free inact active si so bi bo in cs us sy id wa st 0 0 0 3029752 387728 513008 0 0 0 2 3 2 0 0 100 0 0 0 0 0 3029752 387728 513076 0 0 0 0 1005 34 0 0 100 0 0 0 0 0 3029752 387728 513076 0 0 0 22 1004 36 0 0 100 0 0 0 0 0 3029752 387728 513076 0 0 0 0 1004 33 0 0 100 0 0 0 0 0 3029752 387728 513076 0 0 0 0 1003 32 0 0 100 0 0 说明： 使用-a选项显示活跃和非活跃内存时，所显示的内容除增加inact和active外，其他显示内容与例子1相同。 字段说明： Memory（内存）： inact: 非活跃内存大小（当使用-a选项时显示） active: 活跃的内存大小（当使用-a选项时显示） 3）查看系统已经fork了多少次 12# vmstat -f 12744849 forks 说明： 这个数据是从/proc/stat中的processes字段里取得的 4）查看内存使用的详细信息 123456789101112131415161718192021222324252627# vmstat -s 4043760 total memory 1013884 used memory 513012 active memory 387728 inactive memory 3029876 free memory 199616 buffer memory 690980 swap cache 6096656 total swap 0 used swap 6096656 free swap 83587 non-nice user cpu ticks 132 nice user cpu ticks 278599 system cpu ticks 913344692 idle cpu ticks 814550 IO-wait cpu ticks 10547 IRQ cpu ticks 21261 softirq cpu ticks 0 stolen cpu ticks 310215 pages paged in 14254652 pages paged out 0 pages swapped in 0 pages swapped out 288374745 interrupts 146680577 CPU context switches 1351868832 boot time 367291 forks 说明： 这些信息的分别来自于/proc/meminfo,/proc/stat和/proc/vmstat。 5）查看磁盘的读/写 1234567891011121314151617181920212223# vmstat -ddisk- ------------reads------------ ------------writes----------- -----IO------ total merged sectors ms total merged sectors ms cur secram0 0 0 0 0 0 0 0 0 0 0ram1 0 0 0 0 0 0 0 0 0 0ram2 0 0 0 0 0 0 0 0 0 0ram3 0 0 0 0 0 0 0 0 0 0ram4 0 0 0 0 0 0 0 0 0 0ram5 0 0 0 0 0 0 0 0 0 0ram6 0 0 0 0 0 0 0 0 0 0ram7 0 0 0 0 0 0 0 0 0 0ram8 0 0 0 0 0 0 0 0 0 0ram9 0 0 0 0 0 0 0 0 0 0ram10 0 0 0 0 0 0 0 0 0 0ram11 0 0 0 0 0 0 0 0 0 0ram12 0 0 0 0 0 0 0 0 0 0ram13 0 0 0 0 0 0 0 0 0 0ram14 0 0 0 0 0 0 0 0 0 0ram15 0 0 0 0 0 0 0 0 0 0sda 33381 6455 615407 63224 2068111 1495416 28508288 15990289 0 10491hdc 0 0 0 0 0 0 0 0 0 0fd0 0 0 0 0 0 0 0 0 0 0md0 0 0 0 0 0 0 0 0 0 0 这些信息主要来自于/proc/diskstats. merged:表示一次来自于合并的写/读请求,一般系统会把多个连接/邻近的读/写请求合并到一起来操作 6）查看/dev/sda1磁盘的读/写 12345678910# df文件系统 1K-块 已用 可用 已用% 挂载点/dev/sda3 1119336548 27642068 1034835500 3% /tmpfs 32978376 0 32978376 0% /dev/shm/dev/sda1 1032088 59604 920056 7% /boot# vmstat -p /dev/sda1sda1 reads read sectors writes requested writes 18607 4249978 6 48# vmstat -p /dev/sda3sda3 reads read sectors writes requested writes 429350 35176268 28998789 980301488 说明： 这些信息主要来自于/proc/diskstats。 reads:来自于这个分区的读的次数。 read sectors:来自于这个分区的读扇区的次数。 writes:来自于这个分区的写的次数。 requested writes:来自于这个分区的写请求次数。 7）查看系统的slab信息 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143# vmstat -mCache Num Total Size Pagesip_conntrack_expect 0 0 136 28ip_conntrack 3 13 304 13ip_fib_alias 11 59 64 59ip_fib_hash 11 59 64 59AF_VMCI 0 0 960 4bio_map_info 100 105 1064 7dm_mpath 0 0 1064 7jbd_4k 0 0 4096 1dm_uevent 0 0 2608 3dm_tio 0 0 24 144dm_io 0 0 48 77scsi_cmd_cache 10 10 384 10sgpool-128 32 32 4096 1sgpool-64 32 32 2048 2sgpool-32 32 32 1024 4sgpool-16 32 32 512 8sgpool-8 45 45 256 15scsi_io_context 0 0 112 34ext3_inode_cache 51080 51105 760 5ext3_xattr 36 88 88 44journal_handle 18 144 24 144journal_head 56 80 96 40revoke_table 4 202 16 202revoke_record 0 0 32 112uhci_urb_priv 0 0 56 67UNIX 13 33 704 11flow_cache 0 0 128 30msi_cache 33 59 64 59cfq_ioc_pool 14 90 128 30cfq_pool 12 90 216 18crq_pool 16 96 80 48deadline_drq 0 0 80 48as_arq 0 0 96 40mqueue_inode_cache 1 4 896 4isofs_inode_cache 0 0 608 6hugetlbfs_inode_cache 1 7 576 7Cache Num Total Size Pagesext2_inode_cache 0 0 720 5ext2_xattr 0 0 88 44dnotify_cache 0 0 40 92dquot 0 0 256 15eventpoll_pwq 3 53 72 53eventpoll_epi 3 20 192 20inotify_event_cache 0 0 40 92inotify_watch_cache 1 53 72 53kioctx 0 0 320 12kiocb 0 0 256 15fasync_cache 0 0 24 144shmem_inode_cache 254 290 768 5posix_timers_cache 0 0 128 30uid_cache 0 0 128 30ip_mrt_cache 0 0 128 30tcp_bind_bucket 3 112 32 112inet_peer_cache 0 0 128 30secpath_cache 0 0 64 59xfrm_dst_cache 0 0 384 10ip_dst_cache 5 10 384 10arp_cache 1 15 256 15RAW 3 5 768 5UDP 5 10 768 5tw_sock_TCP 0 0 192 20request_sock_TCP 0 0 128 30TCP 4 5 1600 5blkdev_ioc 14 118 64 59blkdev_queue 20 30 1576 5blkdev_requests 13 42 272 14biovec-256 7 7 4096 1biovec-128 7 8 2048 2biovec-64 7 8 1024 4biovec-16 7 15 256 15biovec-4 7 59 64 59biovec-1 23 202 16 202bio 270 270 128 30utrace_engine_cache 0 0 64 59Cache Num Total Size Pagesutrace_cache 0 0 64 59sock_inode_cache 33 48 640 6skbuff_fclone_cache 7 7 512 7skbuff_head_cache 319 390 256 15file_lock_cache 1 22 176 22Acpi-Operand 4136 4248 64 59Acpi-ParseExt 0 0 64 59Acpi-Parse 0 0 40 92Acpi-State 0 0 80 48Acpi-Namespace 2871 2912 32 112delayacct_cache 81 295 64 59taskstats_cache 4 53 72 53proc_inode_cache 1427 1440 592 6sigqueue 0 0 160 24radix_tree_node 13166 13188 536 7bdev_cache 23 24 832 4sysfs_dir_cache 5370 5412 88 44mnt_cache 26 30 256 15inode_cache 2009 2009 560 7dentry_cache 60952 61020 216 18filp 479 1305 256 15names_cache 3 3 4096 1avc_node 14 53 72 53selinux_inode_security 994 1200 80 48key_jar 2 20 192 20idr_layer_cache 74 77 528 7buffer_head 164045 164800 96 40mm_struct 51 56 896 4vm_area_struct 1142 1958 176 22fs_cache 35 177 64 59files_cache 36 55 768 5signal_cache 72 162 832 9sighand_cache 68 84 2112 3task_struct 76 80 1888 2anon_vma 458 864 24 144pid 83 295 64 59shared_policy_node 0 0 48 77Cache Num Total Size Pagesnuma_policy 37 144 24 144size-131072(DMA) 0 0 131072 1size-131072 0 0 131072 1size-65536(DMA) 0 0 65536 1size-65536 1 1 65536 1size-32768(DMA) 0 0 32768 1size-32768 2 2 32768 1size-16384(DMA) 0 0 16384 1size-16384 5 5 16384 1size-8192(DMA) 0 0 8192 1size-8192 7 7 8192 1size-4096(DMA) 0 0 4096 1size-4096 110 111 4096 1size-2048(DMA) 0 0 2048 2size-2048 602 602 2048 2size-1024(DMA) 0 0 1024 4size-1024 344 352 1024 4size-512(DMA) 0 0 512 8size-512 433 480 512 8size-256(DMA) 0 0 256 15size-256 1139 1155 256 15size-128(DMA) 0 0 128 30size-64(DMA) 0 0 64 59size-64 5639 5782 64 59size-32(DMA) 0 0 32 112size-128 801 930 128 30size-32 3005 3024 32 112kmem_cache 137 137 2688 1 这组信息来自于/proc/slabinfo。 slab:由于内核会有许多小对象，这些对象构造销毁十分频繁，比如i-node，dentry，这些对象如果每次构建的时候就向内存要一个页(4kb)，而其实只有几个字节，这样就会非常浪费，为了解决这个问题，就引入了一种新的机制来处理在同一个页框中如何分配小存储区，而slab可以对小对象进行分配,这样就不用为每一个对象分配页框，从而节省了空间，内核对一些小对象创建析构很频繁，slab对这些小对象进行缓冲,可以重复利用,减少内存分配次数。 参考链接： http://www.cnblogs.com/peida/archive/2012/12/25/2833108.html http://man.linuxde.net/vmstat]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-free命令]]></title>
    <url>%2F2018%2F10%2F08%2FLinux%E5%91%BD%E4%BB%A4-free%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[free命令可以显示Linux系统中空闲的、已用的物理内存及swap内存,及被内核使用的buffer。在Linux系统监控的工具中，free命令是最经常使用的命令之一。 语法free(选项) 选项12345678-b 以Byte为单位显示内存使用情况。 -k 以KB为单位显示内存使用情况。 -m 以MB为单位显示内存使用情况。-g 以GB为单位显示内存使用情况。 -o 不显示缓冲区调节列。 -s&lt;间隔秒数&gt; 持续观察内存使用状况。 -t 显示内存总和列。 -V 显示版本信息。 功能free 命令显示系统使用和空闲的内存情况，包括物理内存、交互区内存(swap)和内核缓冲区内存。共享内存将被忽略 常用实例1）显示内存使用情况 123456789101112131415# free total used free shared buffers cachedMem: 32940112 30841684 2098428 0 4545340 11363424-/+ buffers/cache: 14932920 18007192Swap: 32764556 1944984 30819572# free -g total used free shared buffers cachedMem: 31 29 2 0 4 10-/+ buffers/cache: 14 17Swap: 31 1 29# free -m total used free shared buffers cachedMem: 32168 30119 2048 0 4438 11097-/+ buffers/cache: 14583 17584Swap: 31996 1899 30097 说明： 下面是对这些数值的解释： total：总计物理内存的大小。 used：已使用多大。 free：可用有多少。 Shared：多个进程共享的内存总额。 Buffers/cached：磁盘缓存的大小。 第三行（-/+ buffers/cached）： used：已使用多大。 free：可用有多少。 第四行是交换分区SWAP的，也就是他们我们通常所说的虚拟内存。 区别：第二行（mem）的used/free与第三行（-/+ buffers/cache）used/free的区别。这两个的区别在于使用的角度来看，第一行是从OS的角度来看，因为对于OS，buffers/cached 都是属于被使用，所以他的可用内存是2098428KB,已用内存是30841684KB,其中包括，内核（OS）使用+Application(X, oracle,etc)使用的+buffers+cached. 第三行所指的是从应用程序角度来看，对于应用程序来说，buffers/cached 是等于可用的，因为buffer/cached是为了提高文件读取的性能，当应用程序需在用到内存的时候，buffer/cached会很快地被回收。 所以从应用程序的角度来说，可用内存=系统free memory+buffers+cached。 如本机情况的可用内存为： 18007156=2098428KB+4545340KB+11363424KB 接下来解释什么时候内存会被交换，以及按什么方交换。 当可用内存少于额定值的时候，就会开会进行交换.如何看额定值： 12345678910111213141516171819202122232425262728# cat /proc/meminfoMemTotal: 32940112 kBMemFree: 2096700 kBBuffers: 4545340 kBCached: 11364056 kBSwapCached: 1896080 kBActive: 22739776 kBInactive: 7427836 kBHighTotal: 0 kBHighFree: 0 kBLowTotal: 32940112 kBLowFree: 2096700 kBSwapTotal: 32764556 kBSwapFree: 30819572 kBDirty: 164 kBWriteback: 0 kBAnonPages: 14153592 kBMapped: 20748 kBSlab: 590232 kBPageTables: 34200 kBNFS_Unstable: 0 kBBounce: 0 kBCommitLimit: 49234612 kBCommitted_AS: 23247544 kBVmallocTotal: 34359738367 kBVmallocUsed: 278840 kBVmallocChunk: 34359459371 kBHugePages_Total: 0HugePages_Free: 0HugePages_Rsvd: 0Hugepagesize: 交换将通过三个途径来减少系统中使用的物理页面的个数： 1.减少缓冲与页面cache的大小， 2.将系统V类型的内存页面交换出去， 3.换出或者丢弃页面。(Application 占用的内存页，也就是物理内存不足）。 事实上，少量地使用swap是不是影响到系统性能的。 那buffers和cached都是缓存，两者有什么区别呢？ 为了提高磁盘存取效率, Linux做了一些精心的设计, 除了对dentry进行缓存(用于VFS,加速文件路径名到inode的转换), 还采取了两种主要Cache方式：Buffer Cache和Page Cache。前者针对磁盘块的读写，后者针对文件inode的读写。这些Cache有效缩短了 I/O系统调用(比如read,write,getdents)的时间。 磁盘的操作有逻辑级（文件系统）和物理级（磁盘块），这两种Cache就是分别缓存逻辑和物理级数据的。 Page cache实际上是针对文件系统的，是文件的缓存，在文件层面上的数据会缓存到page cache。文件的逻辑层需要映射到实际的物理磁盘，这种映射关系由文件系统来完成。当page cache的数据需要刷新时，page cache中的数据交给buffer cache，因为Buffer Cache就是缓存磁盘块的。但是这种处理在2.6版本的内核之后就变的很简单了，没有真正意义上的cache操作。 Buffer cache是针对磁盘块的缓存，也就是在没有文件系统的情况下，直接对磁盘进行操作的数据会缓存到buffer cache中，例如，文件系统的元数据都会缓存到buffer cache中。 简单说来，page cache用来缓存文件数据，buffer cache用来缓存磁盘数据。在有文件系统的情况下，对文件操作，那么数据会缓存到page cache，如果直接采用dd等工具对磁盘进行读写，那么数据会缓存到buffer cache。 所以我们看linux,只要不用swap的交换空间,就不用担心自己的内存太少.如果常常swap用很多,可能你就要考虑加物理内存了.这也是linux看内存是否够用的标准. 如果是应用服务器的话，一般只看第二行，+buffers/cache,即对应用程序来说free的内存太少了，也是该考虑优化程序或加内存了。 2）以总和的形式显示内存的使用信息 12345# free -t total used free shared buffers cachedMem: 32940112 30845024 2095088 0 4545340 11364324-/+ buffers/cache: 14935360 18004752Swap: 32764556 1944984 30819572Total: 65704668 32790008 32914660 3）周期性的查询内存使用信息 1234567# free -s 10 total used free shared buffers cachedMem: 32940112 30844528 2095584 0 4545340 11364380-/+ buffers/cache: 14934808 18005304Swap: 32764556 1944984 30819572 total used free shared buffers cachedMem: 32940112 30843932 2096180 0 4545340 11364388-/+ buffers/cache: 14934204 18005908Swap: 32764556 1944984 30819572 说明： 每10s 执行一次命令 转载链接： http://www.cnblogs.com/peida/archive/2012/12/25/2831814.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-top命令]]></title>
    <url>%2F2018%2F10%2F08%2FLinux%E5%91%BD%E4%BB%A4-top%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[top命令是Linux下常用的性能分析工具，能够实时显示系统中各个进程的资源占用状况，类似于Windows的任务管理器。下面详细介绍它的使用方法。top是一个动态显示过程,即可以通过用户按键来不断刷新当前状态.如果在前台执行该命令,它将独占前台,直到用户终止该程序为止.比较准确的说,top命令提供了实时的对系统处理器的状态监视.它将显示系统中CPU最“敏感”的任务列表.该命令可以按CPU使用.内存使用和执行时间对任务进行排序；而且该命令的很多特性都可以通过交互式命令或者在个人定制文件中进行设定. 语法top(选项) 选项12345678910-b：以批处理模式操作；-c：显示完整的治命令；-d：屏幕刷新间隔时间；-I：忽略失效过程；-s：保密模式；-S：累积模式；-i&lt;时间&gt;：设置间隔时间；-u&lt;用户名&gt;：指定用户名；-p&lt;进程号&gt;：指定进程；-n&lt;次数&gt;：循环显示的次数。 功能显示当前系统正在执行的进程的相关信息，包括进程ID、内存占用率、CPU占用率等 常用实例1）显示进程信息 1234567891011# toptop - 14:06:23 up 70 days, 16:44, 2 users, load average: 1.25, 1.32, 1.35Tasks: 206 total, 1 running, 205 sleeping, 0 stopped, 0 zombieCpu(s): 5.9%us, 3.4%sy, 0.0%ni, 90.4%id, 0.0%wa, 0.0%hi, 0.2%si, 0.0%stMem: 32949016k total, 14411180k used, 18537836k free, 169884k buffersSwap: 32764556k total, 0k used, 32764556k free, 3612636k cached PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 28894 root 22 0 1501m 405m 10m S 52.2 1.3 2534:16 java 18249 root 18 0 3201m 1.9g 11m S 35.9 6.0 569:39.41 java 2808 root 25 0 3333m 1.0g 11m S 24.3 3.1 526:51.85 java 25668 root 23 0 3180m 704m 11m S 14.0 2.2 360:44.53 java 说明： 统计信息区： 前五行是当前系统情况整体的统计信息区。下面我们看每一行信息的具体意义。 第一行，任务队列信息，同 uptime 命令的执行结果，具体参数说明情况如下： 14:06:23 — 当前系统时间 up 70 days, 16:44 — 系统已经运行了70天16小时44分钟（在这期间系统没有重启过的吆！） 2 users — 当前有2个用户登录系统 load average: 1.15, 1.42, 1.44 — load average后面的三个数分别是1分钟、5分钟、15分钟的负载情况。 load average数据是每隔5秒钟检查一次活跃的进程数，然后按特定算法计算出的数值。如果这个数除以逻辑CPU的数量，结果高于5的时候就表明系统在超负荷运转了。 第二行，Tasks — 任务（进程），具体信息说明如下： 系统现在共有206个进程，其中处于运行中的有1个，205个在休眠（sleep），stoped状态的有0个，zombie状态（僵尸）的有0个。 第三行，cpu状态信息，具体属性说明如下： 5.9%us — 用户空间占用CPU的百分比。 3.4% sy — 内核空间占用CPU的百分比。 0.0% ni — 改变过优先级的进程占用CPU的百分比 90.4% id — 空闲CPU百分比 0.0% wa — IO等待占用CPU的百分比 0.0% hi — 硬中断（Hardware IRQ）占用CPU的百分比 0.2% si — 软中断（Software Interrupts）占用CPU的百分比 第五行，swap交换分区信息，具体信息说明如下： 32764556k total — 交换区总量（32GB） 0k used — 使用的交换区总量（0K） 32764556k free — 空闲交换区总量（32GB） 3612636k cached — 缓冲的交换区总量（3.6GB） 备注： 第四行中使用中的内存总量（used）指的是现在系统内核控制的内存数，空闲内存总量（free）是内核还未纳入其管控范围的数量。纳入内核管理的内存不见得都在使用中，还包括过去使用过的现在可以被重复利用的内存，内核并不把这些可被重新使用的内存交还到free中去，因此在linux上free内存会越来越少，但不用为此担心。 如果出于习惯去计算可用内存数，这里有个近似的计算公式：第四行的free + 第四行的buffers + 第五行的cached，按这个公式此台服务器的可用内存：18537836k +169884k +3612636k = 22GB左右。 对于内存监控，在top里我们要时刻监控第五行swap交换分区的used，如果这个数值在不断的变化，说明内核在不断进行内存和swap的数据交换，这是真正的内存不够用了。 第六行，空行。 第七行以下：各进程（任务）的状态监控，项目列信息说明如下： PID — 进程id USER — 进程所有者 PR — 进程优先级 NI — nice值。负值表示高优先级，正值表示低优先级 VIRT — 进程使用的虚拟内存总量，单位kb。VIRT=SWAP+RES RES — 进程使用的、未被换出的物理内存大小，单位kb。RES=CODE+DATA SHR — 共享内存大小，单位kb S — 进程状态。D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程 %CPU — 上次更新到现在的CPU时间占用百分比 %MEM — 进程使用的物理内存百分比 TIME+ — 进程使用的CPU时间总计，单位1/100秒 COMMAND — 进程名称（命令名/命令行） 其他使用技巧： 1.多U多核CPU监控 在top基本视图中，按键盘数字“1”，可监控每个逻辑CPU的状况： 观察上图，服务器有16个逻辑CPU，实际上是4个物理CPU。再按数字键1，就会返回到top基本视图界面。 2.高亮显示当前运行进程 敲击键盘“b”（打开/关闭加亮效果），top的视图变化如下： 我们发现进程id为2570的“top”进程被加亮了，top进程就是视图第二行显示的唯一的运行态（runing）的那个进程，可以通过敲击“y”键关闭或打开运行态进程的加亮效果。 3.进程字段排序 默认进入top时，各进程是按照CPU的占用量来排序的，在下图中进程ID为28894的java进程排在第一（cpu占用142%），进程ID为574的java进程排在第二（cpu占用16%）。 敲击键盘“x”（打开/关闭排序列的加亮效果），top的视图变化如下： 可以看到，top默认的排序列是“%CPU”。 通过”shift + &gt;”或”shift + &lt;”可以向右或左改变排序列 1下图是按一次”shift + &gt;”的效果图,视图现在已经按照%MEM来排序。 2）显示完整命令 1top -c 3）以批处理模式显示程序信息 1top -b 4）以累积模式显示程序信息 1top -S 5）设置信息更新次数 1top -n 2 6）设置信息更新时间 1top -d 3 说明： 表示更新周期为3秒 7）显示指定的进程信息 1top -p 574 top交互命令在top 命令执行过程中可以使用的一些交互命令。这些命令都是单字母的，如果在命令行中使用了s 选项， 其中一些命令可能会被屏蔽。 h 显示帮助画面，给出一些简短的命令总结说明 k 终止一个进程。 i 忽略闲置和僵死进程。这是一个开关式命令。 q 退出程序 r 重新安排一个进程的优先级别 S 切换到累计模式 s 改变两次刷新之间的延迟时间（单位为s），如果有小数，就换算成m s。输入0值则系统将不断刷新，默认值是5 s f或者F 从当前显示中添加或者删除项目 o或者O 改变显示项目的顺序 l 切换显示平均负载和启动时间信息 m 切换显示内存信息 t 切换显示进程和CPU状态信息 c 切换显示命令名称和完整命令行 M 根据驻留内存大小进行排序 P 根据CPU使用百分比大小进行排序 T 根据时间/累计时间进行排序 W 将当前设置写入~/.toprc文件中 参考链接： http://www.cnblogs.com/peida/archive/2012/12/24/2831353.html http://man.linuxde.net/top]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高并发下map和chan实现的链接池的线程安全及效率]]></title>
    <url>%2F2018%2F10%2F05%2F%E9%AB%98%E5%B9%B6%E5%8F%91%E4%B8%8Bmap%E5%92%8Cchan%E5%AE%9E%E7%8E%B0%E7%9A%84%E9%93%BE%E6%8E%A5%E6%B1%A0%E7%9A%84%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E5%8F%8A%E6%95%88%E7%8E%87%2F</url>
    <content type="text"><![CDATA[1.背景上一次blog写着写着崩掉了，这次一定写完一节保存一节。 目前从事go语言的后台开发，在集群通信时需要用到thrift的rpc。由于集群间通信非常频繁且并发需求很高，所以只能采用连接池的形式。由于集群规模是有限的，每个节点都需要保存平行节点的连接，所以链接池的实现方式应该是map[host]chan conn。在go语言中，我们知道channel是线程安全的，但map却不是线程安全的。所以我们需要适当的加锁来保证其线程安全同时兼顾效率。 2. 链接池的一般设计 新建链接时，需要给定目标地址 获取链接时，需要给定目标地址，若链接池存在链接，则从链接池返回，若不存在链接，则新建链接返回 链接池中存放的链接总是空闲链接 连接使用完后需放回链接池 放回链接池需要给定目标地址 放回链接池时若链接池已满，则关闭该链接并将其交给gc 3.链接池的一般定义123456789type factory func(host string) conntype conn interface &#123; Close() error&#125;type pool struct &#123; m map[string]chan conn mu sync.RWMutex fact factory&#125; 以上是一个通用链接池的实现，用了channel和读写锁，暂时不考虑链接超时等问题。我们的目的是探索这个链接池在高并发情况下的线程安全和get，put效率问题。所以下来我们给出实验主程序。 4.测试主程序测试主程序如下所示 pt是记录的get，put次数，采用原子操作进行累加，其耗时忽略不计。 hosts为集群规模，也是map的大小，一般来说不会太大。 threadnum为并发的协程数 为了方便，此处直接使用net.dial作为工厂方法实现 每一个协程是一个死循环，不断地进行get，put操作。每次操作会使pts加1。 12345678910111213141516171819202122232425262728293031323334353637383940414243func main()&#123; var pts uint64 p := &amp;pool&#123; m :make(map[string]chan conn), mu:sync.RWMutex&#123;&#125;, fact:func(target string)conn&#123; c,_ :=net.Dial("","8080") return c &#125;, &#125; //打印线程，打印get，put效率 be := time.Now() go func ()&#123; for true&#123; //此处先休眠一秒是为了避免第一次时差计算为0导致的除法错误 time.Sleep(1 *time.Second) cost := time.Since(be) / time.Second println(atomic.LoadUint64(&amp;pts)/uint64(cost),"pt/s") &#125; &#125;() time.Sleep(1*time.Second) //打印线程完，此处等待一秒是为对应打印线程第一次休眠，尽量减少误差 //集群规模 hosts := []string&#123;"192.168.0.1","192.168.0.2","192.168.0.3","192.168.0.4"&#125; //并发线程数量 threadnum := 1 for i:=0;i&lt;threadnum;i++&#123; go func()&#123; for true&#123; target := hosts[rand.Int() % len(hosts)] conn := p.Get(target) //------------------使用连接开始 //time.Sleep(1*time.Nanosecond) //------------------使用连接完毕 p.Put(target,conn) atomic.AddUint64(&amp;pts,1) &#125; &#125;() &#125; time.Sleep(100 * time.Second)&#125; 5.单协程情况下的效率5.1 单协程get &amp; put实现单协程模式下，我们不必考虑线程安全的问题，也就不必加锁。此时的get，put实现如下 1234567891011121314151617181920212223func (p *pool)Get(host string) (c conn)&#123; if _,ok := p.m[host];!ok&#123; p.m[host] = make(chan conn,100) &#125; select &#123; case c = &lt;- p.m[host]: &#123;&#125; default: c = p.New(host) &#125; return&#125;func (p *pool)Put(host string,c conn)&#123; select &#123; case p.m[host] &lt;- c: &#123;&#125; default: c.Close() &#125;&#125;func (p *pool)New(host string)conn&#123; return p.fact(host)&#125; 5.2 测试结果我们设置threadnum为1，测试结果如下。其速度大概在5,000,000 次/秒 6.并发情况下效率-全写锁6.1 全写锁的get &amp; put 实现为了保证并发情况下的线程安全，我们需要使用读写锁，那么对get和put操作究竟该如何加锁呢，最安全的形式当然是全写锁的形式，单其效率肯定是最低的，因为这样同一时刻总是只有一个协程在进行写或者读。 123456789101112131415161718192021222324func (p *pool)Get1(host string) (c conn)&#123; p.mu.Lock() defer p.mu.RLock() if _,ok := p.m[host];!ok&#123; p.m[host] = make(chan conn,100) &#125; select &#123; case c = &lt;- p.m[host]: &#123;&#125; default: c = p.New(host) &#125; return&#125;func (p *pool)Put1(host string,c conn)&#123; p.mu.Lock() defer p.mu.Unlock() select &#123; case p.m[host] &lt;- c: &#123;&#125; default: c.Close() &#125;&#125; 6.2 测试结果6.2.1 全写锁下 的多协程测试结果我们设置threadnum为4，测试结果如下，其速度大概在1,000,000次/秒 6.2.2 全写锁下单协程测试结果如果我们将threadnum设置为1，再次测试，其速度为2,800,00次/秒。可以看到，多协程会降低效率，因为协程间切换也会有时间消耗。但我们经常听说多协程会提高运行速度，这也是对的，那么什么时候多协程会提高运速度呢，这就是我说的链接使用时间的问题，当连接使用时间大于锁竞争和协程切换时间的时候，我们用多协程会提高效率。而实际使用中，连接的使用时间总是存在的且一般都大于锁竞争时间和协程切换时间。 6.2.3 单协程下存在链接使用时间的的测试结果在主程序中，我们在get和put间加上休眠时间，此处设置休眠时间为1毫秒即链接使用1毫秒后放链接池。同时协程数设置为1。单协程情况下，其速度大概如下500次/秒。可以看到实际的效率大幅度降低。 6.2.4 多协程下存在链接使用时间的测试结果同样保持链接使用时间为1毫秒，协程数量设置为4，测试结果如下。其速度大概为2,000次/秒，刚好是单协程的4倍。所以实际情况下多协程的使用需要慎重考虑，并不是多协程一定能提高程序的处理速度，相反在某些情况下会降低程序的执行速度。由于本次测试的是链接池的性能和安全，接下来的测试不再添加链接使用时间，只单纯的测试读写锁和效率的问题。本小节算是一个附加测试。 7.并发情况下效率-读写锁1由于全写锁没有实际的使用意义，所以我们需要使用读写锁来提高效率，那么如何保证线程安全添加读写锁呢。首先对于我们的map结构来说，当有写操作的时候，我们的读操作应该是不可靠的，所以不能进行，当读操作时，我们不希望有写操作但其他协程也能同时读取，这桥恰符合读写锁的作用原理。 当加写锁时，所有的读写均不可用 当加读锁时，所有的写操作不可用，读操作可用 7.1 读写锁1 get &amp; put实现考察我们的put程序，只有对map的读，所以只需要加读锁，而在get中，包含了两部分，第一次写操作和第二次的读操作，所以我们很简单的我们想到，需要使用两次锁，第一次写锁，第二次读锁。 123456789101112131415161718192021222324252627func (p *pool)Get2(host string) (c conn)&#123; p.mu.Lock() if _,ok := p.m[host];!ok&#123; p.m[host] = make(chan conn,100) &#125; p.mu.Unlock() p.mu.RLock() defer p.mu.RUnlock() select &#123; case c = &lt;- p.m[host]: &#123;&#125; default: c = p.New(host) &#125; return&#125;func (p *pool)Put1(host string,c conn)&#123; p.mu.RLock() defer p.mu.RUnlock() select &#123; case p.m[host] &lt;- c: &#123;&#125; default: c.Close() &#125;&#125; 7.2 测试结果我们本来期望的是效率应该比全写锁要高一些，但实际情况是低一些，只有800,000次/秒。那问题出在哪里呢。从程序上来看，get多了一次加锁，所以导致锁竞争次数比全写锁要高一些，但我们并不能减少锁次数直接使用读锁，这样是不安全的，程序也会报错。所以我们给出另一种安全的读写锁形式。 8.并发情况下效率-读写锁8.1 读写锁2 get &amp; put实我们从实际的使用来看一下get程序，由于我们给定了hosts，所以其实对map的写入操作只会进行四次，但后来每次进行get时都会加一次写锁，这是没有必要的。仔细看一下第一次写锁，我们加的有些草率，因为首先会读取一次map来判断是否应该进行写入操作，所以我们可以通过增加一次读锁，来减少后来的加写锁。当然有人会说为什么不直接初始化map，这样就没有写操作，这我也考虑过，但是集群规模有可能会扩张并且会动态变化，直接初始化map会显得有些刻意，并且通用性也不强，与其他模块会产生耦合。所以这种做法并没有多少设计上的美感，相反会显得比较low。我们给出第二种读写锁如下。123456789101112131415161718192021222324252627282930func (p *pool)Get3(host string) (c conn)&#123; p.mu.RLock() if _,ok := p.m[host];!ok&#123; p.mu.RUnlock() p.mu.Lock() p.m[host] = make(chan conn,100) p.mu.Unlock() &#125;else&#123; p.mu.RUnlock() &#125; p.mu.RLock() defer p.mu.RUnlock() select &#123; case c = &lt;- p.m[host]: &#123;&#125; default: c = p.New(host) &#125; return&#125;func Put3(host string,c conn)&#123; p.mu.RLock() defer p.mu.RUnlock() select &#123; case p.m[host] &lt;- c: &#123;&#125; default: c.Close() &#125;&#125; 8.2 测试结测试结果如下，其速度大概在3,400,000次/秒。是全写锁性能的4倍左右。 9.defer对锁的性能影响我们经常听说defer的执行效率低，其实是因为defer在函数返回时才执行，这对普通的函数并没有影响，但对所来说，如果我们可以提前释放锁，那么肯定能减少很多锁的无效占用。顺便我们测试一下defer函数对锁的性能影响，对8.1的get &amp; put实现，我们将其中的defer全部替换为函数结束之前手动释放锁。其实只在put中有defer 9.1 无defer的 get &amp; put实现12345678910111213141516171819202122232425262728func (p *pool)Get4(host string) (c conn)&#123; p.mu.RLock() if _,ok := p.m[host];!ok&#123; p.mu.RUnlock() p.mu.Lock() p.m[host] = make(chan conn,100) p.mu.Unlock() &#125;else&#123; p.mu.RUnlock() &#125; select &#123; case c = &lt;- p.m[host]: &#123;&#125; default: c = p.New(host) &#125; return&#125;func (p *pool)Put4(host string,c conn)&#123; p.mu.RLock() select &#123; case p.m[host] &lt;- c: &#123;&#125; default: c.Close() &#125; p.mu.RUnlock()&#125; 9.2 测试结果测试结果如下，仅仅修改了一处defer，速度达到接近4,000,000次/秒，性能提高了15%。还是非常可观的。 10.总结 这篇bolg真不容易，写了我好久 多协程提高程序执行速度是有前提的，并不能无脑提高程序速度 map是非线程安全的，需要谨慎使用 读写锁性能比单纯的写锁(互斥锁)要高很多，尽量使用读写锁 读写锁的使用可以针对具体情况进行优化，还可以使用go race detector来检测是否安全 锁尽量手动释放，当然defer是一种非常优雅的写法，对效率要求不高的程序中我还是喜欢用defer]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>高并发线程安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-wc命令]]></title>
    <url>%2F2018%2F10%2F04%2FLinux%E5%91%BD%E4%BB%A4-wc%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Linux系统中的wc(Word Count)命令的功能为统计指定文件中的字节数、字数、行数，并将统计结果显示输出。 语法wc(选项)(参数) 选项1234567-c 统计字节数。-l 统计行数。-m 统计字符数。这个标志不能与 -c 标志一起使用。-w 统计字数。一个字被定义为由空白、跳格或换行字符分隔的字符串。-L 打印最长行的长度。-help 显示帮助信息--version 显示版本信息 参数文件：需要统计的文件列表。 功能统计指定文件中的字节数、字数、行数，并将统计结果显示输出。该命令统计指定文件中的字节数、字数、行数。如果没有给出文件名，则从标准输入读取。wc同时也给出所指定文件的总统计数。 常用实例1）查看文件的字节数、字数、行数 1234567891011121314151617181920# cat test.txt hnlinuxpeida.cnblogs.comubuntuubuntu linuxredhatRedhatlinuxmint# wc test.txt 7 8 70 test.txt# wc -l test.txt 7 test.txt# wc -c test.txt 70 test.txt# wc -w test.txt 8 test.txt# wc -m test.txt 70 test.txt# wc -L test.txt 17 test.txt 说明： 7 8 70 test.txt 行数 单词数 字节数 文件名 2）用wc命令怎么做到只打印统计数字不打印文件名 1234# wc -l test.txt 7 test.txt# cat test.txt |wc -l7 说明： 使用管道线，这在编写shell脚本时特别有用。 3）用来统计当前目录下的文件数 123456789101112#cd test6# ll总计 604---xr--r-- 1 root mail 302108 11-30 08:39 linklog.log---xr--r-- 1 mail users 302108 11-30 08:39 log2012.log-rw-r--r-- 1 mail users 61 11-30 08:39 log2013.log-rw-r--r-- 1 root mail 0 11-30 08:39 log2014.log-rw-r--r-- 1 root mail 0 11-30 08:39 log2015.log-rw-r--r-- 1 root mail 0 11-30 08:39 log2016.log-rw-r--r-- 1 root mail 0 11-30 08:39 log2017.log# ls -l | wc -l8 说明： 数量中包含当前目录 转载链接： http://www.cnblogs.com/peida/archive/2012/12/18/2822758.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-grep命令]]></title>
    <url>%2F2018%2F10%2F04%2FLinux%E5%91%BD%E4%BB%A4-grep%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[grep（global search regular expression(RE) and print out the line，全面搜索正则表达式并把行打印出来）是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。 grep可用于shell脚本，因为grep通过返回一个状态值来说明搜索的状态，如果模板搜索成功，则返回0，如果搜索不成功，则返回1，如果搜索的文件不存在，则返回2。我们利用这些返回值就可进行一些自动化的文本处理工作。 选项12345678910111213141516171819202122232425-a 不要忽略二进制数据。-A&lt;显示列数&gt; 除了显示符合范本样式的那一行之外，并显示该行之后的内容。-b 在显示符合范本样式的那一行之外，并显示该行之前的内容。-c 计算符合范本样式的列数。-C&lt;显示列数&gt;或-&lt;显示列数&gt; 除了显示符合范本样式的那一列之外，并显示该列之前后的内容。-d&lt;进行动作&gt; 当指定要查找的是目录而非文件时，必须使用这项参数，否则grep命令将回报信息并停止动作。-e&lt;范本样式&gt; 指定字符串作为查找文件内容的范本样式。-E 将范本样式为延伸的普通表示法来使用，意味着使用能使用扩展正则表达式。-f&lt;范本文件&gt; 指定范本文件，其内容有一个或多个范本样式，让grep查找符合范本条件的文件内容，格式为每一列的范本样式。-F 将范本样式视为固定字符串的列表。-G 将范本样式视为普通的表示法来使用。-h 在显示符合范本样式的那一列之前，不标示该列所属的文件名称。-H 在显示符合范本样式的那一列之前，标示该列的文件名称。-i 忽略字符大小写的差别。-l 列出文件内容符合指定的范本样式的文件名称。-L 列出文件内容不符合指定的范本样式的文件名称。-n 在显示符合范本样式的那一列之前，标示出该列的编号。-q 不显示任何信息。-R/-r 此参数的效果和指定“-d recurse”参数相同。-s 不显示错误信息。-v 反转查找。-w 只显示全字符合的列。-x 只显示全列符合的列。-y 此参数效果跟“-i”相同。-o 只输出文件中匹配到的部分。 规则表达式grep的规则表达式: ^ #锚定行的开始 如：’^grep’匹配所有以grep开头的行。 $ #锚定行的结束 如：’grep$’匹配所有以grep结尾的行。 . #匹配一个非换行符的字符 如：’gr.p’匹配gr后接一个任意字符，然后是p。 #匹配零个或多个先前字符 如：’*grep’匹配所有一个或多个空格后紧跟grep的行。 .* #一起用代表任意字符。 [] #匹配一个指定范围内的字符，如’[Gg]rep’匹配Grep和grep。 #匹配一个不在指定范围内的字符，如：’A-FH-Zrep’匹配不包含A-F和H-Z的一个字母开头，紧跟rep的行。 (..) #标记匹配字符，如’(love)‘，love被标记为1。 \&lt; #锚定单词的开始，如:’\&lt;grep’匹配包含以grep开头的单词的行。 > #锚定单词的结束，如’grep&gt;‘匹配包含以grep结尾的单词的行。 x{m} #重复字符x，m次，如：’0{5}‘匹配包含5个o的行。 x{m,} #重复字符x,至少m次，如：’o{5,}‘匹配至少有5个o的行。 x{m,n} #重复字符x，至少m次，不多于n次，如：’o{5,10}‘匹配5–10个o的行。 \w #匹配文字和数字字符，也就是[A-Za-z0-9]，如：’G\w*p’匹配以G后跟零个或多个文字或数字字符，然后是p。 \W #\w的反置形式，匹配一个或多个非单词字符，如点号句号等。 \b #单词锁定符，如: ‘\bgrep\b’只匹配grep。 POSIX字符: 为了在不同国家的字符编码中保持一至，POSIX(The Portable Operating System Interface)增加了特殊的字符类，如[:alnum:]是[A-Za-z0-9]的另一个写法。要把它们放到[]号内才能成为正则表达式，如[A- Za-z0-9]或[[:alnum:]]。在linux下的grep除fgrep外，都支持POSIX的字符类。 [:alnum:] #文字数字字符 [:alpha:] #文字字符 [:digit:] #数字字符 [:graph:] #非空字符（非空格、控制字符） [:lower:] #小写字符 [:cntrl:] #控制字符 [:print:] #非空字符（包括空格） [:punct:] #标点符号 [:space:] #所有空白字符（新行，空格，制表符） [:upper:] #大写字符 [:xdigit:] #十六进制数字（0-9，a-f，A-F） 常用实例1）查找指定进程 123 ps -ef|grep svnroot 4943 1 0 Dec05 ? 00:00:00 svnserve -d -r /opt/svndata/grape/root 16867 16838 0 19:53 pts/0 00:00:00 grep svn 2）查找指定进程个数 12# ps -ef|grep -c svn 2 3）从文件中读取关键词进行搜索 12345678910111213141516# cat test.txt hnlinuxpeida.cnblogs.comubuntuubuntu linuxredhatRedhatlinuxmint# cat test2.txt linuxRedhat# cat test.txt | grep -f test2.txthnlinuxubuntu linuxRedhatlinuxmint 说明： 输出test.txt文件中含有从test2.txt文件中读取出的关键词的内容行 4）从文件中读取关键词进行搜索且显示行号 12345678910111213141516# cat test.txt hnlinuxpeida.cnblogs.comubuntuubuntu linuxredhatRedhatlinuxmint# cat test2.txt linuxRedhat# cat test.txt | grep -nf test2.txt1:hnlinux4:ubuntu linux6:Redhat7:linuxmint 说明： 输出test.txt文件中含有从test2.txt文件中读取出的关键词的内容行，并显示每一行的行号 5）从文件中查找关键词 12345678# grep &apos;linux&apos; test.txt hnlinuxubuntu linuxlinuxmint# grep -n &apos;linux&apos; test.txt 1:hnlinux4:ubuntu linux7:linuxmint 6）从多个文件中查找关键词 12345678910# grep -n &apos;linux&apos; test.txt test2.txt test.txt:1:hnlinuxtest.txt:4:ubuntu linuxtest.txt:7:linuxminttest2.txt:1:linux# grep &apos;linux&apos; test.txt test2.txt test.txt:hnlinuxtest.txt:ubuntu linuxtest.txt:linuxminttest2.txt:linux 说明： 多文件时，输出查询到的信息内容行时，会把文件的命名在行最前面输出并且加上”:”作为标示符 7）grep不显示本身进程 12345678910# ps aux|grep sshroot 2720 0.0 0.0 62656 1212 ? Ss Nov02 0:00 /usr/sbin/sshdroot 16834 0.0 0.0 88088 3288 ? Ss 19:53 0:00 sshd: root@pts/0 root 16901 0.0 0.0 61180 764 pts/0 S+ 20:31 0:00 grep ssh# ps aux|grep [s]shroot 2720 0.0 0.0 62656 1212 ? Ss Nov02 0:00 /usr/sbin/sshdroot 16834 0.0 0.0 88088 3288 ? Ss 19:53 0:00 sshd: root@pts/0 # ps aux | grep ssh | grep -v &quot;grep&quot;root 2720 0.0 0.0 62656 1212 ? Ss Nov02 0:00 /usr/sbin/sshdroot 16834 0.0 0.0 88088 3288 ? Ss 19:53 0:00 sshd: root@pts/0 8）找出已u开头的行内容 123# cat test.txt |grep ^uubuntuubuntu linux 9）输出非u开头的行内容 123456# cat test.txt |grep ^[^u]hnlinuxpeida.cnblogs.comredhatRedhatlinuxmint 10）输出以hat结尾的行内容 123# cat test.txt |grep hat$redhatRedhat 11）查服务器ip地址所在行 1234# ifconfig eth0|grep &quot;[0-9]\&#123;1,3\&#125;\.[0-9]\&#123;1,3\&#125;\.[0-9]\&#123;1,3\&#125;\.[0-9]\&#123;1,3\&#125;&quot; inet addr:192.168.120.204 Bcast:192.168.120.255 Mask:255.255.255.0# ifconfig eth0|grep -E &quot;([0-9]&#123;1,3&#125;\.)&#123;3&#125;[0-9]&quot; inet addr:192.168.120.204 Bcast:192.168.120.255 Mask:255.255.255.0 12）显示包含ed或者at字符的内容行 12345# cat test.txt |grep -E &quot;peida|com&quot;peida.cnblogs.com# cat test.txt |grep -E &quot;ed|at&quot;redhatRedhat 13）显示当前目录下面以.txt 结尾的文件中的所有包含每个字符串至少有7个连续小写字符的字符串的行 1234# grep &apos;[a-z]\&#123;7\&#125;&apos; *.txttest.txt:hnlinuxtest.txt:peida.cnblogs.comtest.txt:linuxmint 14）在多级目录中对文本进行递归搜索 1#grep &quot;text&quot; . -r -n # .表示当前目录。 15）显示过滤注释( # ; 开头) 和空行后的配置信息 1# grep -Ev &quot;^$|^[#;]&quot; server.conf 参考链接： http://man.linuxde.net/grep http://www.cnblogs.com/peida/archive/2012/12/17/2821195.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-kill命令]]></title>
    <url>%2F2018%2F10%2F04%2FLinux%E5%91%BD%E4%BB%A4-kill%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Linux中的kill命令用来终止指定的进程（terminate a process）的运行，是Linux下进程管理的常用命令。通常，终止一个前台进程可以使用Ctrl+C键，但是，对于一个后台进程就须用kill命令来终止，我们就需要先使用ps/pidof/pstree/top等工具获取进程PID，然后使用kill命令来杀掉该进程。kill命令是通过向进程发送指定的信号来结束相应进程的。在默认情况下，采用编号为15的TERM信号。TERM信号将终止所有不能捕获该信号的进程。对于那些可以捕获该信号的进程就要用编号为9的kill信号，强行“杀掉”该进程。 语法kill(选项)(参数) 选项12345-a：当处理当前进程时，不限制命令名和进程号的对应关系；-l &lt;信息编号&gt;：若不加&lt;信息编号&gt;选项，则-l参数会列出全部的信息名称；-p：指定kill 命令只打印相关进程的进程号，而不发送任何信号；-s &lt;信息名称或编号&gt;：指定要送出的信息；-u：指定用户。 参数进程或作业识别号：指定要删除的进程或作业。 功能发送指定的信号到相应进程。不指定型号将发送SIGTERM（15）终止指定进程。如果无法终止该程序可用“-KILL”参数，其发送的信号为SIGKILL（9），将强制结束进程，使用ps命令或者jobs命令可以查看进程号。root用户将影响用户的进程，非root用户只能影响自己的进程。 常用实例1234567891011121314151617# kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL 5) SIGTRAP 6) SIGABRT 7) SIGBUS 8) SIGFPE 9) SIGKILL 10) SIGUSR1 11) SIGSEGV 12) SIGUSR213) SIGPIPE 14) SIGALRM 15) SIGTERM 16) SIGSTKFLT17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGTSTP21) SIGTTIN 22) SIGTTOU 23) SIGURG 24) SIGXCPU25) SIGXFSZ 26) SIGVTALRM 27) SIGPROF 28) SIGWINCH29) SIGIO 30) SIGPWR 31) SIGSYS 34) SIGRTMIN35) SIGRTMIN+1 36) SIGRTMIN+2 37) SIGRTMIN+3 38) SIGRTMIN+439) SIGRTMIN+5 40) SIGRTMIN+6 41) SIGRTMIN+7 42) SIGRTMIN+843) SIGRTMIN+9 44) SIGRTMIN+10 45) SIGRTMIN+11 46) SIGRTMIN+1247) SIGRTMIN+13 48) SIGRTMIN+14 49) SIGRTMIN+15 50) SIGRTMAX-1451) SIGRTMAX-13 52) SIGRTMAX-12 53) SIGRTMAX-11 54) SIGRTMAX-1055) SIGRTMAX-9 56) SIGRTMAX-8 57) SIGRTMAX-7 58) SIGRTMAX-659) SIGRTMAX-5 60) SIGRTMAX-4 61) SIGRTMAX-3 62) SIGRTMAX-263) SIGRTMAX-1 64) SIGRTMAX 说明： 只有第9种信号(SIGKILL)才可以无条件终止进程，其他信号进程都有权利忽略。 下面是常用的信号： HUP 1 终端断线 INT 2 中断（同 Ctrl + C） QUIT 3 退出（同 Ctrl + \） TERM 15 终止 KILL 9 强制终止 CONT 18 继续（与STOP相反， fg/bg命令） STOP 19 暂停（同 Ctrl + Z） 2）得到指定信号的数值 12345678# kill -l KILL9# kill -l SIGKILL9# kill -l TERM15# kill -l SIGTERM15 3）先用ps查找进程，然后用kill杀掉 123456#ps -ef|grep vim root 3268 2884 0 16:21 pts/1 00:00:00 vim install.logroot 3370 2822 0 16:21 pts/0 00:00:00 grep vim# kill 3268 # kill 3268 -bash: kill: (3268) - 没有那个进程 4）彻底杀死进程 123456# ps -ef|grep vim root 3268 2884 0 16:21 pts/1 00:00:00 vim install.logroot 3370 2822 0 16:21 pts/0 00:00:00 grep vim# kill –9 3268 # kill 3268 -bash: kill: (3268) - 没有那个进程 5）杀死指定用户所有进程 12# kill -9 $(ps -ef | grep peidalinux) # kill -u peidalinux 6）init进程是不可杀的 123456789101112# ps -ef|grep initroot 1 0 0 Nov02 ? 00:00:00 init [3] root 17563 17534 0 17:37 pts/1 00:00:00 grep init# kill -9 1# kill -HUP 1# ps -ef|grep initroot 1 0 0 Nov02 ? 00:00:00 init [3] root 17565 17534 0 17:38 pts/1 00:00:00 grep init# kill -KILL 1# ps -ef|grep initroot 1 0 0 Nov02 ? 00:00:00 init [3] root 17567 17534 0 17:38 pts/1 00:00:00 grep init 说明： init是Linux系统操作中不可缺少的程序之一。所谓的init进程，它是一个由内核启动的用户级进程。内核自行启动（已经被载入内存，开始运行，并已初始化所有的设备驱动程序和数据结构等）之后，就通过启动一个用户级程序init的方式，完成引导进程。所以,init始终是第一个进程（其进程编号始终为1）。 其它所有进程都是init进程的子孙。init进程是不可杀的！ 参考链接： http://www.cnblogs.com/peida/archive/2012/12/20/2825837.html http://man.linuxde.net/kill]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-ps命令]]></title>
    <url>%2F2018%2F10%2F04%2FLinux%E5%91%BD%E4%BB%A4-ps%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[ps命令用于报告当前系统的进程状态。可以搭配kill指令随时中断、删除不必要的程序。ps命令是最基本同时也是非常强大的进程查看命令，使用该命令可以确定有哪些进程正在运行和运行的状态、进程是否结束、进程有没有僵死、哪些进程占用了过多的资源等等，总之大部分信息都是可以通过执行该命令得到的。 linux上进程有5种状态: 运行(正在运行或在运行队列中等待) 中断(休眠中, 受阻, 在等待某个条件的形成或接受到信号) 不可中断(收到信号不唤醒和不可运行, 进程必须等待直到有中断发生) 僵死(进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放) 停止(进程收到SIGSTOP, SIGSTP, SIGTIN, SIGTOU信号后停止运行运行) ps工具标识进程的5种状态码: D 不可中断 uninterruptible sleep （usually IO） R 运行 runnable （on run queue） S 中断 sleeping T 停止 traced or stopped Z 僵死 a defunct （“zombie”） process 语法ps(选项) 选项1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162-a：显示所有终端机下执行的程序，除了阶段作业领导者之外。a：显示现行终端机下的所有程序，包括其他用户的程序。-A：显示所有程序。-c：显示CLS和PRI栏位。c：列出程序时，显示每个程序真正的指令名称，而不包含路径，选项或常驻服务的标示。-C&lt;指令名称&gt;：指定执行指令的名称，并列出该指令的程序的状况。-d：显示所有程序，但不包括阶段作业领导者的程序。-e：此选项的效果和指定&quot;A&quot;选项相同。e：列出程序时，显示每个程序所使用的环境变量。-f：显示UID,PPIP,C与STIME栏位。f：用ASCII字符显示树状结构，表达程序间的相互关系。-g&lt;群组名称&gt;：此选项的效果和指定&quot;-G&quot;选项相同，当亦能使用阶段作业领导者的名称来指定。g：显示现行终端机下的所有程序，包括群组领导者的程序。-G&lt;群组识别码&gt;：列出属于该群组的程序的状况，也可使用群组名称来指定。h：不显示标题列。-H：显示树状结构，表示程序间的相互关系。-j或j：采用工作控制的格式显示程序状况。-l或l：采用详细的格式来显示程序状况。L：列出栏位的相关信息。-m或m：显示所有的执行绪。n：以数字来表示USER和WCHAN栏位。-N：显示所有的程序，除了执行ps指令终端机下的程序之外。-p&lt;程序识别码&gt;：指定程序识别码，并列出该程序的状况。p&lt;程序识别码&gt;：此选项的效果和指定&quot;-p&quot;选项相同，只在列表格式方面稍有差异。r：只列出现行终端机正在执行中的程序。-s&lt;阶段作业&gt;：指定阶段作业的程序识别码，并列出隶属该阶段作业的程序的状况。s：采用程序信号的格式显示程序状况。S：列出程序时，包括已中断的子程序资料。-t&lt;终端机编号&gt;：指定终端机编号，并列出属于该终端机的程序的状况。t&lt;终端机编号&gt;：此选项的效果和指定&quot;-t&quot;选项相同，只在列表格式方面稍有差异。-T：显示现行终端机下的所有程序。-u&lt;用户识别码&gt;：此选项的效果和指定&quot;-U&quot;选项相同。u：以用户为主的格式来显示程序状况。-U&lt;用户识别码&gt;：列出属于该用户的程序的状况，也可使用用户名称来指定。U&lt;用户名称&gt;：列出属于该用户的程序的状况。v：采用虚拟内存的格式显示程序状况。-V或V：显示版本信息。-w或w：采用宽阔的格式来显示程序状况。 x：显示所有程序，不以终端机来区分。X：采用旧式的Linux i386登陆格式显示程序状况。-y：配合选项&quot;-l&quot;使用时，不显示F(flag)栏位，并以RSS栏位取代ADDR栏位 。-&lt;程序识别码&gt;：此选项的效果和指定&quot;p&quot;选项相同。--cols&lt;每列字符数&gt;：设置每列的最大字符数。--columns&lt;每列字符数&gt;：此选项的效果和指定&quot;--cols&quot;选项相同。--cumulative：此选项的效果和指定&quot;S&quot;选项相同。--deselect：此选项的效果和指定&quot;-N&quot;选项相同。--forest：此选项的效果和指定&quot;f&quot;选项相同。--headers：重复显示标题列。--help：在线帮助。--info：显示排错信息。--lines&lt;显示列数&gt;：设置显示画面的列数。--no-headers：此选项的效果和指定&quot;h&quot;选项相同，只在列表格式方面稍有差异。--group&lt;群组名称&gt;：此选项的效果和指定&quot;-G&quot;选项相同。--Group&lt;群组识别码&gt;：此选项的效果和指定&quot;-G&quot;选项相同。--pid&lt;程序识别码&gt;：此选项的效果和指定&quot;-p&quot;选项相同。--rows&lt;显示列数&gt;：此选项的效果和指定&quot;--lines&quot;选项相同。--sid&lt;阶段作业&gt;：此选项的效果和指定&quot;-s&quot;选项相同。--tty&lt;终端机编号&gt;：此选项的效果和指定&quot;-t&quot;选项相同。--user&lt;用户名称&gt;：此选项的效果和指定&quot;-U&quot;选项相同。--User&lt;用户识别码&gt;：此选项的效果和指定&quot;-U&quot;选项相同。--version：此选项的效果和指定&quot;-V&quot;选项相同。--widty&lt;每列字符数&gt;：此选项的效果和指定&quot;-cols&quot;选项相同。 常用范例1）显示所有进程信息 123456# ps -A PID TTY TIME CMD 1 ? 00:00:00 init 2 ? 00:00:01 migration/0 3 ? 00:00:00 ksoftirqd/0 4 ? 00:00:01 migration/1 2）显示指定用户信息 123456# ps -u root PID TTY TIME CMD 1 ? 00:00:00 init 2 ? 00:00:01 migration/0 3 ? 00:00:00 ksoftirqd/0 4 ? 00:00:01 migration/1 3）显示所有进程信息，连同命令行 1234567ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 Nov02 ? 00:00:00 init [3] root 2 1 0 Nov02 ? 00:00:01 [migration/0]root 3 1 0 Nov02 ? 00:00:00 [ksoftirqd/0]root 4 1 0 Nov02 ? 00:00:01 [migration/1]root 5 1 0 Nov02 ? 00:00:00 [ksoftirqd/1] 4） ps 与grep 常用组合用法，查找特定进程 1234# ps -ef|grep sshroot 2720 1 0 Nov02 ? 00:00:00 /usr/sbin/sshdroot 17394 2720 0 14:58 ? 00:00:00 sshd: root@pts/0 root 17465 17398 0 15:57 pts/0 00:00:00 grep ssh 5）将目前属于您自己这次登入的 PID 与相关信息列示出来 1234# ps -lF S UID PID PPID C PRI NI ADDR SZ WCHAN TTY TIME CMD4 S 0 17398 17394 0 75 0 - 16543 wait pts/0 00:00:00 bash4 R 0 17469 17398 0 77 0 - 15877 - pts/0 00:00:00 ps 说明： 各相关信息的意义： F 代表这个程序的旗标 (flag)， 4 代表使用者为 super user S 代表这个程序的状态 (STAT)，关于各 STAT 的意义将在内文介绍 UID 程序被该 UID 所拥有 PID 就是这个程序的 ID ！ PPID 则是其上级父程序的ID C CPU 使用的资源百分比 PRI 这个是 Priority (优先执行序) 的缩写，详细后面介绍 NI 这个是 Nice 值，在下一小节我们会持续介绍 ADDR 这个是 kernel function，指出该程序在内存的那个部分。如果是个 running的程序，一般就是 “-“ SZ 使用掉的内存大小 WCHAN 目前这个程序是否正在运作当中，若为 - 表示正在运作 TTY 登入者的终端机位置 TIME 使用掉的 CPU 时间。 CMD 所下达的指令为何 在预设的情况下， ps 仅会列出与目前所在的 bash shell 有关的 PID 而已，所以， 当我使用 ps -l 的时候，只有三个 PID。 6）列出目前所有的正在内存当中的程序 1234567# ps auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.0 0.0 10368 676 ? Ss Nov02 0:00 init [3] root 2 0.0 0.0 0 0 ? S&lt; Nov02 0:01 [migration/0]root 3 0.0 0.0 0 0 ? SN Nov02 0:00 [ksoftirqd/0]root 4 0.0 0.0 0 0 ? S&lt; Nov02 0:01 [migration/1]root 5 0.0 0.0 0 0 ? SN Nov02 0:00 [ksoftirqd/1] 说明： USER：该 process 属于那个使用者账号的 PID ：该 process 的号码 %CPU：该 process 使用掉的 CPU 资源百分比 %MEM：该 process 所占用的物理内存百分比 VSZ ：该 process 使用掉的虚拟内存量 (Kbytes) RSS ：该 process 占用的固定的内存量 (Kbytes) TTY ：该 process 是在那个终端机上面运作，若与终端机无关，则显示 ?，另外， tty1-tty6 是本机上面的登入者程序，若为 pts/0 等等的，则表示为由网络连接进主机的程序。 STAT：该程序目前的状态，主要的状态有 R ：该程序目前正在运作，或者是可被运作 S ：该程序目前正在睡眠当中 (可说是 idle 状态)，但可被某些讯号 (signal) 唤醒。 T ：该程序目前正在侦测或者是停止了 Z ：该程序应该已经终止，但是其父程序却无法正常的终止他，造成 zombie (疆尸) 程序的状态 START：该 process 被触发启动的时间 TIME ：该 process 实际使用 CPU 运作的时间 COMMAND：该程序的实际指令 7）列出类似程序树的程序显示 12345678 ps -axjfWarning: bad syntax, perhaps a bogus &apos;-&apos;? See /usr/share/doc/procps-3.2.7/FAQ PPID PID PGID SID TTY TPGID STAT UID TIME COMMAND 0 1 1 1 ? -1 Ss 0 0:00 init [3] 1 2 1 1 ? -1 S&lt; 0 0:01 [migration/0] 1 3 1 1 ? -1 SN 0 0:00 [ksoftirqd/0] 1 4 1 1 ? -1 S&lt; 0 0:01 [migration/1] 1 5 1 1 ? -1 SN 0 0:00 [ksoftirqd/1] 8）找出与 cron 与 syslog 这两个服务有关的 PID 号码 1234# ps aux | egrep &apos;(cron|syslog)&apos;root 2682 0.0 0.0 83384 2000 ? Sl Nov02 0:00 /sbin/rsyslogd -i /var/run/syslogd.pid -c 5root 2735 0.0 0.0 74812 1140 ? Ss Nov02 0:00 crondroot 17475 0.0 0.0 61180 832 pts/0 S+ 16:27 0:00 egrep (cron|syslog) 说明： 其他实例： 可以用 | 管道和 more 连接起来分页查看 1ps -aux |more 把所有进程显示出来，并输出到ps001.txt文件 1ps -aux &gt; ps001.txt 输出指定的字段 1234# ps -o pid,ppid,pgrp,session,tpgid,comm PID PPID PGRP SESS TPGID COMMAND17398 17394 17398 17398 17478 bash17478 17398 17478 17398 17478 ps 参考链接： http://www.cnblogs.com/peida/archive/2012/12/19/2824418.html http://man.linuxde.net/ps]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-killall命令]]></title>
    <url>%2F2018%2F10%2F04%2FLinux%E5%91%BD%E4%BB%A4-killall%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Linux系统中的killall命令用于杀死指定名字的进程（kill processes by name）。我们可以使用kill命令杀死指定进程PID的进程，如果要找到我们需要杀死的进程，我们还需要在之前使用ps等命令再配合grep来查找进程，而killall把这两个过程合二为一，是一个很好用的命令。 语法killall(选项)(参数) 选项123456789101112-Z 只杀死拥有scontext 的进程-e 要求匹配进程名称-I 忽略小写-g 杀死进程组而不是进程-i 交互模式，杀死进程前先询问用户-l 列出所有的已知信号名称-q 不输出警告信息-s 发送指定的信号-v 报告信号是否成功发送-w 等待进程死亡--help 显示帮助信息--version 显示版本显示 参数进程名称：指定要杀死的进程名称 常用实例1）杀死所有同名进程 1234567# ps -ef|grep viroot 17581 17398 0 17:51 pts/0 00:00:00 vi test.txtroot 17640 17612 0 17:51 pts/2 00:00:00 vi test.logroot 17642 17582 0 17:51 pts/1 00:00:00 grep vi# killall vi# ps -ef|grep viroot 17645 17582 0 17:52 pts/1 00:00:00 grep vi 2）向进程发送指定信号 1234567891011121314151617181920# vi &amp; [1] 17646[root@localhost ~]# killall -TERM vi[1]+ Stopped vi# vi &amp; [2] 17648# ps -ef|grep viroot 17646 17582 0 17:54 pts/1 00:00:00 viroot 17648 17582 0 17:54 pts/1 00:00:00 viroot 17650 17582 0 17:55 pts/1 00:00:00 grep vi[2]+ Stopped vi# killall -TERM vi# ps -ef|grep viroot 17646 17582 0 17:54 pts/1 00:00:00 viroot 17648 17582 0 17:54 pts/1 00:00:00 viroot 17653 17582 0 17:55 pts/1 00:00:00 grep vi# killall -KILL vi[1]- 已杀死 vi[2]+ 已杀死 vi# ps -ef|grep viroot 17656 17582 0 17:56 pts/1 00:00:00 grep vi 3）把所有的登录后的shell给杀掉 123456789# w 18:01:03 up 41 days, 18:53, 3 users, load average: 0.00, 0.00, 0.00USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATroot pts/0 10.2.0.68 14:58 9:52 0.10s 0.10s -bashroot pts/1 10.2.0.68 17:51 0.00s 0.02s 0.00s wroot pts/2 10.2.0.68 17:51 9:24 0.01s 0.01s -bash# killall -9 bash# w 18:01:48 up 41 days, 18:54, 1 user, load average: 0.07, 0.02, 0.00USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATroot pts/0 10.2.0.68 18:01 0.00s 0.01s 0.00s w 说明： 运行命令：killall -9 bash 后，所有bash都会被卡掉了，所以当前所有连接丢失了。需要重新连接并登录。 参考链接： http://www.cnblogs.com/peida/archive/2012/12/21/2827366.html http://man.linuxde.net/killall]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-ln命令]]></title>
    <url>%2F2018%2F10%2F01%2FLinux%E5%91%BD%E4%BB%A4-ln%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[ln是linux中又一个非常重要命令，它的功能是为某一个文件在另外一个位置建立一个同步的链接.当我们需要在不同的目录，用到相同的文件时，我们不需要在每一个需要的目录下都放一个必须相同的文件，我们只要在某个固定的目录，放上该文件，然后在 其它的目录下用ln命令链接（link）它就可以，不必重复的占用磁盘空间。 语法ln(选项)(参数) 选项1234567891011-b或--backup：删除，覆盖目标文件之前的备份；-d或-F或——directory：建立目录的硬连接；-f或——force：强行建立文件或目录的连接，不论文件或目录是否存在；-i或——interactive：覆盖既有文件之前先询问用户；-n或--no-dereference：把符号连接的目的目录视为一般文件；-s或——symbolic：对源文件建立符号连接，而非硬连接；-S&lt;字尾备份字符串&gt;或--suffix=&lt;字尾备份字符串&gt;：用&quot;-b&quot;参数备份目标文件后，备份文件的字尾会被加上一个备份字符串，预设的备份字符串是符号“~”，用户可通过“-S”参数来改变它；-v或——verbose：显示指令执行过程；-V&lt;备份方式&gt;或--version-control=&lt;备份方式&gt;：用“-b”参数备份目标文件后，备份文件的字尾会被加上一个备份字符串，这个字符串不仅可用“-S”参数变更，当使用“-V”参数&lt;备份方式&gt;指定不同备份方式时，也会产生不同字尾的备份字符串；--help：在线帮助；--version：显示版本信息。 参数 源文件：指定连接的源文件。如果使用-s选项创建符号连接，则“源文件”可以是文件或者目录。创建硬连接时，则“源文件”参数只能是文件； 目标文件：指定源文件的目标连接文件。 功能linux文件系统中，有所谓的链接（link），我们可以将其视为档案的别名，而链接又可分为两种：硬链接（hard link）与软链接（symbolic link），硬链接的意思是一个档案可以有多个名称，而软链接的方式则是产生一个特殊的档案，该档案的内容是指向另一个档案的位置。硬链接是存在同一个系统中，而软链接却可以跨越不同的文件系统。 软链接： 1.软链接，以路径的形式存在。类似于Windows操作系统中的快捷方式 2.软链接可以 跨文件系统 ，硬链接不可以 3.软链接可以对一个不存在的文件名进行链接 4.软链接可以对目录进行链接 硬链接: 1.硬链接，以文件副本的形式存在。但不占用实际空间。 2.不允许给目录创建硬链接 3.硬链接只有在同一个文件系统中才能创建 这里有两点要注意： 第一，ln命令会保持每一处链接文件的同步性，也就是说，不论你改动了哪一处，其它的文件都会发生相同的变化； 第二，ln的链接又分软链接和硬链接两种，软链接就是ln –s 源文件 目标文件，它只会在你选定的位置上生成一个文件的镜像，不会占用磁盘空间，硬链接 ln 源文件 目标文件，没有参数-s， 它会在你选定的位置上生成一个和源文件大小相同的文件，无论是软链接还是硬链接，文件都保持同步变化。 ln指令用在链接文件或目录，如同时指定两个以上的文件或目录，且最后的目的地是一个已经存在的目录，则会把前面指定的所有文件或目录复制到该目录中。若同时指定多个文件或目录，且最后的目的地并非是一个已存在的目录，则会出现错误信息。 常用范例1）给文件创建软链接 123456# ll-rw-r--r-- 1 root bin 61 11-13 06:03 log2013.log# ln -s log2013.log link2013# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 1 root bin 61 11-13 06:03 log2013.log 说明： 为log2013.log文件创建软链接link2013，如果log2013.log丢失，link2013将失效 2）给文件创建硬链接 12345678# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 1 root bin 61 11-13 06:03 log2013.log# ln log2013.log ln2013# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 2 root bin 61 11-13 06:03 ln2013-rw-r--r-- 2 root bin 61 11-13 06:03 log2013.log 说明： 为log2013.log创建硬链接ln2013，log2013.log与ln2013的各项属性相同 3）接上面两实例，链接完毕后，删除和重建链接原文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 2 root bin 61 11-13 06:03 ln2013-rw-r--r-- 2 root bin 61 11-13 06:03 log2013.log# rm -rf log2013.log # lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 1 root bin 61 11-13 06:03 ln2013# touch log2013.log# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 1 root bin 61 11-13 06:03 ln2013---xrw-r-- 1 root bin 302108 11-13 06:03 log2012.log-rw-r--r-- 1 root root 0 12-07 16:19 log2013.log# vi log2013.log 2013-012013-022013-032013-042013-052013-062013-072013-082013-092013-102013-112013-12# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 1 root bin 61 11-13 06:03 ln2013-rw-r--r-- 1 root root 96 12-07 16:21 log2013.log# cat link2013 2013-012013-022013-032013-042013-052013-062013-072013-082013-092013-102013-112013-12# cat ln2013 hostnamebaidu=baidu.comhostnamesina=sina.comhostnames=true 说明： 1.源文件被删除后，并没有影响硬链接文件；软链接文件在centos系统下不断的闪烁，提示源文件已经不存在 2.重建源文件后，软链接不在闪烁提示，说明已经链接成功，找到了链接文件系统；重建后，硬链接文件并没有受到源文件影响，硬链接文件的内容还是保留了删除前源文件的内容，说明硬链接已经失效 4）将文件链接为另一个目录中的相同名字 1234567891011121314151617181920212223242526# ln log2013.log test3# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 1 root bin 61 11-13 06:03 ln2013-rw-r--r-- 2 root root 96 12-07 16:21 log2013.log# cd test3# ll-rw-r--r-- 2 root root 96 12-07 16:21 log2013.log# vi log2013.log 2013-012013-022013-032013-042013-052013-062013-072013-082013-092013-10# ll-rw-r--r-- 2 root root 80 12-07 16:36 log2013.log# cd ..# lllrwxrwxrwx 1 root root 11 12-07 16:01 link2013 -&gt; log2013.log-rw-r--r-- 1 root bin 61 11-13 06:03 ln2013-rw-r--r-- 2 root root 80 12-07 16:36 log2013.log 说明： 在test3目录中创建了log2013.log的硬链接，修改test3目录中的log2013.log文件，同时也会同步到源文件 5）给目录创建软链接 123456789101112131415161718192021222324252627# lldrwxr-xr-x 2 root root 4096 12-07 16:36 test3drwxr-xr-x 2 root root 4096 12-07 16:57 test5# cd test5# lllrwxrwxrwx 1 root root 5 12-07 16:57 test3 -&gt; test3# cd test3-bash: cd: test3: 符号连接的层数过多# lllrwxrwxrwx 1 root root 5 12-07 16:57 test3 -&gt; test3# rm -rf test3# ll# ln -sv /opt/soft/test/test3 /opt/soft/test/test5创建指向“/opt/soft/test/test3”的符号链接“/opt/soft/test/test5/test3”# lllrwxrwxrwx 1 root root 20 12-07 16:59 test3 -&gt; /opt/soft/test/test3# # cd test3# ll总计 4-rw-r--r-- 2 root root 80 12-07 16:36 log2013.log# touch log2014.log# ll总计 4-rw-r--r-- 2 root root 80 12-07 16:36 log2013.log-rw-r--r-- 1 root root 0 12-07 17:05 log2014.log 说明： 1.目录只能创建软链接 2.目录创建链接必须用绝对路径，相对路径创建会不成功，会提示：符号连接的层数过多 这样的错误 3.在链接目标目录中修改文件都会在源文件目录中同步变化 参考链接： http://www.cnblogs.com/peida/archive/2012/12/11/2812294.html http://man.linuxde.net/ln]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-cal命令]]></title>
    <url>%2F2018%2F10%2F01%2FLinux%E5%91%BD%E4%BB%A4-cal%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[cal命令用于显示当前日历，或者指定日期的日历。 语法cal(选项)(参数) 选项123456-l：显示单月输出；-3：显示临近三个月的日历；-s：将星期日作为月的第一天；-m：将星期一作为月的第一天；-j：显示“julian”日期；-y：显示当前年的日历。 参数12月：指定月份；年：指定年份。 常用实例1）显示当前月份日历 12345678# cal March 2018 Su Mo Tu We Th Fr Sa 1 2 3 4 5 6 7 8 9 1011 12 13 14 15 16 1718 19 20 21 22 23 2425 26 27 28 29 30 31 2）显示指定月份的日历 1234567891011121314151617# cal 9 2012 九月 2012 日 一 二 三 四 五 六 1 2 3 4 5 6 7 8 9 10 11 12 13 14 1516 17 18 19 20 21 2223 24 25 26 27 28 2930 3）显示2013年日历 12345678910111213141516171819202122232425262728293031323334# cal -y 2013 2013 January February March Su Mo Tu We Th Fr Sa Su Mo Tu We Th Fr Sa Su Mo Tu We Th Fr Sa 1 2 3 4 5 1 2 1 2 6 7 8 9 10 11 12 3 4 5 6 7 8 9 3 4 5 6 7 8 913 14 15 16 17 18 19 10 11 12 13 14 15 16 10 11 12 13 14 15 1620 21 22 23 24 25 26 17 18 19 20 21 22 23 17 18 19 20 21 22 2327 28 29 30 31 24 25 26 27 28 24 25 26 27 28 29 30 31 April May June Su Mo Tu We Th Fr Sa Su Mo Tu We Th Fr Sa Su Mo Tu We Th Fr Sa 1 2 3 4 5 6 1 2 3 4 1 7 8 9 10 11 12 13 5 6 7 8 9 10 11 2 3 4 5 6 7 814 15 16 17 18 19 20 12 13 14 15 16 17 18 9 10 11 12 13 14 1521 22 23 24 25 26 27 19 20 21 22 23 24 25 16 17 18 19 20 21 2228 29 30 26 27 28 29 30 31 23 24 25 26 27 28 29 30 July August September Su Mo Tu We Th Fr Sa Su Mo Tu We Th Fr Sa Su Mo Tu We Th Fr Sa 1 2 3 4 5 6 1 2 3 1 2 3 4 5 6 7 7 8 9 10 11 12 13 4 5 6 7 8 9 10 8 9 10 11 12 13 1414 15 16 17 18 19 20 11 12 13 14 15 16 17 15 16 17 18 19 20 2121 22 23 24 25 26 27 18 19 20 21 22 23 24 22 23 24 25 26 27 2828 29 30 31 25 26 27 28 29 30 31 29 30 October November December Su Mo Tu We Th Fr Sa Su Mo Tu We Th Fr Sa Su Mo Tu We Th Fr Sa 1 2 3 4 5 1 2 1 2 3 4 5 6 7 6 7 8 9 10 11 12 3 4 5 6 7 8 9 8 9 10 11 12 13 1413 14 15 16 17 18 19 10 11 12 13 14 15 16 15 16 17 18 19 20 2120 21 22 23 24 25 26 17 18 19 20 21 22 23 22 23 24 25 26 27 2827 28 29 30 31 24 25 26 27 28 29 30 29 30 31 4）显示自1月1日的天数 12345678# cal -j March 2018 Sun Mon Tue Wed Thu Fri Sat 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 5）星期一显示在第一列 12345678# cal -m March 2018 Mo Tu We Th Fr Sa Su 1 2 3 4 5 6 7 8 9 10 1112 13 14 15 16 17 1819 20 21 22 23 24 2526 27 28 29 30 31 参考链接： http://man.linuxde.net/cal http://www.cnblogs.com/peida/archive/2012/12/14/2817473.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-date命令]]></title>
    <url>%2F2018%2F10%2F01%2FLinux%E5%91%BD%E4%BB%A4-date%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[在linux环境中，不管是编程还是其他维护，时间是必不可少的，也经常会用到时间的运算，熟练运用date命令来表示自己想要表示的时间，肯定可以给自己的工作带来诸多方便。 语法date(选项)(参数) 选项12345-d&lt;字符串&gt;：显示字符串所指的日期与时间。字符串前后必须加上双引号；-s&lt;字符串&gt;：根据字符串来设置日期与时间。字符串前后必须加上双引号；-u：显示GMT；--help：在线帮助；--version：显示版本信息。 参数&lt;+时间日期格式&gt;：指定显示时使用的日期时间格式。 日期格式字符串列表123456789101112131415161718192021222324%H 小时，24小时制（00~23）%I 小时，12小时制（01~12）%k 小时，24小时制（0~23）%l 小时，12小时制（1~12）%M 分钟（00~59）%p 显示出AM或PM%r 显示时间，12小时制（hh:mm:ss %p）%s 从1970年1月1日00:00:00到目前经历的秒数%S 显示秒（00~59）%T 显示时间，24小时制（hh:mm:ss）%X 显示时间的格式（%H:%M:%S）%Z 显示时区，日期域（CST）%a 星期的简称（Sun~Sat）%A 星期的全称（Sunday~Saturday）%h,%b 月的简称（Jan~Dec）%B 月的全称（January~December）%c 日期和时间（Tue Nov 20 14:12:58 2012）%d 一个月的第几天（01~31）%x,%D 日期（mm/dd/yy）%j 一年的第几天（001~366）%m 月份（01~12）%w 一个星期的第几天（0代表星期天）%W 一年的第几个星期（00~53，星期一为第一天）%y 年的最后两个数字（1999则是99） 常用实例1）格式化输出： 12#date +&quot;%Y-%m-%d&quot;2018-03-29 2）输出昨天日期： 12#date -d &quot;1 day ago&quot; +&quot;%Y-%m-%d&quot;2018-03-28 3）2秒后输出： 12#date -d &quot;2 second&quot; +&quot;%Y-%m-%d %H:%M:%S&quot; 2018-03-29 10:08:37 4）apache格式转换： 12date -d &quot;Dec 5, 2009 12:00:37 AM&quot; +&quot;%Y-%m-%d %H:%M.%S&quot;2009-12-05 00:00.37 5）格式转换后时间游走： 12date -d &quot;Dec 5, 2009 12:00:37 AM 2 year ago&quot; +&quot;%Y-%m-%d %H:%M.%S&quot;2007-12-05 00:00.37 6）加减操作： 1234567date +%Y%m%d //显示前天年月日date -d &quot;+1 day&quot; +%Y%m%d //显示前一天的日期date -d &quot;-1 day&quot; +%Y%m%d //显示后一天的日期date -d &quot;-1 month&quot; +%Y%m%d //显示上一月的日期date -d &quot;+1 month&quot; +%Y%m%d //显示下一月的日期date -d &quot;-1 year&quot; +%Y%m%d //显示前一年的日期date -d &quot;+1 year&quot; +%Y%m%d //显示下一年的日期 7）设定时间 1234567date -s //设置当前时间，只有root权限才能设置，其他只能查看date -s 20120523 //设置成20120523，这样会把具体时间设置成空00:00:00date -s 01:01:01 //设置具体时间，不会对日期做更改date -s &quot;01:01:01 2012-05-23&quot; //这样可以设置全部时间date -s &quot;01:01:01 20120523&quot; //这样可以设置全部时间date -s &quot;2012-05-23 01:01:01&quot; //这样可以设置全部时间date -s &quot;20120523 01:01:01&quot; //这样可以设置全部时间 8）有时需要检查一组命令花费的时间，举例： 12345678#!/bin/bashstart=$(date +%s)nmap man.linuxde.net &amp;&gt; /dev/nullend=$(date +%s)difference=$(( end - start ))echo $difference seconds. 参考链接： http://man.linuxde.net/date http://www.cnblogs.com/peida/archive/2012/12/13/2815687.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-diff命令]]></title>
    <url>%2F2018%2F10%2F01%2FLinux%E5%91%BD%E4%BB%A4-diff%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[diff 命令是 linux上非常重要的工具，用于比较文件的内容，特别是比较两个版本不同的文件以找到改动的地方。diff在命令行中打印每一个行的改动。最新版本的diff还支持二进制文件。diff程序的输出被称为补丁 (patch)，因为Linux系统中还有一个patch程序，可以根据diff的输出将a.c的文件内容更新为b.c。diff是svn、cvs、git等版本控制工具不可或缺的一部分。 语法diff(选项)(参数) 选项12345678910111213141516171819202122232425262728293031323334-&lt;行数&gt;：指定要显示多少行的文本。此参数必须与-c或-u参数一并使用；-a或——text：diff预设只会逐行比较文本文件；-b或--ignore-space-change：不检查空格字符的不同；-B或--ignore-blank-lines：不检查空白行；-c：显示全部内容，并标出不同之处；-C&lt;行数&gt;或--context&lt;行数&gt;：与执行“-c-&lt;行数&gt;”指令相同；-d或——minimal：使用不同的演算法，以小的单位来做比较；-D&lt;巨集名称&gt;或ifdef&lt;巨集名称&gt;：此参数的输出格式可用于前置处理器巨集；-e或——ed：此参数的输出格式可用于ed的script文件；-f或-forward-ed：输出的格式类似ed的script文件，但按照原来文件的顺序来显示不同处；-H或--speed-large-files：比较大文件时，可加快速度；-l&lt;字符或字符串&gt;或--ignore-matching-lines&lt;字符或字符串&gt;：若两个文件在某几行有所不同，而之际航同时都包含了选项中指定的字符或字符串，则不显示这两个文件的差异；-i或--ignore-case：不检查大小写的不同；-l或——paginate：将结果交由pr程序来分页；-n或——rcs：将比较结果以RCS的格式来显示；-N或--new-file：在比较目录时，若文件A仅出现在某个目录中，预设会显示：Only in目录，文件A 若使用-N参数，则diff会将文件A 与一个空白的文件比较；-p：若比较的文件为C语言的程序码文件时，显示差异所在的函数名称；-P或--unidirectional-new-file：与-N类似，但只有当第二个目录包含了第一个目录所没有的文件时，才会将这个文件与空白的文件做比较；-q或--brief：仅显示有无差异，不显示详细的信息；-r或——recursive：比较子目录中的文件；-s或--report-identical-files：若没有发现任何差异，仍然显示信息；-S&lt;文件&gt;或--starting-file&lt;文件&gt;：在比较目录时，从指定的文件开始比较；-t或--expand-tabs：在输出时，将tab字符展开；-T或--initial-tab：在每行前面加上tab字符以便对齐；-u，-U&lt;列数&gt;或--unified=&lt;列数&gt;：以合并的方式来显示文件内容的不同；-v或——version：显示版本信息；-w或--ignore-all-space：忽略全部的空格字符；-W&lt;宽度&gt;或--width&lt;宽度&gt;：在使用-y参数时，指定栏宽；-x&lt;文件名或目录&gt;或--exclude&lt;文件名或目录&gt;：不比较选项中所指定的文件或目录；-X&lt;文件&gt;或--exclude-from&lt;文件&gt;；您可以将文件或目录类型存成文本文件，然后在=&lt;文件&gt;中指定此文本文件；-y或--side-by-side：以并列的方式显示文件的异同之处；--help：显示帮助；--left-column：在使用-y参数时，若两个文件某一行内容相同，则仅在左侧的栏位显示该行内容；--suppress-common-lines：在使用-y参数时，仅显示不同之处。 参数 文件1：指定要比较的第一个文件； 文件2：指定要比较的第二个文件。 功能diff命令能比较单个文件或者目录内容。如果指定比较的是文件，则只有当输入为文本文件时才有效。以逐行的方式，比较文本文件的异同处。如果指定比较的是目录的时候，diff命令会比较两个目录下名字相同的文本文件。列出不同的二进制文件、公共子目录和只在一个目录出现的文件。 常用实例1）比较两个文件 123456789101112# diff log2014.log log2013.log 3c3&lt; 2014-03---&gt; 2013-038c8&lt; 2013-07---&gt; 2013-0811,12d10&lt; 2013-11&lt; 2013-12 说明： 上面的“3c3”和“8c8”表示log2014.log和log20143log文件在3行和第8行内容有所不同；”11,12d10”表示第一个文件比第二个文件多了第11和12行。 diff 的normal 显示格式有三种提示: a - add c - change d - delete 2）并排格式输出 12345678910111213141516171819202122232425# diff log2014.log log2013.log -y -W 502013-01 2013-012013-02 2013-022014-03 | 2013-032013-04 2013-042013-05 2013-052013-06 2013-062013-07 2013-072013-07 | 2013-082013-09 2013-092013-10 2013-102013-11 &lt;2013-12 &lt;# diff log2013.log log2014.log -y -W 502013-01 2013-012013-02 2013-022013-03 | 2014-032013-04 2013-042013-05 2013-052013-06 2013-062013-07 2013-072013-08 | 2013-072013-09 2013-09 &gt; 2013-11 &gt; 2013-12 说明： “|”表示前后2个文件内容有不同 “&lt;”表示后面文件比前面文件少了1行内容 “&gt;”表示后面文件比前面文件多了1行内容 3）上下文输出格式 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# diff log2013.log log2014.log -c*** log2013.log 2012-12-07 16:36:26.000000000 +0800--- log2014.log 2012-12-07 18:01:54.000000000 +0800****************** 1,10 **** 2013-01 2013-02! 2013-03 2013-04 2013-05 2013-06 2013-07! 2013-08 2013-09 2013-10--- 1,12 ---- 2013-01 2013-02! 2014-03 2013-04 2013-05 2013-06 2013-07! 2013-07 2013-09 2013-10+ 2013-11+ 2013-12# diff log2014.log log2013.log -c*** log2014.log 2012-12-07 18:01:54.000000000 +0800--- log2013.log 2012-12-07 16:36:26.000000000 +0800****************** 1,12 **** 2013-01 2013-02! 2014-03 2013-04 2013-05 2013-06 2013-07! 2013-07 2013-09 2013-10- 2013-11- 2013-12--- 1,10 ---- 2013-01 2013-02! 2013-03 2013-04 2013-05 2013-06 2013-07! 2013-08 2013-09 2013-10 说明： 这种方式在开头两行作了比较文件的说明，这里有三中特殊字符： “＋” 比较的文件的后者比前着多一行 “－” 比较的文件的后者比前着少一行 “！” 比较的文件两者有差别的行 4）统一格式输出 123456789101112131415161718# diff log2014.log log2013.log -u--- log2014.log 2012-12-07 18:01:54.000000000 +0800+++ log2013.log 2012-12-07 16:36:26.000000000 +0800@@ -1,12 +1,10 @@ 2013-01 2013-02-2014-03+2013-03 2013-04 2013-05 2013-06 2013-07-2013-07+2013-08 2013-09 2013-10-2013-11-2013-12 说明： 它的第一部分，也是文件的基本信息： — log2014.log 2012-12-07 18:01:54.000000000 +0800 +++ log2013.log 2012-12-07 16:36:26.000000000 +0800 “—“表示变动前的文件，”+++”表示变动后的文件。 第二部分，变动的位置用两个@作为起首和结束。 @@ -1,12 +1,10 @@ 前面的”-1,12”分成三个部分：减号表示第一个文件（即log2014.log），”1”表示第1行，”12”表示连续12行。合在一起，就表示下面是第一个文件从第1行开始的连续12行。同样的，”+1,10”表示变动后，成为第二个文件从第1行开始的连续10行。 5）比较文件夹不同 123456789101112131415161718192021222324252627282930313233343536# diff test3 test6Only in test6: linklog.logOnly in test6: log2012.logdiff test3/log2013.log test6/log2013.log1,10c1,3&lt; 2013-01&lt; 2013-02&lt; 2013-03&lt; 2013-04&lt; 2013-05&lt; 2013-06&lt; 2013-07&lt; 2013-08&lt; 2013-09&lt; 2013-10---&gt; hostnamebaidu=baidu.com&gt; hostnamesina=sina.com&gt; hostnames=truediff test3/log2014.log test6/log2014.log1,12d0&lt; 2013-01&lt; 2013-02&lt; 2014-03&lt; 2013-04&lt; 2013-05&lt; 2013-06&lt; 2013-07&lt; 2013-07&lt; 2013-09&lt; 2013-10&lt; 2013-11&lt; 2013-12Only in test6: log2015.logOnly in test6: log2016.logOnly in test6: log2017.log 6）比较两个文件不同，并生产补丁 123456789101112131415161718192021222324# diff -ruN log2013.log log2014.log &gt;patch.log# ll总计 12-rw-r--r-- 2 root root 80 12-07 16:36 log2013.log-rw-r--r-- 1 root root 96 12-07 18:01 log2014.log-rw-r--r-- 1 root root 248 12-07 21:33 patch.log# cat patch.log --- log2013.log 2012-12-07 16:36:26.000000000 +0800+++ log2014.log 2012-12-07 18:01:54.000000000 +0800@@ -1,10 +1,12 @@ 2013-01 2013-02-2013-03+2014-03 2013-04 2013-05 2013-06 2013-07-2013-08+2013-07 2013-09 2013-10+2013-11+2013-12 7）打补丁 1234567891011121314151617181920212223242526# cat log2013.log2013-012013-022013-032013-042013-052013-062013-072013-082013-092013-10# patch log2013.log patch.log patching file log2013.log# cat log2013.log 2013-012013-022014-032013-042013-052013-062013-072013-072013-092013-102013-112013-12 参考链接： http://www.cnblogs.com/peida/archive/2012/12/12/2814048.html http://man.linuxde.net/diff]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-df命令]]></title>
    <url>%2F2018%2F09%2F29%2FLinux%E5%91%BD%E4%BB%A4-df%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[df命令用于显示磁盘分区上的可使用的磁盘空间。默认显示单位为KB。可以利用该命令来获取硬盘被占用了多少空间，目前还剩下多少空间等信息。 语法df(选项)(参数) 选项12345678910111213141516-a或--all：包含全部的文件系统；--block-size=&lt;区块大小&gt;：以指定的区块大小来显示区块数目；-h或--human-readable：以可读性较高的方式来显示信息；-H或--si：与-h参数相同，但在计算时是以1000 Bytes为换算单位而非1024 Bytes；-i或--inodes：显示inode的信息；-k或--kilobytes：指定区块大小为1024字节；-l或--local：仅显示本地端的文件系统；-m或--megabytes：指定区块大小为1048576字节；--no-sync：在取得磁盘使用信息前，不要执行sync指令，此为预设值；-P或--portability：使用POSIX的输出格式；--sync：在取得磁盘使用信息前，先执行sync指令；-t&lt;文件系统类型&gt;或--type=&lt;文件系统类型&gt;：仅显示指定文件系统类型的磁盘信息；-T或--print-type：显示文件系统的类型；-x&lt;文件系统类型&gt;或--exclude-type=&lt;文件系统类型&gt;：不要显示指定文件系统类型的磁盘信息；--help：显示帮助；--version：显示版本信息。 参数文件： 指定文件系统上的文件 常用实例1）查看系统磁盘设备，默认是KB为单位： 123456# df文件系统 1K-块 已用 可用 已用% 挂载点/dev/sda2 146294492 28244432 110498708 21% //dev/sda1 1019208 62360 904240 7% /boottmpfs 1032204 0 1032204 0% /dev/shm/dev/sdb1 2884284108 218826068 2518944764 8% /data1 2）使用-h选项以KB以上的单位来显示，可读性高： 123456# df -h文件系统 容量 已用 可用 已用% 挂载点/dev/sda2 140G 27G 106G 21% //dev/sda1 996M 61M 884M 7% /boottmpfs 1009M 0 1009M 0% /dev/shm/dev/sdb1 2.7T 209G 2.4T 8% /data1 3）查看全部文件系统： 12345678910# df -a文件系统 1K-块 已用 可用 已用% 挂载点/dev/sda2 146294492 28244432 110498708 21% /proc 0 0 0 - /procsysfs 0 0 0 - /sysdevpts 0 0 0 - /dev/pts/dev/sda1 1019208 62360 904240 7% /boottmpfs 1032204 0 1032204 0% /dev/shm/dev/sdb1 2884284108 218826068 2518944764 8% /data1none 0 0 0 - /proc/sys/fs/binfmt_misc 转载链接： http://man.linuxde.net/df]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-du命令]]></title>
    <url>%2F2018%2F09%2F29%2FLinux%E5%91%BD%E4%BB%A4-du%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[du命令也是查看使用空间的，但是与df命令不同的是Linux du命令是对文件和目录磁盘使用的空间的查看，还是和df命令有一些区别的。 语法du [选项][文件] 选项123456789101112131415-a或-all 显示目录中个别文件的大小。-b或-bytes 显示目录或文件大小时，以byte为单位。-c或--total 除了显示个别目录或文件的大小外，同时也显示所有目录或文件的总和。-k或--kilobytes 以KB(1024bytes)为单位输出。-m或--megabytes 以MB为单位输出。-s或--summarize 仅显示总计，只列出最后加总的值。-h或--human-readable 以K，M，G为单位，提高信息的可读性。-x或--one-file-xystem 以一开始处理时的文件系统为准，若遇上其它不同的文件系统目录则略过。-L&lt;符号链接&gt;或--dereference&lt;符号链接&gt; 显示选项中所指定符号链接的源文件大小。-S或--separate-dirs 显示个别目录的大小时，并不含其子目录的大小。-X&lt;文件&gt;或--exclude-from=&lt;文件&gt; 在&lt;文件&gt;指定目录或文件。--exclude=&lt;目录或文件&gt; 略过指定的目录或文件。-D或--dereference-args 显示指定符号链接的源文件大小。-H或--si 与-h参数相同，但是K，M，G是以1000为换算单位。-l或--count-links 重复计算硬件链接的文件。 常用实例1）显示目录或者文件所占空间 12345678910111213# du608 ./test6308 ./test44 ./scf/lib4 ./scf/service/deploy/product4 ./scf/service/deploy/info12 ./scf/service/deploy16 ./scf/service4 ./scf/doc4 ./scf/bin32 ./scf8 ./test31288 . 说明： 只显示当前目录下面的子目录的目录大小和当前目录的总的大小，最下面的1288为当前目录的总大小 2）显示指定文件所占空间 12# du log2012.log 300 log2012.log 3）查看指定目录的所占空间 123456789# du scf4 scf/lib4 scf/service/deploy/product4 scf/service/deploy/info12 scf/service/deploy16 scf/service4 scf/doc4 scf/bin32 scf 4）显示多个文件所占空间 123# du log30.tar.gz log31.tar.gz 4 log30.tar.gz4 log31.tar.gz 5）只显示总和的大小 1234567# du -s1288 .# du -s scf32 scf# cd ..# du -s test1288 test 6）方便阅读的格式显示 123456# du -h test608K test/test6308K test/test44.0K test/scf/lib4.0K test/scf/service/deploy/product4.0K test/scf/service/deploy/info 7）文件和目录都显示 123456789101112# du -ah test4.0K test/log31.tar.gz4.0K test/test13.tar.gz0 test/linklog.log0 test/test6/log2014.log300K test/test6/linklog.log0 test/test6/log2015.log4.0K test/test6/log2013.log300K test/test6/log2012.log0 test/test6/log2017.log0 test/test6/log2016.log608K test/test6 8）显示几个文件或目录各自占用磁盘空间的大小，还统计它们的总和 1234# du -c log30.tar.gz log31.tar.gz 4 log30.tar.gz4 log31.tar.gz8 总计 说明： 加上-c选项后，du不仅显示两个目录各自占用磁盘空间的大小，还在最后一行统计它们的总和。 9）按照空间大小排序 1234567# du|sort -nr|more1288 .608 ./test6308 ./test432 ./scf16 ./scf/service12 ./scf/service/deploy 10）输出当前目录下各个子目录所使用的空间 123456# du -h --max-depth=1608K ./test6308K ./test432K ./scf8.0K ./test31.3M . 参考链接： http://www.cnblogs.com/peida/archive/2012/12/10/2810755.html http://man.linuxde.net/du]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-gzip命令]]></title>
    <url>%2F2018%2F09%2F29%2FLinux%E5%91%BD%E4%BB%A4-gzip%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[gzip命令用来压缩文件。gzip是个使用广泛的压缩程序，文件经它压缩过后，其名称后面会多处“.gz”扩展名。 gzip是在Linux系统中经常使用的一个对文件进行压缩和解压缩的命令，既方便又好用。gzip不仅可以用来压缩大的、较少使用的文件以节省磁盘空间，还可以和tar命令一起构成Linux操作系统中比较流行的压缩文件格式。据统计，gzip命令对文本文件有60%～70%的压缩率。减少文件大小有两个明显的好处，一是可以减少存储空间，二是通过网络传输文件时，可以减少传输的时间。 语法gzip(选项)(参数) 选项1234567891011121314151617-a或——ascii：使用ASCII文字模式；-d或--decompress或----uncompress：解开压缩文件；-f或——force：强行压缩文件。不理会文件名称或硬连接是否存在以及该文件是否为符号连接；-h或——help：在线帮助；-l或——list：列出压缩文件的相关信息；-L或——license：显示版本与版权信息；-n或--no-name：压缩文件时，不保存原来的文件名称及时间戳记；-N或——name：压缩文件时，保存原来的文件名称及时间戳记；-q或——quiet：不显示警告信息；-r或——recursive：递归处理，将指定目录下的所有文件及子目录一并处理；-S或&lt;压缩字尾字符串&gt;或----suffix&lt;压缩字尾字符串&gt;：更改压缩字尾字符串；-t或——test：测试压缩文件是否正确无误；-v或——verbose：显示指令执行过程；-V或——version：显示版本信息；-&lt;压缩效率&gt;：压缩效率是一个介于1~9的数值，预设值为“6”，指定愈大的数值，压缩效率就会愈高；--best：此参数的效果和指定“-9”参数相同；--fast：此参数的效果和指定“-1”参数相同。 参数文件列表：指定要压缩的文件列表。 常用范例1）把test6目录下的每个文件压缩成.gz文件 1234567891011# ll总计 604---xr--r-- 1 root mail 302108 11-30 08:39 linklog.log---xr--r-- 1 mail users 302108 11-30 08:39 log2012.log-rw-r--r-- 1 mail users 61 11-30 08:39 log2013.log# gzip *# ll总计 28---xr--r-- 1 root mail 1341 11-30 08:39 linklog.log.gz---xr--r-- 1 mail users 1341 11-30 08:39 log2012.log.gz-rw-r--r-- 1 mail users 70 11-30 08:39 log2013.log.gz 2）把例1中每个压缩的文件解压，并列出详细的信息 1234567891011121314# ll总计 28---xr--r-- 1 root mail 1341 11-30 08:39 linklog.log.gz---xr--r-- 1 mail users 1341 11-30 08:39 log2012.log.gz-rw-r--r-- 1 mail users 70 11-30 08:39 log2013.log.gz# gzip -dv *linklog.log.gz: 99.6% -- replaced with linklog.loglog2012.log.gz: 99.6% -- replaced with log2012.loglog2013.log.gz: 47.5% -- replaced with log2013.log# ll总计 604---xr--r-- 1 root mail 302108 11-30 08:39 linklog.log---xr--r-- 1 mail users 302108 11-30 08:39 log2012.log-rw-r--r-- 1 mail users 61 11-30 08:39 log2013.log 3）详细显示例1中每个压缩的文件的信息，并不解压 12345# gzip -l * compressed uncompressed ratio uncompressed_name 1341 302108 99.6% linklog.log 1341 302108 99.6% log2012.log 70 61 47.5% log2013.log 4）压缩一个tar备份文件，此时压缩文件的扩展名为.tar.gz 12345#ls -al log.tar-rw-r--r-- 1 root root 307200 11-29 17:54 log.tar# gzip -r log.tar# ls -al log.tar.gz -rw-r--r-- 1 root root 1421 11-29 17:54 log.tar.gz 5）递归的压缩目录 12345678910111213141516# ll总计 604---xr--r-- 1 root mail 302108 11-30 08:39 linklog.log---xr--r-- 1 mail users 302108 11-30 08:39 log2012.log-rw-r--r-- 1 mail users 61 11-30 08:39 log2013.log# cd ..# gzip -rv test6test6/linklog.log: 99.6% -- replaced with test6/linklog.log.gztest6/log2013.log: 47.5% -- replaced with test6/log2013.log.gztest6/log2012.log: 99.6% -- replaced with test6/log2012.log.gz# cd test6# ll总计 28---xr--r-- 1 root mail 1341 11-30 08:39 linklog.log.gz---xr--r-- 1 mail users 1341 11-30 08:39 log2012.log.gz-rw-r--r-- 1 mail users 70 11-30 08:39 log2013.log.gz 说明： 这样，所有test下面的文件都变成了.gz，目录依然存在只是目录里面的文件相应变成了.gz.这就是压缩，和打包不同。因为是对目录操作，所以需要加上-r选项，这样也可以对子目录进行递归了。 6）递归地解压目录 12345678910111213# ll总计 28---xr--r-- 1 root mail 1341 11-30 08:39 linklog.log.gz---xr--r-- 1 mail users 1341 11-30 08:39 log2012.log.gz-rw-r--r-- 1 mail users 70 11-30 08:39 log2013.log.gz# cd ..# gzip -dr test6# cd test6# ll总计 604---xr--r-- 1 root mail 302108 11-30 08:39 linklog.log---xr--r-- 1 mail users 302108 11-30 08:39 log2012.log-rw-r--r-- 1 mail users 61 11-30 08:39 log2013.log 参考链接： http://www.cnblogs.com/peida/archive/2012/12/06/2804323.html http://man.linuxde.net/gzip]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-/etc/group文件详解]]></title>
    <url>%2F2018%2F09%2F28%2FLinux%E5%91%BD%E4%BB%A4-etc-group%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Linux /etc/group文件与/etc/passwd和/etc/shadow文件都是有关于系统管理员对用户和用户组管理时相关的文件。linux /etc/group文件是有关于系统管理员对用户和用户组管理的文件,linux用户组的所有信息都存放在/etc/group文件中。具有某种共同特征的用户集合起来就是用户组（Group）。用户组（Group）配置文件主要有 /etc/group和/etc/gshadow，其中/etc/gshadow是/etc/group的加密信息文件。 将用户分组是Linux系统中对用户进行管理及控制访问权限的一种手段。每个用户都属于某个用户组；一个组中可以有多个用户，一个用户也可以属于不同的组。当一个用户同时是多个组中的成员时，在/etc/passwd文件中记录的是用户所属的主组，也就是登录时所属的默认组，而其他组称为附加组。 用户组的所有信息都存放在/etc/group文件中。此文件的格式是由冒号(:)隔开若干个字段，这些字段具体如下： 组名:口令:组标识号:组内用户列表 具体解释 组名：组名是用户组的名称，由字母或数字构成。与/etc/passwd中的登录名一样，组名不应重复。 口令：口令字段存放的是用户组加密后的口令字。一般Linux系统的用户组都没有口令，即这个字段一般为空，或者是*。 组标识号：组标识号与用户标识号类似，也是一个整数，被系统内部用来标识组。别称GID. 组内用户列表：属于这个组的所有用户的列表，不同用户之间用逗号(,)分隔。这个用户组可能是用户的主组，也可能是附加组。 使用实例12345# cat /etc/grouproot:x:0:root,linuxsirbin:x:1:root,bin,daemondaemon:x:2:root,bin,daemonsys:x:3:root,bin 说明： 我们以root:\x:0:root,linuxsir 为例： 用户组root，x是密码段，表示没有设置密码，GID是0,root用户组下包括root、linuxsir以及GID为0的其它用户。 转载链接： http://www.cnblogs.com/peida/archive/2012/12/05/2802419.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-chown命令]]></title>
    <url>%2F2018%2F09%2F28%2FLinux%E5%91%BD%E4%BB%A4-chown%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[chown将指定文件的拥有者改为指定的用户或组，用户可以是用户名或者用户ID；组可以是组名或者组ID；文件是以空格分开的要改变权限的文件列表，支持通配符。系统管理员经常使用chown命令，在将文件拷贝到另一个用户的名录下之后，让用户拥有使用该文件的权限。 语法chown(选项)(参数) 选项123456789-c或——changes：效果类似“-v”参数，但仅回报更改的部分；-f或--quite或——silent：不显示错误信息；-h或--no-dereference：只对符号连接的文件作修改，而不更改其他任何相关文件；-R或——recursive：递归处理，将指定目录下的所有文件及子目录一并处理；-v或——version：显示指令执行过程；--dereference：效果和“-h”参数相同；--help：在线帮助；--reference=&lt;参考文件或目录&gt;：把指定文件或目录的拥有者与所属群组全部设成和参考文件或目录的拥有者与所属群组相同；--version：显示版本信息。 参数用户：组：指定所有者和所属工作组。当省略“：组”，仅改变文件所有者；文件：指定要改变所有者和工作组的文件列表。支持多个文件和目标，支持shell通配符。 功能通过chown改变文件的拥有者和群组。在更改文件的所有者或所属群组时，可以使用用户名称和用户识别码设置。普通用户不能将自己的文件改变成其他的拥有者。其操作权限一般为管理员。 常用实例1）改变拥有者和群组 1234567# ll---xr--r-- 1 root users 302108 11-30 08:39 linklog.log---xr--r-- 1 root users 302108 11-30 08:39 log2012.log# chown mail:mail log2012.log # ll---xr--r-- 1 root users 302108 11-30 08:39 linklog.log---xr--r-- 1 mail mail 302108 11-30 08:39 log2012.log 2）改变文件拥有者 123456789ll总计 604---xr--r-- 1 root users 302108 11-30 08:39 linklog.log---xr--r-- 1 mail mail 302108 11-30 08:39 log2012.log# chown root: log2012.log # ll总计 604---xr--r-- 1 root users 302108 11-30 08:39 linklog.log---xr--r-- 1 root mail 302108 11-30 08:39 log2012.log 3）改变文件群组 123456789# ll总计 604---xr--r-- 1 root users 302108 11-30 08:39 linklog.log---xr--r-- 1 root root 302108 11-30 08:39 log2012.log# chown :mail log2012.log # ll总计 604---xr--r-- 1 root users 302108 11-30 08:39 linklog.log---xr--r-- 1 root mail 302108 11-30 08:39 log2012.log 4）改变指定目录以及其子目录下的所有文件的拥有者和群组 1234567891011121314151617181920212223# lldrwxr-xr-x 2 root users 4096 11-30 08:39 test6# chown -R -v root:mail test6“test6/log2014.log” 的所有者已更改为 root:mail“test6/linklog.log” 的所有者已更改为 root:mail“test6/log2015.log” 的所有者已更改为 root:mail“test6/log2013.log” 的所有者已更改为 root:mail“test6/log2012.log” 的所有者已保留为 root:mail“test6/log2017.log” 的所有者已更改为 root:mail“test6/log2016.log” 的所有者已更改为 root:mail“test6” 的所有者已更改为 root:mail# lldrwxr-xr-x 2 root mail 4096 11-30 08:39 test6# cd test6# ll总计 604---xr--r-- 1 root mail 302108 11-30 08:39 linklog.log---xr--r-- 1 root mail 302108 11-30 08:39 log2012.log-rw-r--r-- 1 root mail 61 11-30 08:39 log2013.log-rw-r--r-- 1 root mail 0 11-30 08:39 log2014.log-rw-r--r-- 1 root mail 0 11-30 08:39 log2015.log-rw-r--r-- 1 root mail 0 11-30 08:39 log2016.log-rw-r--r-- 1 root mail 0 11-30 08:39 log2017.log 参考链接： http://www.cnblogs.com/peida/archive/2012/12/04/2800684.html http://man.linuxde.net/chown]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-chgrp命令]]></title>
    <url>%2F2018%2F09%2F28%2FLinux%E5%91%BD%E4%BB%A4-chgrp%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[chgrp命令用来改变文件或目录所属的用户组。该命令用来改变指定文件所属的用户组。其中，组名可以是用户组的id，也可以是用户组的组名。文件名可以 是由空格分开的要改变属组的文件列表，也可以是由通配符描述的文件集合。如果用户不是该文件的文件主或超级用户(root)，则不能改变该文件的组。 在UNIX系统家族里，文件或目录权限的掌控以拥有者及所属群组来管理。您可以使用chgrp指令去变更文件与目录的所属群组，设置方式采用群组名称或群组识别码皆可。 语法chgrp(选项)(参数) 选项123456-c或——changes：效果类似“-v”参数，但仅回报更改的部分；-f或--quiet或——silent：不显示错误信息；-h或--no-dereference：只对符号连接的文件作修改，而不是该其他任何相关文件；-R或——recursive：递归处理，将指令目录下的所有文件及子目录一并处理；-v或——verbose：显示指令执行过程；--reference=&lt;参考文件或目录&gt;：把指定文件或目录的所属群组全部设成和参考文件或目录的所属群组相同； 参数组：指定新工作名称 文件： 指定要改变所属组的文件 常用实例将/usr/meng及其子目录下的所有文件的用户组改为mengxin 1chgrp -R mengxin /usr/meng 转载地址： http://man.linuxde.net/chgrp]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-tar命令]]></title>
    <url>%2F2018%2F09%2F26%2FLinux%E5%91%BD%E4%BB%A4-tar%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[tar命令可以为linux的文件和目录创建档案。利用tar，可以为某一特定文件创建档案（备份文件），也可以在档案中改变文件，或者向档案中加入新的文件。tar最初被用来在磁带上创建档案，现在，用户可以在任何设备上创建档案。利用tar命令，可以把一大堆的文件和目录全部打包成一个文件，这对于备份文件或将几个文件组合成为一个文件以便于网络传输是非常有用的。 首先要弄清两个概念：打包和压缩。打包时指将一大堆文件或目录变成为一个总的文件；压缩则是将一个大的文件通过一些压缩算法变成一个小文件。 为什么要区分这两个概念呢？这源于Linux中很多压缩程序只能针对一个文件进行压缩，这样当你想要压缩一大堆文件时，你得先将这一大堆文件先打成一个包（tar命令），然后再用压缩程序进行压缩（gzip bzip2命令）。 语法tar(选项)(参数) 选项1234567891011121314151617181920212223-A或--catenate：新增文件到以存在的备份文件；-B：设置区块大小；-c或--create：建立新的备份文件；-C &lt;目录&gt;：这个选项用在解压缩，若要在特定目录解压缩，可以使用这个选项。-d：记录文件的差别；-x或--extract或--get：从备份文件中还原文件；-t或--list：列出备份文件的内容；-z或--gzip或--ungzip：通过gzip指令处理备份文件；-Z或--compress或--uncompress：通过compress指令处理备份文件；-f&lt;备份文件&gt;或--file=&lt;备份文件&gt;：指定备份文件；-v或--verbose：显示指令执行过程；-r：添加文件到已经压缩的文件；-u：添加改变了和现有的文件到已经存在的压缩文件；-j：支持bzip2解压文件；-v：显示操作过程；-l：文件系统边界设置；-k：保留原有文件不覆盖；-m：保留文件不被覆盖；-w：确认压缩文件的正确性；-p或--same-permissions：用原来的文件权限还原文件；-P或--absolute-names：文件名使用绝对名称，不移除文件名称前的“/”号；-N &lt;日期格式&gt; 或 --newer=&lt;日期时间&gt;：只将较指定日期更新的文件保存到备份文件里；--exclude=&lt;范本样式&gt;：排除符合范本样式的文件。 参数文件或目录：指定要打包的文件或目录列表。 常用实例1）将文件全部打包成tar包： 123tar -cvf log.tar log2012.log 仅打包，不压缩！ tar -zcvf log.tar.gz log2012.log 打包后，以 gzip 压缩 tar -jcvf log.tar.bz2 log2012.log 打包后，以 bzip2 压缩 在选项f之后的文件档名是自己取的，我们习惯上都用 .tar 来作为辨识。 如果加z选项，则以.tar.gz或.tgz来代表gzip压缩过的tar包；如果加j选项，则以.tar.bz2来作为tar包名。 2）查阅上述tar包内有哪些文件： 1tar -ztvf log.tar.gz 由于我们使用 gzip 压缩的log.tar.gz，所以要查阅log.tar.gz包内的文件时，就得要加上z这个选项了。 3）将tar包解压缩： 1tar -zxvf /opt/soft/test/log.tar.gz 在预设的情况下，我们可以将压缩档在任何地方解开的 4）只将tar内的部分文件解压出来： 1tar -zxvf /opt/soft/test/log30.tar.gz log2013.log 5）文件备份下来，并且保存其权限： 1tar -zcvpf log31.tar.gz log2014.log log2015.log log2016.log 这个-p的属性是很重要的，尤其是当您要保留原本文件的属性时。 6）在文件夹当中，比某个日期新的文件才备份： 1tar -N &quot;2012/11/13&quot; -zcvf log17.tar.gz test 7）备份文件夹内容是排除部分文件： 1tar --exclude scf/service -zcvf scf.tar.gz scf/* 8）其实最简单的使用 tar 就只要记忆底下的方式即可： 123压 缩：tar -jcv -f filename.tar.bz2 要被压缩的文件或目录名称查 询：tar -jtv -f filename.tar.bz2解压缩：tar -jxv -f filename.tar.bz2 -C 欲解压缩的目录 转载链接： http://man.linuxde.net/tar]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-chmod命令]]></title>
    <url>%2F2018%2F09%2F26%2FLinux%E5%91%BD%E4%BB%A4-chmod%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[chmod命令用于改变linux系统文件或目录的访问权限。用它控制文件或目录的访问权限。该命令有两种用法。一种是包含字母和操作符表达式的文字设定法；另一种是包含数字的数字设定法。 权限范围的表示法如下： u User，即文件或目录的拥有者；g Group，即文件或目录的所属群组；o Other，除了文件或目录拥有者或所属群组之外，其他用户皆属于这个范围；a All，即全部的用户，包含拥有者，所属群组以及其他用户；r 读取权限，数字代号为“4”;w 写入权限，数字代号为“2”；x 执行或切换权限，数字代号为“1”；- 不具任何权限，数字代号为“0”；s 特殊功能说明：变更文件或目录的权限。 语法chmod(选项)(参数) 选项1234-c或——changes：效果类似“-v”参数，但仅回报更改的部分；-f或--quiet或——silent：不显示错误信息；-R或——recursive：递归处理，将指令目录下的所有文件及子目录一并处理；-v或——verbose：显示指令执行过程； 参数权限模式：指定文件的权限模式；文件：要改变权限的文件。 知识扩展和实例Linux用 户分为：拥有者、组群(Group)、其他（other），Linux系统中，预设的情況下，系统中所有的帐号与一般身份使用者，以及root的相关信 息， 都是记录在/etc/passwd文件中。每个人的密码则是记录在/etc/shadow文件下。 此外，所有的组群名称记录在/etc/group內！ linux文件的用户权限的分析图 例：rwx rw- r– r=读取属性 //值＝4w=写入属性 //值＝2x=执行属性 //值＝1 1234chmod u+x,g+w f01 //为文件f01设置自己可以执行，组员可以写入的权限chmod u=rwx,g=rw,o=r f01chmod 764 f01chmod a+x f01 //对文件f01的u,g,o都设置可执行属性 文件的属主和属组属性设置 12chown user:market f01 //把文件f01给uesr，添加到market组ll -d f1 查看目录f1的属性 参考链接： http://man.linuxde.net/chmod http://www.cnblogs.com/peida/archive/2012/11/29/2794010.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux文件属性详解]]></title>
    <url>%2F2018%2F09%2F26%2FLinux%E6%96%87%E4%BB%B6%E5%B1%9E%E6%80%A7%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Linux 文件或目录的属性主要包括：文件或目录的节点、种类、权限模式、链接数量、所归属的用户和用户组、最近访问或修改的时间等内容。具体情况如下： 123456789101112# ls -lih总计 316K2095120 lrwxrwxrwx 1 root root 11 11-22 06:58 linklog.log -&gt; log2012.log2095112 -rw-r--r-- 1 root root 296K 11-13 06:03 log2012.log2095110 -rw-r--r-- 1 root root 61 11-13 06:03 log2013.log2095107 -rw-r--r-- 1 root root 0 11-13 06:03 log2014.log2095117 -rw-r--r-- 1 root root 0 11-13 06:06 log2015.log2095118 -rw-r--r-- 1 root root 0 11-16 14:41 log2016.log2095119 -rw-r--r-- 1 root root 0 11-16 14:43 log2017.log2095113 drwxr-xr-x 6 root root 4.0K 10-27 01:58 scf2095109 drwxrwxr-x 2 root root 4.0K 11-13 06:08 test32095131 drwxrwxr-x 2 root root 4.0K 11-13 05:50 test4 说明： 第一列：inode 第二列：文件种类和权限； 第三列： 硬链接个数； 第四列： 属主； 第五列：所归属的组； 第六列：文件或目录的大小； 第七列和第八列：最后访问或修改时间； 第九列：文件名或目录名 我们以log2012.log为例： 2095112 -rw-r–r– 1 root root 296K 11-13 06:03 log2012.log inode 的值是：2095112 文件类型：文件类型是-，表示这是一个普通文件； 关于文件的类型，请参考：每天一个linux命令(24)：Linux文件类型与扩展名 文件权限：文件权限是rw-r–r– ，表示文件属主可读、可写、不可执行，文件所归属的用户组不可写，可读，不可执行，其它用户不可写，可读，不可执行； 硬链接个数： log2012.log这个文件没有硬链接；因为数值是1，就是他本身； 文件属主：也就是这个文件归哪于哪个用户 ，它归于root，也就是第一个root； 文件属组：也就是说，对于这个文件，它归属于哪个用户组，在这里是root用户组； 文件大小：文件大小是296k个字节； 访问可修改时间 ：这里的时间是最后访问的时间，最后访问和文件被修改或创建的时间，有时并不是一致的； 当然文档的属性不仅仅包括这些，这些是我们最常用的一些属性。 关于inode： inode 译成中文就是索引节点。每个存储设备或存储设备的分区（存储设备是硬盘、软盘、U盘等等）被格式化为文件系统后，应该有两部份，一部份是inode，另一部份是Block，Block是用来存储数据用的。而inode呢，就是用来存储这些数 据的信息，这些信息包括文件大小、属主、归属的用户组、读写权限等。inode为每个文件进行信息索引，所以就有了inode的数值。操作系统根据指令， 能通过inode值最快的找到相对应的文件。 做个比喻，比如一本书，存储设备或分区就相当于这本书，Block相当于书中的每一页，inode 就相当于这本书前面的目录，一本书有很多的内容，如果想查找某部份的内容，我们可以先查目录，通过目录能最快的找到我们想要看的内容。虽然不太恰当，但还是比较形象。 当我们用ls 查看某个目录或文件时，如果加上-i 参数，就可以看到inode节点了；比如我们前面所说的例子： 12ls -li log2012.log# ls -li log2012.log 2095112 -rw-r--r-- 1 root root 302108 11-13 06:03 log2012.log log2012.log 的inode值是 2095112 ； 查看一个文件或目录的inode，要通过ls 命令的的 -i参数。 转载链接： http://www.cnblogs.com/peida/archive/2012/11/23/2783762.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux文件类型与拓展名]]></title>
    <url>%2F2018%2F09%2F26%2FLinux%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B%E4%B8%8E%E6%8B%93%E5%B1%95%E5%90%8D%2F</url>
    <content type="text"><![CDATA[Linux文件类型和Linux文件的文件名所代表的意义是两个不同的概念。我们通过一般应用程序而创建的比如file.txt、file.tar.gz ，这些文件虽然要用不同的程序来打开，但放在Linux文件类型中衡量的话，大多是常规文件（也被称为普通文件）。 文件类型Linux文件类型常见的有：普通文件、目录文件、字符设备文件和块设备文件、符号链接文件等，现在我们进行一个简要的说明。 普通文件我们用 ls -lh 来查看某个文件的属性，可以看到有类似-rwxrwxrwx，值得注意的是第一个符号是 - ，这样的文件在Linux中就是普通文件。这些文件一般是用一些相关的应用程序创建，比如图像工具、文档工具、归档工具… …. 或 cp工具等。这类文件的删除方式是用rm 命令。 另外，依照文件的内容，又大略可以分为： 1）纯文本档（ASCII）： 这是linux系统中最多的一种文件类型，称为纯文本档是因为人类可以直接读到的数据，例如数字、字母等等。几乎只要我们可以用来做为设定的文件都属于这一种文件类型。举例来说，你可以用命令： cat ~/.bashrc 来看到该文件内容。（cat是将一个文件内容读出来的指令）。 2） 二进制文件(binary)： Linux系统其实仅认识且可以执行二进制文件(binary file)。Linux当中的可执行文件(scripts, 文字型批处理文件不算)就是这种格式的文件。 刚刚使用的命令cat就是一个binary file。 3）数据格式文件(data)： 有些程序在运作的过程当中会读取某些特定格式的文件，那些特定格式的文件可以被称为数据文件 (data file)。举例来说，我们的Linux在使用者登录时，都会将登录的数据记录在 /var/log/wtmp那个文件内，该文件是一个data file，他能够透过last这个指令读出来！ 但是使用cat时，会读出乱码～因为他是属于一种特殊格式的文件？ 目录文件当我们在某个目录下执行，看到有类似 drwxr-xr-x ，这样的文件就是目录，目录在Linux是一个比较特殊的文件。注意它的第一个字符是d。创建目录的命令可以用 mkdir 命令，或cp命令，cp可以把一个目录复制为另一个目录。删除用rm 或rmdir命令。 字符设备或块设备文件当您进入/dev目录，列一下文件，会看到类似如下的: 1234# ls -al /dev/ttycrw-rw-rw- 1 root tty 5, 0 11-03 15:11 /dev/tty# ls -la /dev/sda1brw-r----- 1 root disk 8, 1 11-03 07:11 /dev/sda1 我们看到/dev/tty的属性是 crw-rw-rw- ，注意前面第一个字符是 c ，这表示字符设备文件。比如猫等串口设备。我们看到 /dev/sda1 的属性是 brw-r—– ，注意前面的第一个字符是b，这表示块设备，比如硬盘，光驱等设备。 这个种类的文件，是用mknode来创建，用rm来删除。目前在最新的Linux发行版本中，我们一般不用自己来创建设备文件。因为这些文件是和内核相关联的。 与系统周边及储存等相关的一些文件， 通常都集中在/dev这个目录之下！通常又分为两种： 区块(block)设备档 ： 就是一些储存数据， 以提供系统随机存取的接口设备，举例来说，硬盘与软盘等就是啦！ 你可以随机的在硬盘的不同区块读写，这种装置就是成组设备！你可以自行查一下/dev/sda看看， 会发现第一个属性为[ b ]！ 字符(character)设备文件： 亦即是一些串行端口的接口设备， 例如键盘、鼠标等等！这些设备的特色就是一次性读取的，不能够截断输出。 举例来说，你不可能让鼠标跳到另一个画面，而是滑动到另一个地方！第一个属性为 [ c ]。 数据接口文件(sockets)：数据接口文件（或者：套接口文件），这种类型的文件通常被用在网络上的数据承接了。我们可以启动一个程序来监听客户端的要求， 而客户端就可以透过这个socket来进行数据的沟通了。第一个属性为 [ s ]， 最常在/var/run这个目录中看到这种文件类型了。 例如：当我们启动MySQL服务器时，会产生一个mysql.sock的文件。 12# ls -lh /var/lib/mysql/mysql.sock srwxrwxrwx 1 mysql mysql 0 04-19 11:12 /var/lib/mysql/mysql.sock 注意这个文件的属性的第一个字符是 s。 符号链接文件：当我们查看文件属性时，会看到有类似 lrwxrwxrwx,注意第一个字符是l，这类文件是链接文件。是通过ln -s 源文件名 新文件名 。上面是一个例子，表示setup.log是install.log的软链接文件。怎么理解呢？这和Windows操作系统中的快捷方式有点相似。 符号链接文件的创建方法举例: 123456# ls -lh log2012.log-rw-r--r-- 1 root root 296K 11-13 06:03 log2012.log# ln -s log2012.log linklog.log# ls -lh *.loglrwxrwxrwx 1 root root 11 11-22 06:58 linklog.log -&gt; log2012.log-rw-r--r-- 1 root root 296K 11-13 06:03 log2012.log 数据输送文件（FIFO,pipe）:FIFO也是一种特殊的文件类型，他主要的目的在解决多个程序同时存取一个文件所造成的错误问题。 FIFO是first-in-first-out的缩写。第一个属性为[p] 。 Linux文件扩展名扩展名类型基本上，Linux的文件是没有所谓的扩展名的，一个Linux文件能不能被执行，与他的第一栏的十个属性有关， 与档名根本一点关系也没有。这个观念跟Windows的情况不相同喔！在Windows底下， 能被执行的文件扩展名通常是 .com .exe .bat等等，而在Linux底下，只要你的权限当中具有x的话，例如[ -rwx-r-xr-x ] 即代表这个文件可以被执行。 不过，可以被执行跟可以执行成功是不一样的～举例来说，在root家目录下的install.log 是一个纯文本档，如果经由修改权限成为 -rwxrwxrwx 后，这个文件能够真的执行成功吗？ 当然不行～因为他的内容根本就没有可以执行的数据。所以说，这个x代表这个文件具有可执行的能力， 但是能不能执行成功，当然就得要看该文件的内容. 虽然如此，不过我们仍然希望可以藉由扩展名来了解该文件是什么东西，所以，通常我们还是会以适当的扩展名来表示该文件是什么种类的。底下有数种常用的扩展名： *.sh ： 脚本或批处理文件 (scripts)，因为批处理文件为使用shell写成的，所以扩展名就编成 .sh Z, .tar, .tar.gz, .zip, *.tgz： 经过打包的压缩文件。这是因为压缩软件分别为 gunzip, tar 等等的，由于不同的压缩软件，而取其相关的扩展名！ .html, .php：网页相关文件，分别代表 HTML 语法与 PHP 语法的网页文件。 .html 的文件可使用网页浏览器来直接开启，至于 .php 的文件， 则可以透过 client 端的浏览器来 server 端浏览，以得到运算后的网页结果。 基本上，Linux系统上的文件名真的只是让你了解该文件可能的用途而已，真正的执行与否仍然需要权限的规范才行。例如虽然有一个文件为可执行文件，如常见的/bin/ls这个显示文件属性的指令，不过，如果这个文件的权限被修改成无法执行时，那么ls就变成不能执行。 上述的这种问题最常发生在文件传送的过程中。例如你在网络上下载一个可执行文件，但是偏偏在你的 Linux系统中就是无法执行！呵呵！那么就是可能文件的属性被改变了。不要怀疑，从网络上传送到你的 Linux系统中，文件的属性与权限确实是会被改变的。 Linux文件名长度限制：在Linux底下，使用预设的Ext2/Ext3文件系统时，针对文件名长度限制为： 单一文件或目录的最大容许文件名为 255 个字符 包含完整路径名称及目录 (/) 之完整档名为 4096 个字符 是相当长的档名！我们希望Linux的文件名可以一看就知道该文件在干嘛的， 所以档名通常是很长很长。 Linux文件名的字符的限制：由于Linux在文字接口下的一些指令操作关系，一般来说，你在设定Linux底下的文件名时， 最好可以避免一些特殊字符比较好！例如底下这些： ? &gt; &lt; ; &amp; ! [ ] | \ ‘ “ ` ( ) { } 因为这些符号在文字接口下，是有特殊意义的。另外，文件名的开头为小数点“.”时， 代表这个文件为隐藏文件！同时，由于指令下达当中，常常会使用到 -option 之类的选项， 所以你最好也避免将文件档名的开头以 - 或 + 来命名。 转载链接： http://www.cnblogs.com/peida/archive/2012/11/22/2781912.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux目录结构]]></title>
    <url>%2F2018%2F09%2F26%2FLinux%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[对于每一个Linux学习者来说，了解Linux文件系统的目录结构，是学好Linux的至关重要的一步.，深入了解linux文件目录结构的标准和每个目录的详细功能，对于我们用好linux系统只管重要，下面我们就开始了解一下linux目录结构的相关知识。 当在使用Linux的时候，如果您通过ls –l / 就会发现，在/下包涵很多的目录，比如etc、usr、var、bin … … 等目录，而在这些目录中，我们进去看看，发现也有很多的目录或文件。文件系统在Linux下看上去就象树形结构，所以我们可以把文件系统的结构形象的称为 树形结构。 文件系统的是用来组织和排列文件存取的，所以它是可见的，在Linux中，我们可以通过ls等工具来查看其结构，在Linux系统中，我们见到的都是树形结构；比如操作系统安装在一个文件系统中，他表现为由/ 起始的树形结构。linux文件系统的最顶端是/，我们称/为Linux的root，也就是 Linux操作系统的文件系统。Linux的文件系统的入口就是/，所有的目录、文件、设备都在/之下，/就是Linux文件系统的组织者，也是最上级的领导者。 由于linux是开放源代码，各大公司和团体根据linux的核心代码做各自的操作，编程。这样就造成在根下的目录的不同。这样就造成个人不能使用他人的linux系统的PC。因为你根本不知道一些基本的配置，文件在哪里。。。这就造成了混乱。这就是FHS（Filesystem Hierarchy Standard ）机构诞生的原因。该机构是linux爱好者自发的组成的一个团体，主要是是对linux做一些基本的要求，不至于是操作者换一台主机就成了linux的‘文盲’。 根据FHS(http://www.pathname.com/fhs/)的官方文件指出，) 他们的主要目的是希望让使用者可以了解到已安装软件通常放置于那个目录下， 所以他们希望独立的软件开发商、操作系统制作者、以及想要维护系统的用户，都能够遵循FHS的标准。 也就是说，FHS的重点在于规范每个特定的目录下应该要放置什么样子的数据而已。 这样做好处非常多，因为Linux操作系统就能够在既有的面貌下(目录架构不变)发展出开发者想要的独特风格。 事实上，FHS是根据过去的经验一直再持续的改版的，FHS依据文件系统使用的频繁与否与是否允许使用者随意更动， 而将目录定义成为四种交互作用的形态，用表格来说有点像底下这样： 可分享的(shareable) 不可分享的(unshareable) 不变的(static) /usr (软件放置处) /etc (配置文件) /opt (第三方协力软件) /boot (开机与核心档) 可变动的(variable) /var/mail (使用者邮件信箱) /var/run (程序相关) /var/spool/news (新闻组) /var/lock (程序相关) 四中类型: 可分享的： 可以分享给其他系统挂载使用的目录，所以包括执行文件与用户的邮件等数据， 是能够分享给网络上其他主机挂载用的目录； 不可分享的： 自己机器上面运作的装置文件或者是与程序有关的socket文件等， 由于仅与自身机器有关，所以当然就不适合分享给其他主机了。 不变的： 有些数据是不会经常变动的，跟随着distribution而不变动。 例如函式库、文件说明文件、系统管理员所管理的主机服务配置文件等等； 可变动的： 经常改变的数据，例如登录文件、一般用户可自行收受的新闻组等。 事实上，FHS针对目录树架构仅定义出三层目录底下应该放置什么数据而已，分别是底下这三个目录的定义： / (root, 根目录)：与开机系统有关； /usr (unix software resource)：与软件安装/执行有关； /var (variable)：与系统运作过程有关。 根目录 (/) 的意义与内容：根目录是整个系统最重要的一个目录，因为不但所有的目录都是由根目录衍生出来的， 同时根目录也与开机/还原/系统修复等动作有关。 由于系统开机时需要特定的开机软件、核心文件、开机所需程序、 函式库等等文件数据，若系统出现错误时，根目录也必须要包含有能够修复文件系统的程序才行。 因为根目录是这么的重要，所以在FHS的要求方面，他希望根目录不要放在非常大的分区， 因为越大的分区内你会放入越多的数据，如此一来根目录所在分区就可能会有较多发生错误的机会。 因此FHS标准建议：根目录(/)所在分区应该越小越好， 且应用程序所安装的软件最好不要与根目录放在同一个分区内，保持根目录越小越好。 如此不但效能较佳，根目录所在的文件系统也较不容易发生问题。说白了，就是根目录和Windows的C盘一个样。 根据以上原因，FHS认为根目录(/)下应该包含如下子目录： 目录 应放置档案内容 /bin 系统有很多放置执行档的目录，但/bin比较特殊。因为/bin放置的是在单人维护模式下还能够被操作的指令。在/bin底下的指令可以被root与一般帐号所使用，主要有：cat,chmod(修改权限), chown, date, mv, mkdir, cp, bash等等常用的指令。 /boot 主要放置开机会使用到的档案，包括Linux核心档案以及开机选单与开机所需设定档等等。Linux kernel常用的档名为：vmlinuz ，如果使用的是grub这个开机管理程式，则还会存在/boot/grub/这个目录。 /dev 在Linux系统上，任何装置与周边设备都是以档案的型态存在于这个目录当中。 只要通过存取这个目录下的某个档案，就等于存取某个装置。比要重要的档案有/dev/null, /dev/zero, /dev/tty , /dev/lp, / dev/hd, /dev/sd*等等 /etc 系统主要的设定档几乎都放置在这个目录内，例如人员的帐号密码档、各种服务的启始档等等。 一般来说，这个目录下的各档案属性是可以让一般使用者查阅的，但是只有root有权力修改。 FHS建议不要放置可执行档(binary)在这个目录中。 比较重要的档案有：/etc/inittab, /etc/init.d/, /etc/modprobe.conf, /etc/X11/, /etc/fstab, /etc/sysconfig/等等。 另外，其下重要的目录有：/etc/init.d/ ：所有服务的预设启动script都是放在这里的，例如要启动或者关闭iptables的话： /etc/init.d/iptables start、/etc/init.d/ iptables stop/etc/xinetd.d/ ：这就是所谓的super daemon管理的各项服务的设定档目录。/etc/X11/ ：与X Window有关的各种设定档都在这里，尤其是xorg.conf或XF86Config这两个X Server的设定档。 /home 这是系统预设的使用者家目录(home directory)。 在你新增一个一般使用者帐号时，预设的使用者家目录都会规范到这里来。比较重要的是，家目录有两种代号： ~ ：代表当前使用者的家目录，而 ~guest：则代表用户名为guest的家目录。 /lib 系统的函式库非常的多，而/lib放置的则是在开机时会用到的函式库，以及在/bin或/sbin底下的指令会呼叫的函式库而已 。 什么是函式库呢？妳可以将他想成是外挂，某些指令必须要有这些外挂才能够顺利完成程式的执行之意。 尤其重要的是/lib/modules/这个目录，因为该目录会放置核心相关的模组(驱动程式)。 /media media是媒体的英文，顾名思义，这个/media底下放置的就是可移除的装置。 包括软碟、光碟、DVD等等装置都暂时挂载于此。 常见的档名有：/media/floppy, /media/cdrom等等。 /mnt 如果妳想要暂时挂载某些额外的装置，一般建议妳可以放置到这个目录中。在古早时候，这个目录的用途与/media相同啦。 只是有了/media之后，这个目录就用来暂时挂载用了。 /opt 这个是给第三方协力软体放置的目录 。 什么是第三方协力软体啊？举例来说，KDE这个桌面管理系统是一个独立的计画，不过他可以安装到Linux系统中，因此KDE的软体就建议放置到此目录下了。 另外，如果妳想要自行安装额外的软体(非原本的distribution提供的)，那么也能够将你的软体安装到这里来。 不过，以前的Linux系统中，我们还是习惯放置在/usr/local目录下。 /root 系统管理员(root)的家目录。 之所以放在这里，是因为如果进入单人维护模式而仅挂载根目录时，该目录就能够拥有root的家目录，所以我们会希望root的家目录与根目录放置在同一个分区中。 /sbin Linux有非常多指令是用来设定系统环境的，这些指令只有root才能够利用来设定系统，其他使用者最多只能用来查询而已。放在/sbin底下的为开机过程中所需要的，里面包括了开机、修复、还原系统所需要的指令。至于某些伺服器软体程式，一般则放置到/usr/sbin/当中。至于本机自行安装的软体所产生的系统执行档(system binary)，则放置到/usr/local/sbin/当中了。常见的指令包括：fdisk, fsck, ifconfig, init, mkfs等等。 /srv srv可以视为service的缩写，是一些网路服务启动之后，这些服务所需要取用的资料目录。 常见的服务例如WWW, FTP等等。 举例来说，WWW伺服器需要的网页资料就可以放置在/srv/www/里面。呵呵，看来平时我们编写的代码应该放到这里了。 /tmp 这是让一般使用者或者是正在执行的程序暂时放置档案的地方。这个目录是任何人都能够存取的，所以你需要定期的清理一下。当然，重要资料不可放置在此目录啊。 因为FHS甚至建议在开机时，应该要将/tmp下的资料都删除。 事实上FHS针对根目录所定义的标准就仅限于上表，不过仍旧有些目录也需要我们了解一下，具体如下： 目录 应放置文件内容 /lost+found 这个目录是使用标准的ext2/ext3档案系统格式才会产生的一个目录，目的在于当档案系统发生错误时，将一些遗失的片段放置到这个目录下。 这个目录通常会在分割槽的最顶层存在，例如你加装一个硬盘于/disk中，那在这个系统下就会自动产生一个这样的目录/disk/lost+found /proc 这个目录本身是一个虚拟文件系统(virtual filesystem)喔。 他放置的资料都是在内存当中，例如系统核心、行程资讯(process)（是进程吗?）、周边装置的状态及网络状态等等。因为这个目录下的资料都是在记忆体（内存）当中，所以本身不占任何硬盘空间。比较重要的档案（目录）例如： /proc/cpuinfo, /proc/dma, /proc/interrupts, /proc/ioports, /proc/net/*等等。呵呵，是虚拟内存吗[guest]？ /sys 这个目录其实跟/proc非常类似，也是一个虚拟的档案系统，主要也是记录与核心相关的资讯。 包括目前已载入的核心模组与核心侦测到的硬体装置资讯等等。 这个目录同样不占硬盘容量。 除了这些目录的内容之外，另外要注意的是，因为根目录与开机有关，开机过程中仅有根目录会被挂载， 其他分区则是在开机完成之后才会持续的进行挂载的行为。就是因为如此，因此根目录下与开机过程有关的目录， 就不能够与根目录放到不同的分区去。那哪些目录不可与根目录分开呢？有底下这些： /etc：配置文件 /bin：重要执行档 /dev：所需要的装置文件 /lib：执行档所需的函式库与核心所需的模块 /sbin：重要的系统执行文件 这五个目录千万不可与根目录分开在不同的分区。请背下来啊。 /usr 的意义与内容：依据FHS的基本定义，/usr里面放置的数据属于可分享的与不可变动的(shareable, static)， 如果你知道如何透过网络进行分区的挂载(例如在服务器篇会谈到的NFS服务器)，那么/usr确实可以分享给局域网络内的其他主机来使用喔。 /usr不是user的缩写，其实usr是Unix Software Resource的缩写， 也就是Unix操作系统软件资源所放置的目录，而不是用户的数据啦。这点要注意。 FHS建议所有软件开发者，应该将他们的数据合理的分别放置到这个目录下的次目录，而不要自行建立该软件自己独立的目录。 因为是所有系统默认的软件(distribution发布者提供的软件)都会放置到/usr底下，因此这个目录有点类似Windows 系统的C:\Windows\ + C:\Program files\这两个目录的综合体，系统刚安装完毕时，这个目录会占用最多的硬盘容量。 一般来说，/usr的次目录建议有底下这些： 目录 应放置文件内容 /usr/X11R6/ 为X Window System重要数据所放置的目录，之所以取名为X11R6是因为最后的X版本为第11版，且该版的第6次释出之意。 /usr/bin/ 绝大部分的用户可使用指令都放在这里。请注意到他与/bin的不同之处。(是否与开机过程有关) /usr/include/ c/c++等程序语言的档头(header)与包含档(include)放置处，当我们以tarball方式 (*.tar.gz 的方式安装软件)安装某些数据时，会使用到里头的许多包含档。 /usr/lib/ 包含各应用软件的函式库、目标文件(object file)，以及不被一般使用者惯用的执行档或脚本(script)。 某些软件会提供一些特殊的指令来进行服务器的设定，这些指令也不会经常被系统管理员操作， 那就会被摆放到这个目录下啦。要注意的是，如果你使用的是X86_64的Linux系统， 那可能会有/usr/lib64/目录产生 /usr/local/ 统管理员在本机自行安装自己下载的软件(非distribution默认提供者)，建议安装到此目录， 这样会比较便于管理。举例来说，你的distribution提供的软件较旧，你想安装较新的软件但又不想移除旧版， 此时你可以将新版软件安装于/usr/local/目录下，可与原先的旧版软件有分别啦。 你可以自行到/usr/local去看看，该目录下也是具有bin, etc, include, lib…的次目录 /usr/sbin/ 非系统正常运作所需要的系统指令。最常见的就是某些网络服务器软件的服务指令(daemon) /usr/share/ 放置共享文件的地方，在这个目录下放置的数据几乎是不分硬件架构均可读取的数据， 因为几乎都是文本文件嘛。在此目录下常见的还有这些次目录：/usr/share/man：联机帮助文件/usr/share/doc：软件杂项的文件说明/usr/share/zoneinfo：与时区有关的时区文件 /usr/src/ 一般原始码建议放置到这里，src有source的意思。至于核心原始码则建议放置到/usr/src/linux/目录下。 /var 的意义与内容：如果/usr是安装时会占用较大硬盘容量的目录，那么/var就是在系统运作后才会渐渐占用硬盘容量的目录。 因为/var目录主要针对常态性变动的文件，包括缓存(cache)、登录档(log file)以及某些软件运作所产生的文件， 包括程序文件(lock file, run file)，或者例如MySQL数据库的文件等等。常见的次目录有： 目录 应放置文件内容 /var/cache/ 应用程序本身运作过程中会产生的一些暂存档 /var/lib/ 程序本身执行的过程中，需要使用到的数据文件放置的目录。在此目录下各自的软件应该要有各自的目录。 举例来说，MySQL的数据库放置到/var/lib/mysql/而rpm的数据库则放到/var/lib/rpm去 /var/lock/ 某些装置或者是文件资源一次只能被一个应用程序所使用，如果同时有两个程序使用该装置时， 就可能产生一些错误的状况，因此就得要将该装置上锁(lock)，以确保该装置只会给单一软件所使用。 举例来说，刻录机正在刻录一块光盘，你想一下，会不会有两个人同时在使用一个刻录机烧片？ 如果两个人同时刻录，那片子写入的是谁的数据？所以当第一个人在刻录时该刻录机就会被上锁， 第二个人就得要该装置被解除锁定(就是前一个人用完了)才能够继续使用 /var/log/ 非常重要。这是登录文件放置的目录。里面比较重要的文件如/var/log/messages, /var/log/wtmp(记录登入者的信息)等。 /var/mail/ 放置个人电子邮件信箱的目录，不过这个目录也被放置到/var/spool/mail/目录中，通常这两个目录是互为链接文件。 /var/run/ 某些程序或者是服务启动后，会将他们的PID放置在这个目录下 /var/spool/ 这个目录通常放置一些队列数据，所谓的“队列”就是排队等待其他程序使用的数据。 这些数据被使用后通常都会被删除。举例来说，系统收到新信会放置到/var/spool/mail/中， 但使用者收下该信件后该封信原则上就会被删除。信件如果暂时寄不出去会被放到/var/spool/mqueue/中， 等到被送出后就被删除。如果是工作排程数据(crontab)，就会被放置到/var/spool/cron/目录中。 由于FHS仅是定义出最上层(/)及次层(/usr, /var)的目录内容应该要放置的文件或目录数据， 因此，在其他次目录层级内，就可以随开发者自行来配置了。 目录树(directory tree) :在Linux底下，所有的文件与目录都是由根目录开始的。那是所有目录与文件的源头, 然后再一个一个的分支下来，因此，我们也称这种目录配置方式为：目录树(directory tree), 这个目录树的主要特性有： 目录树的启始点为根目录 (/, root)； 每一个目录不止能使用本地端的 partition 的文件系统，也可以使用网络上的 filesystem 。举例来说， 可以利用 Network File System (NFS) 服务器挂载某特定目录等。 每一个文件在此目录树中的文件名(包含完整路径)都是独一无二的。 如果我们将整个目录树以图的方法来显示，并且将较为重要的文件数据列出来的话，那么目录树架构就如下图所示： 绝对路径与相对路径除了需要特别注意的FHS目录配置外，在文件名部分我们也要特别注意。因为根据档名写法的不同，也可将所谓的路径(path)定义为绝对路径(absolute)与相对路径(relative)。 这两种文件名/路径的写法依据是这样的： 绝对路径： 由根目录(/)开始写起的文件名或目录名称， 例如 /home/dmtsai/.bashrc； 相对路径： 相对于目前路径的文件名写法。 例如 ./home/dmtsai 或 http://www.cnblogs.com/home/dmtsai/ 等等。反正开头不是 / 就属于相对路径的写法 而你必须要了解，相对路径是以你当前所在路径的相对位置来表示的。举例来说，你目前在 /home 这个目录下， 如果想要进入 /var/log 这个目录时，可以怎么写呢？ cd /var/log (absolute) cd ../var/log (relative) 因为你在 /home 底下，所以要回到上一层 (../) 之后，才能继续往 /var 来移动的，特别注意这两个特殊的目录： . ：代表当前的目录，也可以使用 ./ 来表示； .. ：代表上一层目录，也可以 ../ 来代表。 这个 . 与 .. 目录概念是很重要的，你常常会看到 cd .. 或 ./command 之类的指令下达方式， 就是代表上一层与目前所在目录的工作状态。 实例1：如何先进入/var/spool/mail/目录，再进入到/var/spool/cron/目录内？ 命令： cd /var/spool/mail cd ../cron 说明： 由于/var/spool/mail与/var/spool/cron是同样在/var/spool/目录中。如此就不需要在由根目录开始写起了。这个相对路径是非常有帮助的，尤其对于某些软件开发商来说。 一般来说，软件开发商会将数据放置到/usr/local/里面的各相对目录。 但如果用户想要安装到不同目录呢？就得要使用相对路径。 实例2：网络文件常常提到类似./run.sh之类的数据，这个指令的意义为何？ 说明： 由于指令的执行需要变量的支持，若你的执行文件放置在本目录，并且本目录并非正规的执行文件目录(/bin, /usr/bin等为正规)，此时要执行指令就得要严格指定该执行档。./代表本目录的意思，所以./run.sh代表执行本目录下， 名为run.sh的文件。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-find参数详解]]></title>
    <url>%2F2018%2F09%2F23%2FLinux%E5%91%BD%E4%BB%A4-find%E5%8F%82%E6%95%B0%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[find一些常用参数的一些常用实例和一些具体用法和注意事项。 使用name选项：文件名选项是find命令最常用的选项，要么单独使用该选项，要么和其他选项一起使用。 可以使用某种文件名模式来匹配文件，记住要用引号将文件名模式引起来。 不管当前路径是什么，如果想要在自己的根目录$HOME中查找文件名符合*.log的文件，使用~作为 ‘pathname’参数，波浪号~代表了你的$HOME目录。 find ~ -name &quot;*.log&quot; -print 想要在当前目录及子目录中查找所有的‘ *.log‘文件，可以用： find . -name &quot;*.log&quot; -print 想要的当前目录及子目录中查找文件名以一个大写字母开头的文件，可以用： find . -name &quot;[A-Z]*&quot; -print 想要在/etc目录中查找文件名以host开头的文件，可以用： find /etc -name &quot;host*&quot; -print 想要查找$HOME目录中的文件，可以用： find ~ -name &quot;*&quot; -print 或find . -print 要想让系统高负荷运行，就从根目录开始查找所有的文件。 find / -name &quot;*&quot; -print 如果想在当前目录查找文件名以一个个小写字母开头，最后是4到9加上.log结束的文件： find . -name &quot;[a-z]*[4-9].log&quot; -print 用perm选项：按照文件权限模式用-perm选项,按文件权限模式来查找文件的话。最好使用八进制的权限表示法。 如在当前目录下查找文件权限位为755的文件，即文件属主可以读、写、执行，其他用户可以读、执行的文件，可以用： 12345# find . -perm 755 -print../scf./scf/lib./scf/service 还有一种表达方法：在八进制数字前面要加一个横杠-，表示都匹配，如-007就相当于777，-005相当于555, 忽略某个目录：如果在查找文件时希望忽略某个目录，因为你知道那个目录中没有你所要查找的文件，那么可以使用-prune选项来指出需要忽略的目录。在使用-prune选项时要当心，因为如果你同时使用了-depth选项，那么-prune选项就会被find命令忽略。如果希望在test目录下查找文件，但不希望在test/test3目录下查找，可以用： 12345# find test -path &quot;test/test3&quot; -prune -o -printtesttest/log2014.logtest/log2015.logtest/test4 说明： find [-path ..][expression] 在路径列表的后面的是表达式 -path “test” -prune -o -print 是 -path “test” -a -prune -o -print 的简写表达式按顺序求值, -a 和 -o 都是短路求值，与 shell 的 &amp;&amp; 和 || 类似如果 -path “test” 为真，则求值 -prune , -prune 返回真，与逻辑表达式为真；否则不求值 -prune，与逻辑表达式为假。如果 -path “test” -a -prune 为假，则求值 -print ，-print返回真，或逻辑表达式为真；否则不求值 -print，或逻辑表达式为真。 这个表达式组合特例可以用伪码写为: if -path “test” then -prune else -print 避开多个文件夹:123456# find test \( -path test/test4 -o -path test/test3 \) -prune -o -printtesttest/log2014.logtest/log2015.logtest/scftest/scf/lib 说明： 圆括号表示表达式的结合。 \ 表示引用，即指示 shell 不对后面的字符作特殊解释，而留给 find 命令去解释其意义。 使用user和nouser选项：按文件属主查找文件： 实例1：在$HOME目录中查找文件属主为peida的文件 1find ~ -user peida -print 实例2：在/etc目录下查找文件属主为peida的文件: 1find /etc -user peida -print 实例3：为了查找属主帐户已经被删除的文件，可以使用-nouser选项。在/home目录下查找所有的这类文件 1find/home -nouser -print 说明： 这样就能够找到那些属主在/etc/passwd文件中没有有效帐户的文件。在使用-nouser选项时，不必给出用户名； find命令能够为你完成相应的工作。 使用group和nogroup选项：就像user和nouser选项一样，针对文件所属于的用户组， find命令也具有同样的选项，为了在/apps目录下查找属于gem用户组的文件，可以用： 1find /apps -group gem -print 要查找没有有效所属用户组的所有文件，可以使用nogroup选项。下面的find命令从文件系统的根目录处查找这样的文件: 1find / -nogroup -print 按照更改时间或访问时间等查找文件：如果希望按照更改时间来查找文件，可以使用mtime,atime或ctime选项。如果系统突然没有可用空间了，很有可能某一个文件的长度在此期间增长迅速，这时就可以用mtime选项来查找这样的文件。 用减号-来限定更改时间在距今n日以内的文件，而用加号+来限定更改时间在距今n日以前的文件。 希望在系统根目录下查找更改时间在5日以内的文件，可以用： 1find / -mtime -5 -print 为了在/var/adm目录下查找更改时间在3日以前的文件，可以用: 1find /var/adm -mtime +3 -print 查找比某个文件新或旧的文件：如果希望查找更改时间比某个文件新但比另一个文件旧的所有文件，可以使用-newer选项。 它的一般形式为： newest_file_name ! oldest_file_name 其中，！是逻辑非符号。 1）查找更改时间比文件log2012.log新但比文件log2017.log旧的文件 1234567891011121314151617# ll总计 316-rw-r--r-- 1 root root 302108 11-13 06:03 log2012.log-rw-r--r-- 1 root root 61 11-13 06:03 log2013.log-rw-r--r-- 1 root root 0 11-13 06:03 log2014.log-rw-r--r-- 1 root root 0 11-13 06:06 log2015.log-rw-r--r-- 1 root root 0 11-16 14:41 log2016.log-rw-r--r-- 1 root root 0 11-16 14:43 log2017.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-13 06:08 test3drwxrwxr-x 2 root root 4096 11-13 05:50 test4# find -newer log2012.log ! -newer log2017.log../log2015.log./log2017.log./log2016.log./test3 2）查找更改时间在比log2012.log文件新的文件 123456 find -newer log2012.log../log2015.log./log2017.log./log2016.log./test3 使用type选项： 2）：在/etc目录下查找所有的目录 1find /etc -type d -print 2）：在当前目录下查找除目录以外的所有类型的文件 1find . ! -type d -print 3）：在/etc目录下查找所有的符号链接文件 1find /etc -type l -print 使用size选项：可以按照文件长度来查找文件，这里所指的文件长度既可以用块（block）来计量，也可以用字节来计量。以字节计量文件长度的表达形式为N c；以块计量文件长度只用数字表示即可。 在按照文件长度查找文件时，一般使用这种以字节表示的文件长度，在查看文件系统的大小，因为这时使用块来计量更容易转换。 1）：在当前目录下查找文件长度大于1 M字节的文件 1find . -size +1000000c -print 2）：在/home/apache目录下查找文件长度恰好为100字节的文件: 1find /home/apache -size 100c -print 3）：在当前目录下查找长度超过10块的文件（一块等于512字节） 1find . -size +10 -print 使用depth选项：在使用find命令时，可能希望先匹配所有的文件，再在子目录中查找。使用depth选项就可以使find命令这样做。这样做的一个原因就是，当在使用find命令向磁带上备份文件系统时，希望首先备份所有的文件，其次再备份子目录中的文件。 1)：find命令从文件系统的根目录开始，查找一个名为CON.FILE的文件。 1find / -name &quot;CON.FILE&quot; -depth -print 说明： 它将首先匹配所有的文件然后再进入子目录中查找 使用mount选项：在当前的文件系统中查找文件（不进入其他文件系统），可以使用find命令的mount选项。 1）：从当前目录开始查找位于本文件系统中文件名以XC结尾的文件 1find . -name &quot;*.XC&quot; -mount -print 转载链接： http://www.cnblogs.com/peida/archive/2012/11/16/2773289.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-find之xargs]]></title>
    <url>%2F2018%2F09%2F23%2FLinux%E5%91%BD%E4%BB%A4-find%E4%B9%8Bxargs%2F</url>
    <content type="text"><![CDATA[在使用 find命令的-exec选项处理匹配到的文件时， find命令将所有匹配到的文件一起传递给exec执行。但有些系统对能够传递给exec的命令长度有限制，这样在find命令运行几分钟之后，就会出现溢出错误。错误信息通常是“参数列太长”或“参数列溢出”。这就是xargs命令的用处所在，特别是与find命令一起使用。 find命令把匹配到的文件传递给xargs命令，而xargs命令每次只获取一部分文件而不是全部，不像-exec选项那样。这样它可以先处理最先获取的一部分文件，然后是下一批，并如此继续下去。 在有些系统中，使用-exec选项会为处理每一个匹配到的文件而发起一个相应的进程，并非将匹配到的文件全部作为参数一次执行；这样在有些情况下就会出现进程过多，系统性能下降的问题，因而效率不高； 而使用xargs命令则只有一个进程。另外，在使用xargs命令时，究竟是一次获取所有的参数，还是分批取得参数，以及每一次获取参数的数目都会根据该命令的选项及系统内核中相应的可调参数来确定。 使用实例： 1） 查找系统中的每一个普通文件，然后使用xargs命令来测试它们分别属于哪类文件 123456789101112#ll总计 312-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.log-rw-r--r-- 1 root root 0 11-12 22:25 log2013.log-rw-r--r-- 1 root root 0 11-12 22:25 log2014.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 11-12 19:32 test3drwxrwxrwx 2 root root 4096 11-12 19:32 test4# find . -type f -print | xargs file./log2014.log: empty./log2013.log: empty./log2012.log: ASCII text 2）在整个系统中查找内存信息转储文件(core dump) ，然后把结果保存到/tmp/core.log 文件中 12345678# find / -name &quot;core&quot; -print | xargs echo &quot;&quot; &gt;/tmp/core.log# cd /tmp# ll总计 16-rw-r--r-- 1 root root 1524 11-12 22:29 core.logdrwx------ 2 root root 4096 11-12 22:24 ssh-TzcZDx1766drwx------ 2 root root 4096 11-12 22:28 ssh-ykiRPk1815drwx------ 2 root root 4096 11-03 07:11 vmware-root 3）在当前目录下查找所有用户具有读、写和执行权限的文件，并收回相应的写权限 1234567891011121314151617# ll总计 312-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.log-rw-r--r-- 1 root root 0 11-12 22:25 log2013.log-rw-r--r-- 1 root root 0 11-12 22:25 log2014.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 11-12 19:32 test3drwxrwxrwx 2 root root 4096 11-12 19:32 test4# find . -perm -7 -print | xargs chmod o-w# ll总计 312-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.log-rw-r--r-- 1 root root 0 11-12 22:25 log2013.log-rw-r--r-- 1 root root 0 11-12 22:25 log2014.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-12 19:32 test3drwxrwxr-x 2 root root 4096 11-12 19:32 test4 说明： 执行命令后，文件夹scf、test3和test4的权限都发生改变 4）用grep命令在所有的普通文件中搜索hostname这个词 1234# find . -type f -print | xargs grep &quot;hostname&quot;./log2013.log:hostnamebaidu=baidu.com./log2013.log:hostnamesina=sina.com./log2013.log:hostnames=true 5）用grep命令在当前目录下的所有普通文件中搜索hostnames这个词 123# find . -name \* -type f -print | xargs grep &quot;hostnames&quot;./log2013.log:hostnamesina=sina.com./log2013.log:hostnames=true 说明： 注意，在上面的例子中， \用来取消find命令中的*在shell中的特殊含义。 6）使用xargs执行mv 12345678910111213141516171819202122# ll总计 316-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.log-rw-r--r-- 1 root root 61 11-12 22:44 log2013.log-rw-r--r-- 1 root root 0 11-12 22:25 log2014.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-12 22:54 test3drwxrwxr-x 2 root root 4096 11-12 19:32 test4# cd test4/# ll总计 0[root@localhost test4]# cd ..# find . -name &quot;*.log&quot; | xargs -i mv &#123;&#125; test4# ll总计 12drwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-13 05:50 test3drwxrwxr-x 2 root root 4096 11-13 05:50 test4# cd test4/# ll总计 304-rw-r--r-- 1 root root 302108 11-12 22:54 log2012.log-rw-r--r-- 1 root root 61 11-12 22:54 log2013.log-rw-r--r-- 1 root root 0 11-12 22:54 log2014.log 7）find后执行xargs提示xargs: argument line too long解决方法： 12#find . -type f -atime +0 -print0 | xargs -0 -l1 -t rm -frm -f 说明： -l1是一次处理一个；-t是处理之前打印出命令 8）使用-i参数默认的前面输出用{}代替，-I参数可以指定其他代替字符，如例子中的[] 1234567891011121314151617181920# ll总计 12drwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-13 05:50 test3drwxrwxr-x 2 root root 4096 11-13 05:50 test4# cd test4# find . -name &quot;file&quot; | xargs -I [] cp [] ..# ll总计 304-rw-r--r-- 1 root root 302108 11-12 22:54 log2012.log-rw-r--r-- 1 root root 61 11-12 22:54 log2013.log-rw-r--r-- 1 root root 0 11-12 22:54 log2014.log# cd ..# ll总计 316-rw-r--r-- 1 root root 302108 11-13 06:03 log2012.log-rw-r--r-- 1 root root 61 11-13 06:03 log2013.log-rw-r--r-- 1 root root 0 11-13 06:03 log2014.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-13 05:50 test3drwxrwxr-x 2 root root 4096 11-13 05:50 test4 说明： 使用-i参数默认的前面输出用{}代替，-I参数可以指定其他代替字符，如例子中的[] 9）xargs的-p参数的使用 123456789101112131415161718192021222324252627# ll总计 0-rw-r--r-- 1 root root 0 11-13 06:06 log2015.log# cd ..# ll总计 316-rw-r--r-- 1 root root 302108 11-13 06:03 log2012.log-rw-r--r-- 1 root root 61 11-13 06:03 log2013.log-rw-r--r-- 1 root root 0 11-13 06:03 log2014.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-13 06:06 test3drwxrwxr-x 2 root root 4096 11-13 05:50 test4# cd test3# find . -name &quot;*.log&quot; | xargs -p -i mv &#123;&#125; ..mv ./log2015.log .. ?...y# ll总计 0# cd ..# ll总计 316-rw-r--r-- 1 root root 302108 11-13 06:03 log2012.log-rw-r--r-- 1 root root 61 11-13 06:03 log2013.log-rw-r--r-- 1 root root 0 11-13 06:03 log2014.log-rw-r--r-- 1 root root 0 11-13 06:06 log2015.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-13 06:08 test3drwxrwxr-x 2 root root 4096 11-13 05:50 test4 说明： -p参数会提示让你确认是否执行后面的命令,y执行，n不执行。 转载链接： http://www.cnblogs.com/peida/archive/2012/11/15/2770888.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-find之exec]]></title>
    <url>%2F2018%2F09%2F23%2FLinux%E5%91%BD%E4%BB%A4-find%E4%B9%8Bexec%2F</url>
    <content type="text"><![CDATA[find是我们很常用的一个Linux命令，但是我们一般查找出来的并不仅仅是看看而已，还会有进一步的操作，这个时候exec的作用就显现出来了。 exec解释：-exec参数后面跟的是command命令，它的终止是以；为结束标志的，所以这句命令后面的分号是不可缺少的，考虑到各个系统中分号会与不同的意义，所以前面加反斜杠。 {}花括号代表前面find查找出来的文件名。 使用find时，只要把想要的操作写在一个文件里，就可以用exec来配合find查找，很方便的。在有些操作系统中只允许-exec选项执行诸如l s或ls -l这样的命令。大多数用户使用这一选项是为了查找旧文件并删除它们。建议在真正执行rm命令删除文件之前，最好先用ls命令看一下，确认它们是所要删除的文件。 exec选项后面跟随着所要执行的命令或脚本，然后是一对儿{ }，一个空格和一个\，最后是一个分号。为了使用exec选项，必须要同时使用print选项。如果验证一下find命令，会发现该命令只输出从当前路径起的相对路径及文件名。 常用实例1）ls -l命令放在find命令的-exec选项中 1234567# find . -type f -exec ls -l &#123;&#125; \; -rw-r--r-- 1 root root 127 10-28 16:51 ./log2014.log-rw-r--r-- 1 root root 0 10-28 14:47 ./test4/log3-2.log-rw-r--r-- 1 root root 0 10-28 14:47 ./test4/log3-3.log-rw-r--r-- 1 root root 0 10-28 14:47 ./test4/log3-1.log-rw-r--r-- 1 root root 33 10-28 16:54 ./log2013.log-rw-r--r-- 1 root root 302108 11-03 06:19 ./log2012.log 说明： 上面的例子中，find命令匹配到了当前目录下的所有普通文件，并在-exec选项中使用ls -l命令将它们列出。 2）在目录中查找更改时间在n日以前的文件并删除它们 12345678910111213141516171819# ll总计 328-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.log-rw-r--r-- 1 root root 33 10-28 16:54 log2013.log-rw-r--r-- 1 root root 127 10-28 16:51 log2014.loglrwxrwxrwx 1 root root 7 10-28 15:18 log_link.log -&gt; log.log-rw-r--r-- 1 root root 25 10-28 17:02 log.log-rw-r--r-- 1 root root 37 10-28 17:07 log.txtdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 10-28 14:47 test3drwxrwxrwx 2 root root 4096 10-28 14:47 test4# find . -type f -mtime +14 -exec rm &#123;&#125; \;# ll总计 312-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.loglrwxrwxrwx 1 root root 7 10-28 15:18 log_link.log -&gt; log.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 11-12 19:32 test3drwxrwxrwx 2 root root 4096 11-12 19:32 test4 说明： 在shell中用任何方式删除文件之前，应当先查看相应的文件，一定要小心！当使用诸如mv或rm命令时，可以使用-exec选项的安全模式。它将在对每个匹配到的文件进行操作之前提示你。 3）在目录中查找更改时间在n日以前的文件并删除它们，在删除之前先给出提示 12345678910111213141516# ll总计 312-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.loglrwxrwxrwx 1 root root 7 10-28 15:18 log_link.log -&gt; log.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 11-12 19:32 test3drwxrwxrwx 2 root root 4096 11-12 19:32 test4# find . -name &quot;*.log&quot; -mtime +5 -ok rm &#123;&#125; \;&lt; rm ... ./log_link.log &gt; ? y&lt; rm ... ./log2012.log &gt; ? n# ll总计 312-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 11-12 19:32 test3drwxrwxrwx 2 root root 4096 11-12 19:32 test4 说明： 在上面的例子中， find命令在当前目录中查找所有文件名以.log结尾、更改时间在5日以上的文件，并删除它们，只不过在删除之前先给出提示。 按y键删除文件，按n键不删除。 4）-exec中使用grep命令 123# find /etc -name &quot;passwd*&quot; -exec grep &quot;root&quot; &#123;&#125; \;root:x:0:0:root:/root:/bin/bashroot:x:0:0:root:/root:/bin/bash 说明： 任何形式的命令都可以在-exec选项中使用。 在上面的例子中我们使用grep命令。find命令首先匹配所有文件名为“ passwd*”的文件，例如passwd、passwd.old、passwd.bak，然后执行grep命令看看在这些文件中是否存在一个root用户。 5）查找文件移动到指定目录 123456789101112131415161718192021# ll总计 12drwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-12 22:49 test3drwxrwxr-x 2 root root 4096 11-12 19:32 test4# cd test3/# ll总计 304-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.log-rw-r--r-- 1 root root 61 11-12 22:44 log2013.log-rw-r--r-- 1 root root 0 11-12 22:25 log2014.log# find . -name &quot;*.log&quot; -exec mv &#123;&#125; .. \;# ll总计 0[root@localhost test3]# cd ..# ll总计 316-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.log-rw-r--r-- 1 root root 61 11-12 22:44 log2013.log-rw-r--r-- 1 root root 0 11-12 22:25 log2014.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-12 22:50 test3drwxrwxr-x 2 root root 4096 11-12 19:32 test4 6）用exec选项执行cp命令 123456789101112131415161718192021# ll总计 0# cd ..# ll总计 316-rw-r--r-- 1 root root 302108 11-03 06:19 log2012.log-rw-r--r-- 1 root root 61 11-12 22:44 log2013.log-rw-r--r-- 1 root root 0 11-12 22:25 log2014.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxr-x 2 root root 4096 11-12 22:50 test3drwxrwxr-x 2 root root 4096 11-12 19:32 test4# find . -name &quot;*.log&quot; -exec cp &#123;&#125; test3 \;cp: “./test3/log2014.log” 及 “test3/log2014.log” 为同一文件cp: “./test3/log2013.log” 及 “test3/log2013.log” 为同一文件cp: “./test3/log2012.log” 及 “test3/log2012.log” 为同一文件# cd test3# ll总计 304-rw-r--r-- 1 root root 302108 11-12 22:54 log2012.log-rw-r--r-- 1 root root 61 11-12 22:54 log2013.log-rw-r--r-- 1 root root 0 11-12 22:54 log2014.log 转载链接： http://www.cnblogs.com/peida/archive/2012/11/14/2769248.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-find命令概述]]></title>
    <url>%2F2018%2F09%2F23%2FLinux%E5%91%BD%E4%BB%A4-find%E5%91%BD%E4%BB%A4%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[Linux下find命令在目录结构中搜索文件，并执行指定的操作。Linux下find命令提供了相当多的查找条件，功能很强大。由于find具有强大的功能，所以它的选项也很多，其中大部分选项都值得我们花时间来了解一下。即使系统中含有网络文件系统( NFS)，find命令在该文件系统中同样有效，只你具有相应的权限。 在运行一个非常消耗资源的find命令时，很多人都倾向于把它放在后台执行，因为遍历一个大的文件系统可能会花费很长的时间(这里是指30G字节以上的文件系统)。 命令格式find pathname -options [-print -exec -ok ...] 命令功能用于在文件树种查找文件，并作出相应的处理 命令参数1234pathname: find命令所查找的目录路径。例如用.来表示当前目录，用/来表示系统根目录。 -print： find命令将匹配的文件输出到标准输出。 -exec： find命令对匹配的文件执行该参数所给出的shell命令。相应命令的形式为&apos;command&apos; &#123; &#125; \;，注意&#123; &#125;和\；之间的空格。 -ok： 和-exec的作用相同，只不过以一种更为安全的模式来执行该参数所给出的shell命令，在执行每一个命令之前，都会给出提示，让用户来确定是否执行。 命令选项12345678910111213141516171819202122232425262728-name 按照文件名查找文件。-perm 按照文件权限来查找文件。-prune 使用这一选项可以使find命令不在当前指定的目录中查找，如果同时使用-depth选项，那么-prune将被find命令忽略。-user 按照文件属主来查找文件。-group 按照文件所属的组来查找文件。-mtime -n +n 按照文件的更改时间来查找文件， - n表示文件更改时间距现在n天以内，+ n表示文件更改时间距现在n天以前。find命令还有-atime和-ctime 选项，但它们都和-m time选项。-nogroup 查找无有效所属组的文件，即该文件所属的组在/etc/groups中不存在。-nouser 查找无有效属主的文件，即该文件的属主在/etc/passwd中不存在。-newer file1 ! file2 查找更改时间比文件file1新但比文件file2旧的文件。-type 查找某一类型的文件，诸如：b - 块设备文件。d - 目录。c - 字符设备文件。p - 管道文件。l - 符号链接文件。f - 普通文件。-size n：[c] 查找文件长度为n块的文件，带有c时表示文件长度以字节计。-depth：在查找文件时，首先查找当前目录中的文件，然后再在其子目录中查找。-fstype：查找位于某一类型文件系统中的文件，这些文件系统类型通常可以在配置文件/etc/fstab中找到，该配置文件中包含了本系统中有关文件系统的信息。-mount：在查找文件时不跨越文件系统mount点。-follow：如果find命令遇到符号链接文件，就跟踪至链接所指向的文件。-cpio：对匹配的文件使用cpio命令，将这些文件备份到磁带设备中。另外,下面三个的区别:-amin n 查找系统中最后N分钟访问的文件-atime n 查找系统中最后n*24小时访问的文件-cmin n 查找系统中最后N分钟被改变文件状态的文件-ctime n 查找系统中最后n*24小时被改变文件状态的文件-mmin n 查找系统中最后N分钟被改变文件数据的文件-mtime n 查找系统中最后n*24小时被改变文件数据的文件 使用实例1）查找指定时间内修改过的文件 123456# find -atime -2../logs/monitor./.bashrc./.bash_profile./.bash_history 说明： 超找48小时内修改过的文件 2）根据关键字查找 123456789# find . -name &quot;*.log&quot; ./log_link.log./log2014.log./test4/log3-2.log./test4/log3-3.log./test4/log3-1.log./log2013.log./log2012.log./log.log 说明： 在当前目录查找 以.log结尾的文件。 “. “代表当前目录 3）按照目录或文件的权限来查找文件 12345# find /opt/soft/test/ -perm 777/opt/soft/test/log_link.log/opt/soft/test/test4/opt/soft/test/test5/test3/opt/soft/test/test3 说明： 查找/opt/soft/test/目录下 权限为 777的文件 4）按类型查找 12345find . -type f -name &quot;*.log&quot;./log2014.log./test4/log3-2.log./test4/log3-3.log./test4/log3-1.log 说明： 查找当目录，以.log结尾的普通文件 5）查找当前所有目录并排序 12345678# find . -type d | sort../scf./scf/bin./scf/doc./scf/lib./scf/service./scf/service/deploy 6）按大小查找文件 12345678# find . -size +1000c -print../test4./scf./scf/lib./scf/service./scf/service/deploy./scf/service/deploy/product 说明： 查找当前目录大于1K的文件 转载链接： http://www.cnblogs.com/peida/archive/2012/11/13/2767374.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-locate命令]]></title>
    <url>%2F2018%2F09%2F23%2FLinux%E5%91%BD%E4%BB%A4-locate%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[locate 让使用者可以很快速的搜寻档案系统内是否有指定的档案。其方法是先建立一个包括系统内所有档案名称及路径的数据库，之后当寻找时就只需查询这个数据库，而不必实际深入档案系统之中了。在一般的 distribution 之中，数据库的建立都被放在 crontab 中自动执行。 语法locate (选项)(参数) 选项12345678910-e 将排除在寻找的范围之外。-1 如果 是 1．则启动安全模式。在安全模式下，使用者不会看到权限无法看到 的档案。这会始速度减慢，因为 locate 必须至实际的档案系统中取得档案的 权限资料。-f 将特定的档案系统排除在外，例如我们没有到理要把 proc 档案系统中的档案 放在资料库中-q 安静模式，不会显示任何错误讯息-n 至多显示 n个输出-r 使用正规运算式 做寻找的条件-o 指定资料库存的名称-d 指定资料库的路径-h 显示辅助讯息-V 显示程式的版本讯息 参数查找字符串：要查找的文件名中含有的字符串。 功能locate命令可以在搜寻数据库是快速找到档案，数据库有updatedb程序来更新，updatedb是由cron daemon周期性建立的，locate命令在搜寻数据库时比由整个由硬盘来搜寻资料来得快，但locate所找到的档案若是最近才建立或刚更名的，可能会找不到，在内定值中，updatedb每天会跑一次，可以由修改crontab来更新设定值。（/etc/crontab） locate指令和find找寻档案的功能类似，但locate是透过update程序将硬盘中的所有档案盒目录资料先建立一个索引数据库，在执行locate时直接找该索引，查询速度会较快，索引数据库一般是由操作系统管理，但也可以直接下达update强迫系统立即修改索引数据库。 常用范例1）查找和pwd相关的所有文件 1234567# locate pwd/bin/pwd/etc/.pwd.lock/sbin/unix_chkpwd/usr/bin/pwdx/usr/include/pwd.h/usr/lib/python2.7/dist-packages/twisted/python/fakepwd.py 2） 搜索etc目录下所有以sh开头的文件 1234 # locate /etc/sh/etc/shadow/etc/shadow-/etc/shells 3）搜索etc目录下，所有以m开头的文件 12345# locate /etc/m/etc/magic/etc/magic.mime/etc/mailcap/etc/mailcap.order 参考链接： http://www.cnblogs.com/peida/archive/2012/11/12/2765750.html http://man.linuxde.net/locate_slocate]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-whereis命令]]></title>
    <url>%2F2018%2F09%2F22%2FLinux%E5%91%BD%E4%BB%A4-whereis%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[whereis命令用来定位指令的二进制程序、源代码文件和man手册页等相关文件的路径。 whereis命令只能用于程序名的搜索，而且只搜索二进制文件（参数-b）、man说明文件（参数-m）和源代码文件（参数-s）。如果省略参数，则返回所有信息。 和find相比，whereis查找的速度非常快，这是因为linux系统会将 系统内的所有文件都记录在一个数据库文件中，当使用whereis和下面即将介绍的locate时，会从数据库中查找数据，而不是像find命令那样，通 过遍历硬盘来查找，效率自然会很高。 但是该数据库文件并不是实时更新，默认情况下时一星期更新一次，因此，我们在用whereis和locate 查找文件时，有时会找到已经被删除的数据，或者刚刚建立文件，却无法查找到，原因就是因为数据库文件没有被更新。 语法whereis(选项)(参数) 选项1234567-b 定位可执行文件。-m 定位帮助文件。-s 定位源代码文件。-u 搜索默认路径下除可执行文件、源代码文件、帮助文件以外的其它文件。-B 指定搜索可执行文件的路径。-M 指定搜索帮助文件的路径。-S 指定搜索源代码文件的路径。 常用范例1）将和**文件相关的文件都查找出来 12# whereis svnsvn: /usr/bin/svn /usr/local/svn /usr/share/man/man1/svn.1.gz 说明： tomcat没安装，找不出来，svn安装找出了很多相关文件 2）只将二进制文件 查找出来 12# whereis -b svnsvn: /usr/bin/svn /usr/local/svn 说明： whereis -m svn 查出说明文档路径，whereis -s svn 找source源文件。]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-which命令]]></title>
    <url>%2F2018%2F09%2F22%2FLinux%E5%91%BD%E4%BB%A4-which%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[我们经常在linux要查找某个文件，但不知道放在哪里了，可以使用下面的一些命令来搜索： which 查看可执行文件的位置。 whereis 查看文件的位置。 locate 配合数据库查看文件位置。 find 实际搜寻硬盘查询文件名称。 which命令的作用是，在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。也就是说，使用which命令，就可以看到某个系统命令是否存在，以及执行的到底是哪一个位置的命令。 语法which 可执行文件名称 选项1234-n 指定文件名长度，指定的长度必须大于或等于所有文件中最长的文件名。-p 与-n参数相同，但此处的包括了文件的路径。-w 指定输出时栏位的宽度。-V 显示版本信息 功能which指令会在PATH变量指定的路径中，搜索某个系统命令的位置，并且返回第一个搜索结果。 常用范例1）查找文件、显示命令路径 12# which pwd/bin/pwd 参考链接： http://www.cnblogs.com/peida/archive/2012/11/08/2759805.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-tail命令]]></title>
    <url>%2F2018%2F09%2F21%2FLinux%E5%91%BD%E4%BB%A4-tail%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[tail 命令从指定点开始将文件写到标准输出.使用tail命令的-f选项可以方便的查阅正在改变的日志文件,tail -f filename会把filename里最尾部的内容显示在屏幕上,并且不断刷新,使你看到最新的文件内容. 语法tail(选项)(参数) 选项12345678-f 循环读取-q 不显示处理信息-v 显示详细的处理信息-c&lt;数目&gt; 显示的字节数-n&lt;行数&gt; 显示行数--pid=PID 与-f合用,表示在进程ID,PID死掉之后结束. -q, --quiet, --silent 从不输出给出文件名的首部 -s, --sleep-interval=S 与-f合用,表示在每次反复的间隔休眠S秒 功能用于显示指定文件末尾内容，不指定文件时，作为输入信息进行处理。常用查看日志文件。 常用范例1）显示文件末尾内容 123456# tail -n 5 log2014.log 2014-092014-102014-112014-12=========================== 说明： 显示文件最后5行内容 2）循环查看文件内容 1234567891011121314# ping 192.168.120.204 &gt; test.log &amp;# tail -f test.log PING 192.168.120.204 (192.168.120.204) 56(84) bytes of data.64 bytes from 192.168.120.204: icmp_seq=1 ttl=64 time=0.038 ms64 bytes from 192.168.120.204: icmp_seq=2 ttl=64 time=0.036 ms64 bytes from 192.168.120.204: icmp_seq=3 ttl=64 time=0.033 ms64 bytes from 192.168.120.204: icmp_seq=4 ttl=64 time=0.027 ms64 bytes from 192.168.120.204: icmp_seq=5 ttl=64 time=0.032 ms64 bytes from 192.168.120.204: icmp_seq=6 ttl=64 time=0.026 ms64 bytes from 192.168.120.204: icmp_seq=7 ttl=64 time=0.030 ms64 bytes from 192.168.120.204: icmp_seq=8 ttl=64 time=0.029 ms64 bytes from 192.168.120.204: icmp_seq=9 ttl=64 time=0.044 ms64 bytes from 192.168.120.204: icmp_seq=10 ttl=64 time=0.033 ms64 bytes from 192.168.120.204: icmp_seq=11 ttl=64 time=0.027 ms 3）从第5行开始显示文件 12345678910111213141516171819202122# cat log2014.log 2014-012014-022014-032014-042014-052014-062014-072014-082014-092014-102014-112014-12# tail -n +5 log2014.log2014-052014-062014-072014-082014-092014-102014-112014-12 参考链接： http://www.cnblogs.com/peida/archive/2012/11/07/2758084.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-head命令]]></title>
    <url>%2F2018%2F09%2F21%2FLinux%E5%91%BD%E4%BB%A4-head%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[head 与 tail 就像它的名字一样的浅显易懂，它是用来显示开头或结尾某个数量的文字区块，head 用来显示档案的开头至标准输出中，而 tail 想当然尔就是看档案的结尾。 语法head(选项)(参数) 选项1234-n&lt;数字&gt;：指定显示头部内容的行数；-c&lt;字符数&gt;：指定显示头部内容的字符数；-v：总是显示文件名的头信息；-q：不显示文件名的头信息。 功能head 用来显示档案的开头至标准输出中，默认head命令打印其相应文件的开头10行。 常用范例1）显示文件的前n行 12345678910111213141516171819# cat log2014.log 2014-012014-022014-032014-042014-052014-062014-072014-082014-092014-102014-112014-12# head -n 5 log2014.log 2014-012014-022014-032014-042014-05 2）显示文件前n个字节 1234# head -c 20 log2014.log2014-012014-022014 3）文件的除了最后n个字节以外的内容 12345678910111213# head -c -32 log2014.log2014-012014-022014-032014-042014-052014-062014-072014-082014-092014-102014-112014-12 4）输出文件除了最后n行的全部内容 12345678# head -n -6 log2014.log2014-012014-022014-032014-042014-052014-062014-07 参考链接： http://www.cnblogs.com/peida/archive/2012/11/06/2756278.html http://man.linuxde.net/head]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-less命令]]></title>
    <url>%2F2018%2F09%2F21%2FLinux%E5%91%BD%E4%BB%A4-less%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[less 工具也是对文件或其它输出进行分页显示的工具，应该说是linux正统查看文件内容的工具，功能极其强大。less 的用法比起 more 更加的有弹性。在 more 的时候，我们并没有办法向前面翻， 只能往后面看，但若使用了 less 时，就可以使用 [pageup][pagedown] 等按键的功能来往前往后翻看文件，更容易用来查看一个文件的内容！除此之外，在 less 里头可以拥有更多的搜索功能，不止可以向下搜，也可以向上搜。 语法less(选项)(参数) 选项1234567891011121314151617181920212223242526-b &lt;缓冲区大小&gt; 设置缓冲区的大小-e 当文件显示结束后，自动离开-f 强迫打开特殊文件，例如外围设备代号、目录和二进制文件-g 只标志最后搜索的关键词-i 忽略搜索时的大小写-m 显示类似more命令的百分比-N 显示每行的行号-o &lt;文件名&gt; 将less 输出的内容在指定文件中保存起来-Q 不使用警告音-s 显示连续空行为一行-S 行过长时间将超出部分舍弃-x &lt;数字&gt; 将“tab”键显示为规定的数字空格/字符串：向下搜索“字符串”的功能?字符串：向上搜索“字符串”的功能n：重复前一个搜索（与 / 或 ? 有关）N：反向重复前一个搜索（与 / 或 ? 有关）b 向后翻一页d 向后翻半页h 显示帮助界面Q 退出less 命令u 向前滚动半页y 向前滚动一行空格键 滚动一行回车键 滚动一页[pagedown]： 向下翻动一页[pageup]： 向上翻动一页 功能less 与 more 类似，但使用 less 可以随意浏览文件，而 more 仅能向前移动，却不能向后移动，而且 less 在查看之前不会加载整个文件。 常用实例1）ps查看进程信息并通过less分页显示 1ps -ef | less 2 ) 查看命令历史使用记录并通过less分页显示 1history | less 3）浏览多个文件 1less log2013.log log2014.log 说明： 输入 ：n后，切换到 log2014.log 输入 ：p 后，切换到log2013.log 附加备注1.全屏导航 ctrl + F - 向前移动一屏 ctrl + B - 向后移动一屏 ctrl + D - 向前移动半屏 ctrl + U - 向后移动半屏 2.单行导航 j - 向前移动一行 k - 向后移动一行 3.其它导航 G - 移动到最后一行 g - 移动到第一行 q / ZZ - 退出 less 命令 4.其它有用的命令 v - 使用配置的编辑器编辑当前文件 h - 显示 less 的帮助文档 &amp;pattern - 仅显示匹配模式的行，而不是整个文件 5.标记导航 当使用 less 查看大文件时，可以在任何一个位置作标记，可以通过命令导航到标有特定标记的文本位置： ma - 使用 a 标记文本的当前位置 ‘a - 导航到标记 a 处 参考链接： http://www.cnblogs.com/peida/archive/2012/11/05/2754477.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-more命令]]></title>
    <url>%2F2018%2F09%2F19%2FLinux%E5%91%BD%E4%BB%A4-more%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[more命令，功能类似 cat ，cat命令是整个文件的内容从上到下显示在屏幕上。 more会以一页一页的显示方便使用者逐页阅读，而最基本的指令就是按空白键（space）就往下一页显示，按 b 键就会往回（back）一页显示，而且还有搜寻字串的功能 。more命令从前向后读取文件，因此在启动时就加载整个文件。 语法more(语法)(参数) 选项123456789+n 从笫n行开始显示-n 定义屏幕大小为n行+/pattern 在每个档案显示前搜寻该字串（pattern），然后从该字串前两行之后开始显示 -c 从顶部清屏，然后显示-d 提示“Press space to continue，’q’ to quit（按空格键继续，按q键退出）”，禁用响铃功能-l 忽略Ctrl+l（换页）字符-p 通过清除窗口而不是滚屏来对文件进行换页，与-c选项相似-s 把连续的多个空行显示为一行-u 把文件内容中的下画线去掉 功能more命令和cat的功能一样都是查看文件里的内容，但有所不同的是more可以按页来查看文件的内容，还支持直接跳转行等功能。 常用操作命令123456789Enter 向下n行，需要定义。默认为1行Ctrl+F 向下滚动一屏空格键 向下滚动一屏Ctrl+B 返回上一屏= 输出当前行的行号：f 输出文件名和当前行的行号V 调用vi编辑器!命令 调用Shell，并执行命令 q 退出more 常用范例1）显示文件中从第3行起的内容 123456789101112# cat log2012.log 2012-012012-022012-032012-04-day12012-04-day22012-04-day3# more +3 log2012.log 2012-032012-04-day12012-04-day22012-04-day3 2）从文件中查找第一个出现”day3”字符串的行，并从该处前两行开始显示输出 1234567# more +/day3 log2012.log ...skipping2012-04-day12012-04-day22012-04-day32012-052012-05-day1 3）设定每屏显示行数 123456# more -5 log2012.log 2012-012012-022012-032012-04-day12012-04-day2 4）列一个目录下的文件，由于内容太多，我们应该学会用more来分页显示。这得和管道 | 结合起来 1234567891011# ls -l | more -5总计 36-rw-r--r-- 1 root root 308 11-01 16:49 log2012.log-rw-r--r-- 1 root root 33 10-28 16:54 log2013.log-rw-r--r-- 1 root root 127 10-28 16:51 log2014.loglrwxrwxrwx 1 root root 7 10-28 15:18 log_link.log -&gt; log.log-rw-r--r-- 1 root root 25 10-28 17:02 log.log-rw-r--r-- 1 root root 37 10-28 17:07 log.txtdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 10-28 14:47 test3drwxrwxrwx 2 root root 4096 10-28 14:47 test4 说明： 每页显示5个文件信息，按 Ctrl+F 或者 空格键 将会显示下5条文件信息。 参考链接： http://www.cnblogs.com/peida/archive/2012/11/02/2750588.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-nl命令]]></title>
    <url>%2F2018%2F09%2F19%2FLinux%E5%91%BD%E4%BB%A4-nl%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[nl命令读取 file 参数（缺省情况下标准输入），计算输入中的行号，将计算过的行号写入标准输出。在输出中，nl命令根据您在命令行中指定的标志来计算左边的行。输入文本必须写在逻辑页中。每个逻辑页有头、主体和页脚节（可以有空节）。除非使用-p选项，nl 命令在每个逻辑页开始的地方重新设置行号。可以单独为头、主体和页脚节设置行计算标志（例如，头和页脚行可以被计算然而文本行不能）。其默认的结果与cat -n有点不太一样， nl 可以将行号做比较多的显示设计，包括位数与是否自动补齐0等等的功能。 语法nl (选项) (参数) 选项123456789-b ：指定行号指定的方式，主要有两种： -b a ：表示不论是否为空行，也同样列出行号(类似 cat -n)； -b t ：如果有空行，空的那一行不要列出行号(默认值)；-n ：列出行号表示的方法，主要有三种： -n ln ：行号在萤幕的最左方显示； -n rn ：行号在自己栏位的最右方显示，且不加 0 ； -n rz ：行号在自己栏位的最右方显示，且加 0 ；-w ：行号栏位的占用的位数。-p ：在逻辑定界符处不重新开始计算。 常用范例1）用 nl 列出 log2015.log 的内容： 123456# nl log2015.log1 2015-012 2015-023 ====== 说明：文件中的空白行，nl 不会加上行号 2）用 nl 列出 log2015.log 的内容，空本行也加上行号： 123456# nl -b a log2015.log1 2015-012 2015-02345 ====== 3）让行号前面自动补上0，统一输出格式： 1234567891011121314151617181920212223242526272829# nl -b a -n rz log2015.log000001 2015-01000002 2015-02000003 2015-03000004 2015-04000005 2015-05000006 2015-06000007 2015-07000008 2015-08000009 2015-09000010 2015-10000011 2015-11000012 2015-12000013 =======# nl -b a -n rz -w 3 log2015.log001 2015-01002 2015-02003 2015-03004 2015-04005 2015-05006 2015-06007 2015-07008 2015-08009 2015-09010 2015-10011 2015-11012 2015-12013 ======= 说明：nl -b a -n rz命令行号默认为六位，要调整位数可以加上参数-w 3调整为3位。 参考链接： http://man.linuxde.net/nl]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-cat命令]]></title>
    <url>%2F2018%2F09%2F19%2FLinux%E5%91%BD%E4%BB%A4-cat%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[cat命令的用途是连接文件或标准输入并打印。这个命令常用来显示文件内容，或者将几个文件连接起来显示，或者从标准输入读取内容并显示，它常与重定向符号配合使用。 语法cat(选项)(参数) 选项12345678910-A, --show-all 等价于 -vET-b, --number-nonblank 对非空输出行编号-e 等价于 -vE-E, --show-ends 在每行结束处显示 $-n, --number 对输出的所有行编号,由1开始对所有输出的行数编号-s, --squeeze-blank 有连续两行以上的空白行，就代换为一行的空白行 -t 与 -vT 等价-T, --show-tabs 将跳格字符显示为 ^I-u (被忽略)-v, --show-nonprinting 使用 ^ 和 M- 引用，除了 LFD 和 TAB 之外 功能1.一次显示整个文件:cat filename 2.从键盘创建一个文件:cat &gt; filename 只能创建新文件,不能编辑已有文件. 3.将几个文件合并为一个文件:cat file1 file2 &gt; file 常用范例1）把 log2012.log 的文件内容加上行号后输入 log.log 这个文件里 12345# cat log.log [root@localhost test]# cat -n log2012.log &gt; log.log# cat -n log.log 1 2012-01 2 2012-02 2）使用here doc来生成文件 12345678910111213# cat &gt;log.txt &lt;&lt;EOF&gt; Hello&gt; World&gt; Linux&gt; PWD=$(pwd)&gt; EOF# ls -l log.txt -rw-r--r-- 1 root root 37 10-28 17:07 log.txt# cat log.txt HelloWorldLinuxPWD=/opt/soft/test 说明： 注意粗体部分，here doc可以进行字符串替换。 备注： tac (反向列示) 12345# tac log.txt PWD=/opt/soft/testLinuxWorldHello 说明： tac 是将 cat 反写过来，所以他的功能就跟 cat 相反， cat 是由第一行到最后一行连续显示在萤幕上，而 tac 则是由最后一行到第一行反向在萤幕上显示出来！ 参考链接： http://www.cnblogs.com/peida/archive/2012/10/30/2746968.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-touch命令]]></title>
    <url>%2F2018%2F09%2F18%2FLinux%E5%91%BD%E4%BB%A4-touch%2F</url>
    <content type="text"><![CDATA[linux的touch命令不常用，一般在使用make的时候可能会用到，用来修改文件时间戳，或者新建一个不存在的文件。 语法touch(选项)(参数) 选项123456789-a：或--time=atime或--time=access或--time=use 只更改存取时间；-c：或--no-create 不建立任何文件；-d：&lt;时间日期&gt; 使用指定的日期时间，而非现在的时间；-f：此参数将忽略不予处理，仅负责解决BSD版本touch指令的兼容性问题；-m：或--time=mtime或--time=modify 只更该变动时间；-r：&lt;参考文件或目录&gt; 把指定文件或目录的日期时间，统统设成和参考文件或目录的日期时间相同；-t：&lt;日期时间&gt; 使用指定的日期时间，而非现在的时间；--help：在线帮助；--version：显示版本信息。 功能touch命令参数可更改文档或目录的日期时间，包括存取时间和更改时间。 常用范例1）创建不存在的文件 1234# touch log2012.log log2013.log# ll-rw-r--r-- 1 root root 0 10-28 16:01 log2012.log-rw-r--r-- 1 root root 0 10-28 16:01 log2013.log 如果log2014.log不存在，则不创建文件 1234# touch -c log2014.log# ll-rw-r--r-- 1 root root 0 10-28 16:01 log2012.log-rw-r--r-- 1 root root 0 10-28 16:01 log2013.log 2）更新log.log的时间和log2012.log时间戳相同 123456789# ll-rw-r--r-- 1 root root 0 10-28 16:01 log2012.log-rw-r--r-- 1 root root 0 10-28 16:01 log2013.log-rw-r--r-- 1 root root 0 10-28 14:48 log.log# touch -r log.log log2012.log # ll-rw-r--r-- 1 root root 0 10-28 14:48 log2012.log-rw-r--r-- 1 root root 0 10-28 16:01 log2013.log-rw-r--r-- 1 root root 0 10-28 14:48 log.log 3）设定文件的时间戳 123456789# ll-rw-r--r-- 1 root root 0 10-28 14:48 log2012.log-rw-r--r-- 1 root root 0 10-28 16:01 log2013.log-rw-r--r-- 1 root root 0 10-28 14:48 log.log# touch -t 201211142234.50 log.log# ll-rw-r--r-- 1 root root 0 10-28 14:48 log2012.log-rw-r--r-- 1 root root 0 10-28 16:01 log2013.log-rw-r--r-- 1 root root 0 2012-11-14 log.log 说明： -t time 使用指定的时间值 time 作为指定文件相应时间戳记的新值．此处的 time规定为如下形式的十进制数: [[CC]YY]MMDDhhmm[.SS] 这里，CC为年数中的前两位，即”世纪数”；YY为年数的后两位，即某世纪中的年数．如果不给出CC的值，则touch 将把年数CCYY限定在1969–2068之内．MM为月数，DD为天将把年数CCYY限定在1969–2068之内．MM为月数，DD为天数，hh 为小时数(几点)，mm为分钟数，SS为秒数．此处秒的设定范围是0–61，这样可以处理闰秒．这些数字组成的时间是环境变量TZ指定的时区中的一个时 间．由于系统的限制，早于1970年1月1日的时间是错误的。 参考链接： http://www.cnblogs.com/peida/archive/2012/10/30/2745714.html http://man.linuxde.net/touch]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-cp命令]]></title>
    <url>%2F2018%2F09%2F18%2FLinux%E5%91%BD%E4%BB%A4-cp%2F</url>
    <content type="text"><![CDATA[cp命令用来复制文件或者目录，是Linux系统中最常用的命令之一。一般情况下，shell会设置一个别名，在命令行下复制文件时，如果目标文件已经存在，就会询问是否覆盖，不管你是否使用-i参数。但是如果是在shell脚本中执行cp时，没有-i参数时不会询问是否覆盖。这说明命令行和shell脚本的执行方式有些不同。 语法cp(选项)(参数) 选项123456789101112-a：此参数的效果和同时指定&quot;-dpR&quot;参数相同；-d：当复制符号连接时，把目标文件或目录也建立为符号连接，并指向与源文件或目录连接的原始文件或目录；-f：强行复制文件或目录，不论目标文件或目录是否已存在；-i：覆盖既有文件之前先询问用户；-l：对源文件建立硬连接，而非复制文件；-p：保留源文件或目录的属性；-R/r：递归处理，将指定目录下的所有文件与子目录一并处理；-s：对源文件建立符号连接，而非复制文件；-u：使用这项参数后只会在源文件的更改时间较目标文件更新时或是名称相互对应的目标文件并不存在时，才复制文件；-S：在备份文件时，用指定的后缀“SUFFIX”代替文件的默认后缀；-b：覆盖已存在的文件目标前将目标文件备份；-v：详细显示命令执行的操作。 参数 源文件：制定源文件列表。默认情况下，cp命令不能复制目录，如果要复制目录，则必须使用-R选项； 目标文件：指定目标文件。当“源文件”为多个文件时，要求“目标文件”为指定的目录。 常用范例1）复制单个文件到目标目录，文件在目标文件中不存在 123456789101112# cp log.log test5# ll-rw-r--r-- 1 root root 0 10-28 14:48 log.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 10-28 14:47 test3drwxr-xr-x 2 root root 4096 10-28 14:53 test5# cd test5# ll-rw-r--r-- 1 root root 0 10-28 14:46 log5-1.log-rw-r--r-- 1 root root 0 10-28 14:46 log5-2.log-rw-r--r-- 1 root root 0 10-28 14:46 log5-3.log-rw-r--r-- 1 root root 0 10-28 14:53 log.log 说明： 在没有带-a参数时，两个文件的时间是不一样的。在带了-a参数时，两个文件的时间是一致的。 2）目标文件存在时，会询问是否覆盖 12345678910# cp log.log test5cp：是否覆盖“test5/log.log”? n# cp -a log.log test5cp：是否覆盖“test5/log.log”? y# cd test5/# ll-rw-r--r-- 1 root root 0 10-28 14:46 log5-1.log-rw-r--r-- 1 root root 0 10-28 14:46 log5-2.log-rw-r--r-- 1 root root 0 10-28 14:46 log5-3.log-rw-r--r-- 1 root root 0 10-28 14:48 log.log 说明： 目标文件存在时，会询问是否覆盖。这是因为cp是cp -i的别名。目标文件存在时，即使加了-f标志，也还会询问是否覆盖。 3）复制整个目录 目标目录存在时： 12345678910111213#cp -a test3 test5 # ll-rw-r--r-- 1 root root 0 10-28 14:48 log.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 10-28 14:47 test3drwxr-xr-x 3 root root 4096 10-28 15:11 test5# cd test5/# ll-rw-r--r-- 1 root root 0 10-28 14:46 log5-1.log-rw-r--r-- 1 root root 0 10-28 14:46 log5-2.log-rw-r--r-- 1 root root 0 10-28 14:46 log5-3.log-rw-r--r-- 1 root root 0 10-28 14:48 log.logdrwxrwxrwx 2 root root 4096 10-28 14:47 test3 目标目录不存在时： 1234567# cp -a test3 test4# ll-rw-r--r-- 1 root root 0 10-28 14:48 log.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 10-28 14:47 test3drwxrwxrwx 2 root root 4096 10-28 14:47 test4drwxr-xr-x 3 root root 4096 10-28 15:11 test5 说明： 注意目标目录存在与否结果是不一样的。目标目录存在时，整个源目录被复制到目标目录里面。 4）复制的 log.log 建立一个链接到 log_link.log 12345678# cp -s log.log log_link.log# lllrwxrwxrwx 1 root root 7 10-28 15:18 log_link.log -&gt; log.log-rw-r--r-- 1 root root 0 10-28 14:48 log.logdrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 10-28 14:47 test3drwxrwxrwx 2 root root 4096 10-28 14:47 test4drwxr-xr-x 3 root root 4096 10-28 15:11 test5 说明： 那个 log_link.log 是由 -s 的参数造成的，建立的是一个『快捷方式』，所以您会看到在文件的最右边，会显示这个文件是『连结』到哪里去的！ 5）将文件file复制到目录/usr/men/tmp下，并改名为file1 1cp file /usr/men/tmp/file1 6）将目录/usr/men下的所有文件及其子目录复制到目录/usr/zh中 1cp -i /usr/men m*.c /usr/zh 我们在Linux下使用cp命令复制文件时候，有时候会需要覆盖一些同名文件，覆盖文件的时候都会有提示：需要不停的按Y来确定执行覆盖。文件数量不多还好，但是要是几百个估计按Y都要吐血了，于是折腾来半天总结了一个方法： 1234567891011cp aaa/* /bbb复制目录aaa下所有到/bbb目录下，这时如果/bbb目录下有和aaa同名的文件，需要按Y来确认并且会略过aaa目录下的子目录。cp -r aaa/* /bbb这次依然需要按Y来确认操作，但是没有忽略子目录。cp -r -a aaa/* /bbb依然需要按Y来确认操作，并且把aaa目录以及子目录和文件属性也传递到了/bbb。\cp -r -a aaa/* /bbb成功，没有提示按Y、传递了目录属性、没有略过目录。 参考链接： http://www.cnblogs.com/peida/archive/2012/10/29/2744185.html http://man.linuxde.net/cp]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库基本操作17-预处理及存储过程]]></title>
    <url>%2F2018%2F09%2F17%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C17-%E9%A2%84%E5%A4%84%E7%90%86%E5%8F%8A%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[1.8 预处理预编译一次，可以多次执行。用来解决一条SQL语句频繁执行的问题。 12预处理语句：prepare 预处理名字 from ‘sql语句’执行预处理：execute 预处理名字 [using 变量] 例题一： 1234567891011121314151617mysql&gt; prepare stmt from &apos;select * from stuinfo&apos;; # 创建预处理Query OK, 0 rows affected (0.00 sec)Statement preparedmysql&gt; execute stmt; # 执行预处理+--------+----------+--------+--------+---------+------------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress |+--------+----------+--------+--------+---------+------------+| s25301 | 张秋丽 | 男 | 18 | 1 | 北京 || s25302 | 李文才 | 男 | 31 | 3 | 上海 || s25303 | 李斯文 | 女 | 22 | 2 | 北京 || s25304 | 欧阳俊雄 | 男 | 28 | 4 | 天津 || s25305 | 诸葛丽丽 | 女 | 23 | 7 | 河南 || s25318 | 争青小子 | 男 | 26 | 6 | 天津 || s25319 | 梅超风 | 女 | 23 | 5 | 河北 |+--------+----------+--------+--------+---------+------------+7 rows in set (0.00 sec) 例题二：传递参数 123456789101112131415mysql&gt; delimiter // mysql&gt; prepare stmt from &apos;select * from stuinfo where stuno=?&apos; // -- ?是位置占位符Query OK, 0 rows affected (0.00 sec)Statement preparedmysql&gt; set @id=&apos;s25301&apos;; -- 变量以@开头，通过set给变量赋值 -&gt; execute stmt using @id // -- 执行预处理，传递参数Query OK, 0 rows affected (0.00 sec)+--------+---------+--------+--------+---------+------------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress |+--------+---------+--------+--------+---------+------------+| s25301 | 张秋丽 | 男 | 18 | 1 | 北京 |+--------+---------+--------+--------+---------+------------+1 row in set (0.00 sec) 1234脚下留心：1、?是位置占位符2、变量以@开头3、通过set给变量赋值 例题三：传递多个参数 1234567891011121314151617mysql&gt; prepare stmt from &apos;select * from stuinfo where stusex=? and stuaddress=?&apos; //Query OK, 0 rows affected (0.00 sec)Statement preparedmysql&gt; set @sex=&apos;男&apos;; -&gt; set @addr=&apos;北京&apos;; -&gt; execute stmt using @sex,@addr //Query OK, 0 rows affected (0.00 sec)Query OK, 0 rows affected (0.00 sec)+--------+---------+--------+--------+---------+------------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress |+--------+---------+--------+--------+---------+------------+| s25301 | 张秋丽 | 男 | 18 | 1 | 北京 |+--------+---------+--------+--------+---------+------------+1 row in set (0.00 sec) 1.9 存储过程【procedure】1.7.1 存储过程的优点 存储过程可以减少网络流量 允许模块化设计 支持事务 1.7.2 创建存储过程 语法： 123456create procedure 存储过程名(参数)begin //sql语句end;脚下留心：由于过程中有很多SQL语句，每个语句的结束都要用（；）结束。默认情况下，分号既表示语句结束，又表示向服务器发送SQL语句。我们希望分号仅表示语句的结束，不要将SQL语句发送到服务器执行，通过delimiter来更改结束符。 例题 123456mysql&gt; delimiter //mysql&gt; create procedure proc() -- 创建存储过程 -&gt; begin -&gt; select * from stuinfo; -&gt; end //Query OK, 0 rows affected (0.00 sec) 1.7.3 调用存储过程语法： 1call 存储过程名() 例题： 12345678910111213mysql&gt; call proc() // -- 调用存储过程+--------+----------+--------+--------+---------+------------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress |+--------+----------+--------+--------+---------+------------+| s25301 | 张秋丽 | 男 | 18 | 1 | 北京 || s25302 | 李文才 | 男 | 31 | 3 | 上海 || s25303 | 李斯文 | 女 | 22 | 2 | 北京 || s25304 | 欧阳俊雄 | 男 | 28 | 4 | 天津 || s25305 | 诸葛丽丽 | 女 | 23 | 7 | 河南 || s25318 | 争青小子 | 男 | 26 | 6 | 天津 || s25319 | 梅超风 | 女 | 23 | 5 | 河北 |+--------+----------+--------+--------+---------+------------+7 rows in set (0.00 sec) 1.7.4 删除存储过程语法 1drop procedure [if exists] 存储过程名 例题： 12mysql&gt; drop procedure proc // -- 删除存储过程Query OK, 0 rows affected (0.00 sec) 1.7.5 查看存储过程的信息1show create procedure 存储过程名\G 例题 123456789101112mysql&gt; show create procedure proc \G*************************** 1. row *************************** Procedure: proc sql_mode: STRICT_TRANS_TABLES,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION Create Procedure: CREATE DEFINER=`root`@`localhost` PROCEDURE `proc`()beginselect * from stuinfo;endcharacter_set_client: gbkcollation_connection: gbk_chinese_ci Database Collation: utf8_general_ci1 row in set (0.00 sec) 1.7.6 显示所有的存储过程1mysql&gt; show procedure status \G 1.7.7 存储过程的参数存储过程的参数分为：输入参数（in）【默认】，输出参数（out），输入输出参数（inout） 存储过程不能使用return返回值，要返回值只能通过“输出参数”来向外传递值。 例题一：传递学号，获取对应的信息 1234567891011mysql&gt; create procedure proc(in param varchar(10)) -- 输入参数 -&gt; select * from stuinfo where stuno=param //Query OK, 0 rows affected (0.00 sec)mysql&gt; call proc(&apos;s25301&apos;) //+--------+---------+--------+--------+---------+------------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress |+--------+---------+--------+--------+---------+------------+| s25301 | 张秋丽 | 男 | 18 | 1 | 北京 |+--------+---------+--------+--------+---------+------------+1 row in set (0.00 sec) 例题二：查找同桌 12345678910111213141516mysql&gt; create procedure proc(name varchar(10)) -&gt; begin -&gt; declare seat tinyint; -- 声明局部变量 -&gt; select stuseat into seat from stuinfo where stuname=name; -- 将座位号保存到变量中 -&gt; select * from stuinfo where stuseat=seat+1 or stuseat=seat-1; -- 查找同桌 -&gt; end //Query OK, 0 rows affected (0.00 sec)mysql&gt; call proc(&apos;李文才&apos;) //+--------+----------+--------+--------+---------+------------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress |+--------+----------+--------+--------+---------+------------+| s25303 | 李斯文 | 女 | 22 | 2 | 北京 || s25304 | 欧阳俊雄 | 男 | 28 | 4 | 天津 |+--------+----------+--------+--------+---------+------------+2 rows in set (0.00 sec) 强调 123451、通过declare关键字声明局部变量；全局变量@开头就可以了2、给变量赋值有两种方法 方法一：set 变量名=值 方法二：select 字段 into 变量 from 表 where 条件3、声明的变量不能与列名同名 例题三：输出参数 12345678910111213141516mysql&gt; create procedure proc(num int, out result int) //out 表示输出参数 -&gt; begin -&gt; set result=num*num; -&gt; end //Query OK, 0 rows affected (0.00 sec)mysql&gt; call proc(10,@result) //Query OK, 0 rows affected (0.00 sec)mysql&gt; select @result //+---------+| @result |+---------+| 100 |+---------+1 row in set (0.00 sec) 例题四：输入输出参数 12345678910111213141516171819mysql&gt; create procedure proc(inout num int) # inout 表示是输入输出参数 -&gt; begin -&gt; set num=num*num; -&gt; end //Query OK, 0 rows affected (0.00 sec)mysql&gt; set @num=10; -&gt; call proc(@num); -&gt; select @num //Query OK, 0 rows affected (0.00 sec)Query OK, 0 rows affected (0.00 sec)+------+| @num |+------+| 100 |+------+1 row in set (0.00 sec)]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL基本操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库基本操作16-简单函数处理]]></title>
    <url>%2F2018%2F09%2F17%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C16-%E7%AE%80%E5%8D%95%E5%87%BD%E6%95%B0%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[1.7 函数1.7.1 数字类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950mysql&gt; select rand(); # 生成随机数+---------------------+| rand() |+---------------------+| 0.18474003969201822 |+---------------------+1 row in set (0.00 sec)mysql&gt; select * from stuinfo order by rand(); # 随机排序mysql&gt; select * from stuinfo order by rand() limit 2; # 随机抽两个学生+--------+----------+--------+--------+---------+------------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress |+--------+----------+--------+--------+---------+------------+| s25305 | 诸葛丽丽 | 女 | 23 | 7 | 河南 || s25304 | 欧阳俊雄 | 男 | 28 | 4 | 天津 |+--------+----------+--------+--------+---------+------------+2 rows in set (0.00 sec)mysql&gt; select round(3.5); #四舍五入+------------+| round(3.5) |+------------+| 4 |+------------+1 row in set (0.00 sec)mysql&gt; select ceil(3.1); # 向上取整+-----------+| ceil(3.1) |+-----------+| 4 |+-----------+1 row in set (0.00 sec)mysql&gt; select floor(3.9); # 向下取整+------------+| floor(3.9) |+------------+| 3 |+------------+1 row in set (0.00 sec)mysql&gt; select truncate(3.1415926,3); # 截取数字+-----------------------+| truncate(3.1415926,3) |+-----------------------+| 3.141 |+-----------------------+1 row in set (0.00 sec) 1.7.2 字符串类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091mysql&gt; select ucase(&apos;i am a boy!&apos;); # 转成大写+----------------------+| ucase(&apos;i am a boy!&apos;) |+----------------------+| I AM A BOY! |+----------------------+1 row in set (0.00 sec)mysql&gt; select lcase(&apos;I Am A Boy!&apos;); #转成小写+----------------------+| lcase(&apos;I Am A Boy!&apos;) |+----------------------+| i am a boy! |+----------------------+1 row in set (0.00 sec)mysql&gt; select left(&apos;abcde&apos;,3); # 从左边开始截取，截取3个+-----------------+| left(&apos;abcde&apos;,3) |+-----------------+| abc |+-----------------+1 row in set (0.00 sec)mysql&gt; select right(&apos;abcde&apos;,3); # 从右边开始截取，截取3个+------------------+| right(&apos;abcde&apos;,3) |+------------------+| cde |+------------------+1 row in set (0.00 sec)mysql&gt; select substring(&apos;abcde&apos;,2,3); #从第2个位置开始截取，截取3个【位置从1开始】+------------------------+| substring(&apos;abcde&apos;,2,3) |+------------------------+| bcd |+------------------------+1 row in set (0.00 sec)mysql&gt; select concat(&apos;中国&apos;,&apos;上海&apos;); # 字符串相连+-----------------------+| concat(&apos;中国&apos;,&apos;上海&apos;) |+-----------------------+| 中国上海 |+-----------------------+1 row in set (0.00 sec)mysql&gt; select concat(stuname,&apos;-&apos;,stusex) from stuinfo; # 将表中的姓名和性别连接起来+----------------------------+| concat(stuname,&apos;-&apos;,stusex) |+----------------------------+| 张秋丽-男 || 李文才-男 || 李斯文-女 || 欧阳俊雄-男 || 诸葛丽丽-女 || 争青小子-男 || 梅超风-女 |+----------------------------+7 rows in set (0.00 sec)# coalesce(字段1，字段2) 如果字段1不为空就显示字段1，否则，显示字段2mysql&gt; select stuname,coalesce(writtenexam,&apos;缺考&apos;),coalesce(labexam,&apos;缺考&apos;) from stuinfo natural left join stumarks; # 将考试成绩为空的显示为缺考+----------+------------------------------+--------------------------+| stuname | coalesce(writtenexam,&apos;缺考&apos;) | coalesce(labexam,&apos;缺考&apos;) |+----------+------------------------------+--------------------------+| 张秋丽 | 77 | 82 || 李文才 | 50 | 90 || 李斯文 | 88 | 58 || 欧阳俊雄 | 65 | 50 || 诸葛丽丽 | 缺考 | 缺考 || 争青小子 | 56 | 48 || 梅超风 | 缺考 | 缺考 |+----------+------------------------------+--------------------------+mysql&gt; select length(&apos;锄禾日当午&apos;); # 字节长度+----------------------+| length(&apos;锄禾日当午&apos;) |+----------------------+| 10 |+----------------------+1 row in set (0.00 sec)mysql&gt; select char_length(&apos;锄禾日当午&apos;); # 字符个数+---------------------------+| char_length(&apos;锄禾日当午&apos;) |+---------------------------+| 5 |+---------------------------+1 row in set (0.00 sec) 1.7.3 时间类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162mysql&gt; select unix_timestamp(); #获取时间戳+------------------+| unix_timestamp() |+------------------+| 1537084508 |+------------------+1 row in set (0.00 sec)mysql&gt; select from_unixtime(unix_timestamp()); # 将时间戳转成年-月-日 小时:分钟:秒的格式+---------------------------------+| from_unixtime(unix_timestamp()) |+---------------------------------+| 2018-09-16 15:55:56 |+---------------------------------+1 row in set (0.00 sec)mysql&gt; select now(); # 获取当前日期时间+---------------------+| now() |+---------------------+| 2018-09-16 15:57:04 |+---------------------+1 row in set (0.00 sec)mysql&gt; select year(now()) 年,month(now()) 月, day(now()) 日,hour(now()) 小,minute(now()) 分钟,second(now()) 秒;+------+------+------+------+------+------+| 年 | 月 | 日 | 小时 | 分钟 | 秒 |+------+------+------+------+------+------+| 2018 | 9 | 16 | 15 | 59 | 14 |+------+------+------+------+------+------+1 row in set (0.00 sec)mysql&gt; select dayname(now()) 星期,monthname(now()),dayofyear(now()) 本年的第几天;+--------+------------------+--------------+| 星期 | monthname(now()) | 本年的第几天 |+--------+------------------+--------------+| Sunday | September | 259 |+--------+------------------+--------------+1 row in set (0.00 sec)mysql&gt; select datediff(now(),&apos;2008-8-8&apos;); # 日期相减+----------------------------+| datediff(now(),&apos;2008-8-8&apos;) |+----------------------------+| 3691 |+----------------------------+1 row in set (0.00 sec)mysql&gt; select convert(now(),date),convert(now(),time); # 将now()转成日期和时间+---------------------+---------------------+| convert(now(),date) | convert(now(),time) |+---------------------+---------------------+| 2018-09-16 | 16:07:24 |+---------------------+---------------------+mysql&gt; select cast(now() as date),cast(now() as time); # 将now()转成日期和时间+---------------------+---------------------+| cast(now() as date) | cast(now() as time) |+---------------------+---------------------+| 2018-09-16 | 16:08:03 |+---------------------+---------------------+1 row in set (0.00 sec) 1.7.4 加密函数123456+----------------------------------+------------------------------------------+| md5(&apos;root&apos;) | sha(&apos;root&apos;) |+----------------------------------+------------------------------------------+| 63a9f0ea7bb98050796b649e85481845 | dc76e9f0c0006e8f919e0c515c66dbba3982f785 |+----------------------------------+------------------------------------------+1 row in set (0.00 sec) 1.7.5 判断函数语法 1if(表达式,值1,值2) 例题： 123456789101112131415161718192021222324mysql&gt; select if(10%2=0,&apos;偶数&apos;,&apos;奇数&apos;);+--------------------------+| if(10%2=0,&apos;偶数&apos;,&apos;奇数&apos;) |+--------------------------+| 偶数 |+--------------------------+1 row in set (0.00 sec)# 语文和数学都超过60分才通过mysql&gt; select stuname,ch,math,if(ch&gt;=60 &amp;&amp; math&gt;=60,&apos;通过&apos;,&apos;不通过&apos;) &apos;是否通过&apos; from stu;+----------+------+------+----------+| stuname | ch | math | 是否通过 |+----------+------+------+----------+| 张秋丽 | 80 | NULL | 不通过 || 李文才 | 77 | 76 | 通过 || 李斯文 | 55 | 82 | 不通过 || 欧阳俊雄 | NULL | 74 | 不通过 || 诸葛丽丽 | 72 | 56 | 不通过 || 争青小子 | 86 | 92 | 通过 || 梅超风 | 74 | 67 | 通过 || Tom | 65 | 67 | 通过 || Tabm | 88 | 77 | 通过 |+----------+------+------+----------+9 rows in set (0.00 sec)]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL基本操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库基本操作15-事务及索引]]></title>
    <url>%2F2018%2F09%2F17%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C15-%E4%BA%8B%E5%8A%A1%E5%8F%8A%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[1.5 事务【transaction】 事务是一个不可分割的执行单元 事务作为一个整体要么一起执行，要么一起回滚 插入测试数据 123456789mysql&gt; create table bank( -&gt; cardid char(4) primary key, -&gt; money int -&gt; );Query OK, 0 rows affected (0.00 sec)mysql&gt; insert into bank values (&apos;1001&apos;,1000),(&apos;1002&apos;,100);Query OK, 2 rows affected (0.00 sec)Records: 2 Duplicates: 0 Warnings: 0 1.5.1 事务操作123开启事务：start transaction或begin [work]提交事务：commit回滚事务：rollback 例题： 12345678910mysql&gt; delimiter // # 更改定界符mysql&gt; start transaction; # 开启事务 -&gt; update bank set money=money-100 where cardid=&apos;1001&apos;; -&gt; update bank set money=money+100 where cardid=&apos;1002&apos; //Query OK, 0 rows affected (0.00 sec)mysql&gt; commit // # 提交事务mysql&gt; rollback // # 回滚事务 1234思考：事务什么时候产生？什么时候结束？答：开启的时候产生，提交事务或回滚事务都结束脚下留心：只有innodb和BDB才支持事务，myisam不支持事务。 1.5.2 设置事务的回滚点语法： 12设置回滚点： savepoint 回滚点名回滚到回滚点： rollback to 回滚点 例题： 12345678910111213141516171819202122232425262728mysql&gt; start transaction;Query OK, 0 rows affected (0.00 sec)mysql&gt; insert into bank values (&apos;1003&apos;,1000);Query OK, 1 row affected (0.00 sec)mysql&gt; savepoint aa; # 设置回滚点 aaQuery OK, 0 rows affected (0.00 sec)mysql&gt; insert into bank values (&apos;1004&apos;,500);Query OK, 1 row affected (0.00 sec) mysql&gt; savepoint bb; # 设置回滚点bbQuery OK, 0 rows affected (0.00 sec) mysql&gt; rollback to aa; # 回滚到aa点Query OK, 0 rows affected (0.00 sec)mysql&gt; commit; # 提交事务mysql&gt; select * from bank ;+--------+-------+| cardid | money |+--------+-------+| 1001 | 800 || 1002 | 200 || 1003 | 1000 |+--------+-------+ 1.5.3 事务的特性（ACID） 原子性（Atomicity）：事务是一个整体，不可以再分，要么一起执行，要么一起不执行。 一致性（Consistency）：事务完成时，数据必须处于一致的状态。 隔离性（Isolation）：每个事务都是相互隔离的 永久性（Durability）：事务完成后，对数据的修改是永久性的。 1.6 索引【index】 索引的优点：查询速度快 索引的缺点： 增、删、改（数据操作语句）效率低了 索引占用空间 1.6.1 索引的类型 普通索引 唯一索引（唯一键） 主键索引：只要主键就自动创建主键索引，不需要手动创建。 全文索引，搜索引擎使用，MySQL不支持中文的全文索引，我们通过sphinx去解决中文的全文索引。 1.6.2 创建普通索引【create index】 语法： 12create index [索引名] on 表名 （字段名）alter table 表名 add index [索引的名称] （列名） 例题： 1234567891011121314151617# 创建索引方法一mysql&gt; create index ix_stuname on stuinfo(stuname);Query OK, 0 rows affected (0.08 sec)Records: 0 Duplicates: 0 Warnings: 0# 创建索引方法二mysql&gt; alter table stuinfo add index ix_address (stuaddress);Query OK, 0 rows affected (0.08 sec)Records: 0 Duplicates: 0 Warnings: 0# 创建表的时候就添加索引mysql&gt; create table emp( -&gt; id int, -&gt; name varchar(10), -&gt; index ix_name (name) # 创建索引 -&gt; );Query OK, 0 rows affected (0.00 sec) 1.6.3 创建唯一索引123语法一：create unique index 索引名 on 表名 （字段名）语法二：alter table 表名 add unqiue [index] [索引的名称] （列名）语法三：创建表的时候添加唯一索引，和创建唯一键是一样的。 例题 1234567891011121314151617# 方法一：mysql&gt; create unique index UQ_stuname on stu(stuname);Query OK, 0 rows affected (0.06 sec)Records: 0 Duplicates: 0 Warnings: 0# 方法二：mysql&gt; alter table stu add unique UQ_address (stuaddress);Query OK, 0 rows affected (0.02 sec)Records: 0 Duplicates: 0 Warnings: 0# 方法三mysql&gt; create table stu2( -&gt; id int, -&gt; name varchar(20), -&gt; unique UQ_name(name) -&gt; );Query OK, 0 rows affected (0.01 sec) 1.6.4 删除索引语法 1drop index 索引名 on 表名 例题 123mysql&gt; drop index ix_stuname on stuinfo;Query OK, 0 rows affected (0.03 sec)Records: 0 Duplicates: 0 Warnings: 0 1.6.5 创建索引的指导原则 该列用于频繁搜索 改列用于排序 公共字段要创建索引 如果表中的数据很少，不需要创建索引。MySQL搜索索引的时间比逐条搜索数据的时间要长。 如果一个字段上的数据只有几个不同的值，改字段不适合做索引，比如性别。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL基本操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库基本操作14-视图及视图算法]]></title>
    <url>%2F2018%2F09%2F17%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C14-%E8%A7%86%E5%9B%BE%E5%8F%8A%E8%A7%86%E5%9B%BE%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[1.4 视图【view】1、 视图是一张虚拟表，它表示一张表的部分或多张表的综合的结构。 2、 视图仅仅是表结构，没有表数据。视图的结构和数据建立在表的基础上。 1.4.1 创建视图语法 123create [or replace] view 视图的名称as select语句 例题： 1234mysql&gt; create view vw_stu -&gt; as -&gt; select stuname,stusex,writtenexam,labexam from stuinfo inner join stumarks using(stuno);Query OK, 0 rows affected (0.00 sec) 1多学一招：因为视图是一个表结构，所以创建视图后，会在数据库文件夹中多一个与视图名同名的.frm文件 1.4.2 使用视图视图是一张虚拟表，视图的用法和表的用法一样 1234567891011121314mysql&gt; select * from vw_stu;+----------+--------+-------------+---------+| stuname | stusex | writtenexam | labexam |+----------+--------+-------------+---------+| 李斯文 | 女 | 80 | 58 || 李文才 | 男 | 50 | 90 || 欧阳俊雄 | 男 | 65 | 50 || 张秋丽 | 男 | 77 | 82 || 争青小子 | 男 | 56 | 48 |+----------+--------+-------------+---------+mysql&gt; update vw_stu set writtenexam=88 where stuname=&apos;李斯文&apos;;Query OK, 1 row affected (0.05 sec)Rows matched: 1 Changed: 1 Warnings: 0 1.4.3 查看视图的结构语法： 1desc 视图名 例题 123456789mysql&gt; desc vw_stu;+-------------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------------+-------------+------+-----+---------+-------+| stuname | varchar(10) | NO | | NULL | || stusex | char(2) | NO | | NULL | || writtenexam | int(11) | YES | | NULL | || labexam | int(11) | YES | | NULL | |+-------------+-------------+------+-----+---------+-------+ 1.4.4 查看创建视图的语法语法： 1show create view 视图名 例题 1.4.5 显示所有视图1234567891011121314151617181920212223242526272829303132333435363738394041424344 #方法一：mysql&gt; show tables;+------------------+| Tables_in_itcast |+------------------+| stu || stuinfo || stumarks || t1 || t2 || vw_stu |# 方法二mysql&gt; select table_name from information_schema.views;+------------+| table_name |+------------+| vw_stu |+------------+1 row in set (0.05 sec)+------------------+#方法三mysql&gt; show table status where comment=&apos;view&apos; \G*************************** 1. row *************************** Name: vw_stu Engine: NULL Version: NULL Row_format: NULL Rows: NULL Avg_row_length: NULL Data_length: NULLMax_data_length: NULL Index_length: NULL Data_free: NULL Auto_increment: NULL Create_time: NULL Update_time: NULL Check_time: NULL Collation: NULL Checksum: NULL Create_options: NULL Comment: VIEW1 row in set (0.00 sec) 1.4.6 更改视图语法： 123alter view 视图名as select 语句 例题： 1234mysql&gt; alter view vw_stu -&gt; as -&gt; select * from stuinfo;Query OK, 0 rows affected (0.00 sec) 1.4.7 删除视图语法： 1drop view [if exists] 视图1,视图2,… 例题 12mysql&gt; drop view vw_stu;Query OK, 0 rows affected (0.00 sec) 1.4.8 视图的作用 筛选数据，防止未经许可访问敏感数据 隐藏表结构 降低SQL语句的复杂度 1.4.9 视图的算法 场景：找出语文成绩最高的男生和女生 1234567mysql&gt; select * from (select * from stu order by ch desc) as t group by stusex;+--------+----------+--------+--------+---------+------------+------+------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress | ch | math |+--------+----------+--------+--------+---------+------------+------+------+| s25321 | Tabm | 女 | 23 | 9 | 河北 | 88 | 77 || s25318 | 争青小子 | 男 | 26 | 6 | 天津 | 86 | 92 |+--------+----------+--------+--------+---------+------------+------+------+ 我们可以将子查询封装到视图中 1234mysql&gt; create view vw_stu -&gt; as -&gt; select * from stu order by ch desc;Query OK, 0 rows affected (0.00 sec) 可以将上面的子查询更改成视图，但是，结果和上面不一样 1234567mysql&gt; select * from vw_stu group by stusex;+--------+---------+--------+--------+---------+------------+------+------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress | ch | math |+--------+---------+--------+--------+---------+------------+------+------+| s25301 | 张秋丽 | 男 | 18 | 1 | 北京 | 80 | NULL || s25303 | 李斯文 | 女 | 22 | 2 | 北京 | 55 | 82 |+--------+---------+--------+--------+---------+------------+------+------+ 原因：这是因为视图的算法造成的 1231. merge：合并算法，将视图的语句和外层的语句合并后在执行。2. temptable：临时表算法，将视图生成一个临时表，再执行外层语句3. undefined：未定义，MySQL到底用merge还是用temptable由MySQL决定，这是一个默认的算法，一般视图都会选择merge算法，因为merge效率高。 解决：在创建视图的时候指定视图的算法 123create algorithm=temptable view 视图名as select 语句 指定算法创建视图 123456789101112mysql&gt; create algorithm=temptable view vw_stu -&gt; as -&gt; select * from stu order by ch desc;Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from vw_stu group by stusex; # 结果是一致的+--------+----------+--------+--------+---------+------------+------+------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress | ch | math |+--------+----------+--------+--------+---------+------------+------+------+| s25321 | Tabm | 女 | 23 | 9 | 河北 | 88 | 77 || s25318 | 争青小子 | 男 | 26 | 6 | 天津 | 86 | 92 |+--------+----------+--------+--------+---------+------------+------+------+]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL基本操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库基本操作13-子查询]]></title>
    <url>%2F2018%2F09%2F17%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C13-%E5%AD%90%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[1.3 子查询语法 1语法：select 语句 where 条件 (select … from 表) 外面的查询称为父查询，括号中的查询称为子查询 子查询为父查询提供查询条件 1.3.1 例题 1、查找笔试80分的学生 123456mysql&gt; select * from stuinfo where stuno=(select stuno from stumarks where writtenexam=80);+--------+---------+--------+--------+---------+------------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress |+--------+---------+--------+--------+---------+------------+| s25303 | 李斯文 | 女 | 22 | 2 | 北京 |+--------+---------+--------+--------+---------+------------+ 2、查找笔试最高分的学生 123456789101112131415161718# 方法一：mysql&gt; select * from stuinfo where stuno=(select stuno from stumarks order by writtenexam desc limit 1);+--------+---------+--------+--------+---------+------------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress |+--------+---------+--------+--------+---------+------------+| s25303 | 李斯文 | 女 | 22 | 2 | 北京 |+--------+---------+--------+--------+---------+------------+1 row in set (0.00 sec)# 方法二：mysql&gt; select * from stuinfo where stuno=(select stuno from stumarks where writtenexam=(select max(writtenexam) from stumarks));+--------+---------+--------+--------+---------+------------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress |+--------+---------+--------+--------+---------+------------+| s25303 | 李斯文 | 女 | 22 | 2 | 北京 |+--------+---------+--------+--------+---------+------------+1 row in set (0.00 sec) 1脚下留心：上面的例题，子查询只能返回一个值。如果子查询返回多个值就不能用“=”了,需要用 in 1.3.2 in|not in子查询用于子查询的返回结果多个值。 1、查找笔试成绩及格的同学 123456789mysql&gt; select * from stuinfo where stuno in (select stuno from stumarks where writtenexam&gt;=60);+--------+----------+--------+--------+---------+------------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress |+--------+----------+--------+--------+---------+------------+| s25301 | 张秋丽 | 男 | 18 | 1 | 北京 || s25303 | 李斯文 | 女 | 22 | 2 | 北京 || s25304 | 欧阳俊雄 | 男 | 28 | 4 | 天津 |+--------+----------+--------+--------+---------+------------+3 rows in set (0.00 sec) 2、查询不及格的同学 1234567mysql&gt; select * from stuinfo where stuno in (select stuno from stumarks where writtenexam&lt;=60);+--------+----------+--------+--------+---------+------------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress |+--------+----------+--------+--------+---------+------------+| s25302 | 李文才 | 男 | 31 | 3 | 上海 || s25318 | 争青小子 | 男 | 26 | 6 | 天津 |+--------+----------+--------+--------+---------+------------+ 3、查询没有通过的同学（不及格，缺考） 12345678910mysql&gt; select * from stuinfo where stuno not in (select stuno from stumarks where writtenexam&gt;=60);+--------+----------+--------+--------+---------+------------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress |+--------+----------+--------+--------+---------+------------+| s25302 | 李文才 | 男 | 31 | 3 | 上海 || s25305 | 诸葛丽丽 | 女 | 23 | 7 | 河南 || s25318 | 争青小子 | 男 | 26 | 6 | 天津 || s25319 | 梅超风 | 女 | 23 | 5 | 河北 |+--------+----------+--------+--------+---------+------------+4 rows in set (0.00 sec) 1.3.3 exists和not exists1、 如果有人笔试超过80分就显示所有的学生 123456789101112mysql&gt; select * from stuinfo where exists (select * from stumarks where writtenexam&gt;=80);+--------+----------+--------+--------+---------+------------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress |+--------+----------+--------+--------+---------+------------+| s25301 | 张秋丽 | 男 | 18 | 1 | 北京 || s25302 | 李文才 | 男 | 31 | 3 | 上海 || s25303 | 李斯文 | 女 | 22 | 2 | 北京 || s25304 | 欧阳俊雄 | 男 | 28 | 4 | 天津 || s25305 | 诸葛丽丽 | 女 | 23 | 7 | 河南 || s25318 | 争青小子 | 男 | 26 | 6 | 天津 || s25319 | 梅超风 | 女 | 23 | 5 | 河北 |+--------+----------+--------+--------+---------+------------+ 2、 如果没有人超过80分就显示所有的学生 12mysql&gt; select * from stuinfo where not exists (select * from stumarks where writtenexam&gt;=80);Empty set (0.02 sec) 1.3.4 子查询分类1、标量子查询：子查询返回的结果就一个 2、列子查询：子查询返回的结果是一个列表 3、行子查询：子查询返回的结果是一行 例题：查询成绩最高的男生和女生 1234567mysql&gt; select stuname,stusex,ch from stu where (stusex,ch) in (select stusex,max(ch) from stu group by stusex);+----------+--------+------+| stuname | stusex | ch |+----------+--------+------+| 争青小子 | 男 | 86 || Tabm | 女 | 88 |+----------+--------+------+ 4、表子查询：子查询返回的结果当成一个表 例题：查询成绩最高的男生和女生 1234567mysql&gt; select stuname,stusex,ch from (select * from stu order by ch desc) as t group by stusex;+----------+--------+------+| stuname | stusex | ch |+----------+--------+------+| Tabm | 女 | 88 || 争青小子 | 男 | 86 |+----------+--------+------+ 1脚下留心：from后面是一个表，如果子查询的结果当成表来看，必须将子查询的结果取别名。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL基本操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库基本操作12-多表查询1]]></title>
    <url>%2F2018%2F09%2F17%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C12-%E5%A4%9A%E8%A1%A8%E6%9F%A5%E8%AF%A21%2F</url>
    <content type="text"><![CDATA[1.1 目标 理解多表查询 理解子查询 能够创建视图 能够删除视图 能够查看创建视图的SQL语句 能够理解事务的作用 能够操作事务 理解索引的作用 能够创建索引 能够删除索引 知道常用的函数 了解预处理语句的作用 能够使用预处理语句 了解存储过程的作用 能够创建存储过程 能够调用存储过程 1.2 多表查询分类将多个表的数据横向的联合起来。1、 内连接2、 外连接 左外连接 右外连接3、 交叉连接4、 自然连接 1.2.1 内连接【inner join】123语法一：select 列名 from 表1 inner join 表2 on 表1.公共字段=表2.公共字段语法二：select 列名 from 表1,表2 where 表1.公共字段=表2.公共字段 例题 123456789101112131415161718192021222324252627282930313233343536方法一：mysql&gt; select stuname,stusex,writtenexam,labexam from stuinfo inner join stumarks on stuinfo.stuno=stumarks.stuno;+----------+--------+-------------+---------+| stuname | stusex | writtenexam | labexam |+----------+--------+-------------+---------+| 李斯文 | 女 | 80 | 58 || 李文才 | 男 | 50 | 90 || 欧阳俊雄 | 男 | 65 | 50 || 张秋丽 | 男 | 77 | 82 || 争青小子 | 男 | 56 | 48 |+----------+--------+-------------+---------+方法二：mysql&gt; select stuinfo.stuno,stuname,stusex,writtenexam,labexam from stuinfo,stumarks where stuinfo.stuno=stumarks.stuno;+--------+----------+--------+-------------+---------+| stuno | stuname | stusex | writtenexam | labexam |+--------+----------+--------+-------------+---------+| s25303 | 李斯文 | 女 | 80 | 58 || s25302 | 李文才 | 男 | 50 | 90 || s25304 | 欧阳俊雄 | 男 | 65 | 50 || s25301 | 张秋丽 | 男 | 77 | 82 || s25318 | 争青小子 | 男 | 56 | 48 |+--------+----------+--------+-------------+---------+可以给表取别名mysql&gt; select i.stuno,stuname,stusex,writtenexam,labexam from stuinfo i,stumarks s where i.stuno=s.stuno;+--------+----------+--------+-------------+---------+| stuno | stuname | stusex | writtenexam | labexam |+--------+----------+--------+-------------+---------+| s25303 | 李斯文 | 女 | 80 | 58 || s25302 | 李文才 | 男 | 50 | 90 || s25304 | 欧阳俊雄 | 男 | 65 | 50 || s25301 | 张秋丽 | 男 | 77 | 82 || s25318 | 争青小子 | 男 | 56 | 48 |+--------+----------+--------+-------------+---------+5 rows in set (0.00 sec) 脚下留下：显示公共字段需要指定表名 1234思考：select * from 表1 inner join 表2 on 表1.公共字段=表2.公共字段 和select * from 表2 inner join 表1 on 表1.公共字段=表2.公共字段 结果是否一样？答：一样的，因为内连接获取的是两个表的公共部分 123多学一招：三个表的内连接如何实现？select * from 表1 inner join 表2 on 表1.公共字段=表2.公共字段inner join 表3 on 表2.公共字段=表3.公共字段 1.2.2 左外连接【left join】以左边的表为标准，如果右边的表没有对应的记录，用NULL填充。 1语法：select 列名 from 表1 left join 表2 on 表1.公共字段=表2.公共字段 例题 123456789101112mysql&gt; select stuname,writtenexam,labexam from stuinfo left join stumarks on stuinfo.stuno=stumarks.stuno;+----------+-------------+---------+| stuname | writtenexam | labexam |+----------+-------------+---------+| 张秋丽 | 77 | 82 || 李文才 | 50 | 90 || 李斯文 | 80 | 58 || 欧阳俊雄 | 65 | 50 || 诸葛丽丽 | NULL | NULL || 争青小子 | 56 | 48 || 梅超风 | NULL | NULL |+----------+-------------+---------+ 12345思考：select * from 表1 left join 表2 on 表1.公共字段=表2.公共字段和select * from 表2 left join 表1 on 表1.公共字段=表2.公共字段 是否一样？答：不一样，左连接一左边的表为准。 1.2.3 右外连接【right join】以右边的表为标准，如果左边的表没有对应的记录，用NULL填充。 1语法：select 列名 from 表1 right join 表2 on 表1.公共字段=表2.公共字段 例题 123456789101112mysql&gt; select stuname,writtenexam,labexam from stuinfo right join stumarks on stuinfo.stuno=stumarks.stuno;+----------+-------------+---------+| stuname | writtenexam | labexam |+----------+-------------+---------+| 李斯文 | 80 | 58 || 李文才 | 50 | 90 || 欧阳俊雄 | 65 | 50 || 张秋丽 | 77 | 82 || 争青小子 | 56 | 48 || NULL | 66 | 77 |+----------+-------------+---------+6 rows in set (0.00 sec) 123456思考：select * from 表1 left join 表2 on 表1.公共字段=表2.公共字段和select * from 表2 right join 表1 on 表1.公共字段=表2.公共字段 是否一样？答：一样的 1.2.4 交叉连接【cross join】插入测试数据 123456789101112131415mysql&gt; create table t1( -&gt; id int, -&gt; name varchar(10) -&gt; );Query OK, 0 rows affected (0.06 sec)mysql&gt; insert into t1 values (1,&apos;tom&apos;),(2,&apos;berry&apos;);Query OK, 2 rows affected (0.00 sec)mysql&gt; create table t2( -&gt; id int, -&gt; score int);Query OK, 0 rows affected (0.02 sec)mysql&gt; insert into t2 values (1,88),(2,99); 1、如果没有连接表达式返回的是笛卡尔积 123456789mysql&gt; select * from t1 cross join t2; # 返回笛卡尔积+------+-------+------+-------+| id | name | id | score |+------+-------+------+-------+| 1 | tom | 1 | 88 || 2 | berry | 1 | 88 || 1 | tom | 2 | 99 || 2 | berry | 2 | 99 |+------+-------+------+-------+ 2、如果有连接表达式等价于内连接 1234567mysql&gt; select * from t1 cross join t2 where t1.id=t2.id;+------+-------+------+-------+| id | name | id | score |+------+-------+------+-------+| 1 | tom | 1 | 88 || 2 | berry | 2 | 99 |+------+-------+------+-------+ 1.2.5 自然连接【natural】1自动的判断连接条件，它是过同名字段来判断的 自然连接又分为： 自然内连接 natural join. 自然左外连接 natural left join. 自然右外连接 natural right join 例题： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859# 自然内连接mysql&gt; select * from stuinfo natural join stumarks;+--------+----------+--------+--------+---------+------------+---------+-------------+---------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress | examNo | writtenExam | labExam |+--------+----------+--------+--------+---------+------------+---------+-------------+---------+| s25303 | 李斯文 | 女 | 22 | 2 | 北京 | s271811 | 80 | 58 || s25302 | 李文才 | 男 | 31 | 3 | 上海 | s271813 | 50 | 90 || s25304 | 欧阳俊雄 | 男 | 28 | 4 | 天津 | s271815 | 65 | 50 || s25301 | 张秋丽 | 男 | 18 | 1 | 北京 | s271816 | 77 | 82 || s25318 | 争青小子 | 男 | 26 | 6 | 天津 | s271819 | 56 | 48 |+--------+----------+--------+--------+---------+------------+---------+-------------+---------+5 rows in set (0.00 sec)# 自然左外连接mysql&gt; select * from stuinfo natural left join stumarks;+--------+----------+--------+--------+---------+------------+---------+-------------+---------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress | examNo | writtenExam | labExam |+--------+----------+--------+--------+---------+------------+---------+-------------+---------+| s25301 | 张秋丽 | 男 | 18 | 1 | 北京 | s271816 | 77 82 || s25302 | 李文才 | 男 | 31 | 3 | 上海 | s271813 | 50 | 90 || s25303 | 李斯文 | 女 | 22 | 2 | 北京 | s271811 | 80 | 58 || s25304 | 欧阳俊雄 | 男 | 28 | 4 | 天津 | s271815 | 65 50 || s25305 | 诸葛丽丽 | 女 | 23 | 7 | 河南 | NULL | NULL NULL || s25318 | 争青小子 | 男 | 26 | 6 | 天津 | s271819 | 56 48 || s25319 | 梅超风 | 女 | 23 | 5 | 河北 | NULL | NULL |ULL |+--------+----------+--------+--------+---------+------------+---------+-------------+---------+7 rows in set (0.00 sec)# 自然右外连接mysql&gt; select * from stuinfo natural right join stumarks;+--------+---------+-------------+---------+----------+--------+--------+---------+------------+| stuNo | examNo | writtenExam | labExam | stuName | stuSex | stuAge | stuSeat | stuAddress |+--------+---------+-------------+---------+----------+--------+--------+---------+------------+| s25303 | s271811 | 80 | 58 | 李斯文 | 女 | 22 | 2 | 北京 || s25302 | s271813 | 50 | 90 | 李文才 | 男 | 31 | 3 | 上海 || s25304 | s271815 | 65 | 50 | 欧阳俊雄 | 男 | 28 | 4 | 天津 || s25301 | s271816 | 77 | 82 | 张秋丽 | 男 | 18 | 1 | 北京 || s25318 | s271819 | 56 | 48 | 争青小子 | 男 | 26 | 6 | 天津 || s25320 | s271820 | 66 | 77 | NULL | NULL | NULL | NULL | NULL |+--------+---------+-------------+---------+----------+--------+--------+---------+------------+6 rows in set (0.00 sec) 自然连接结论： 表连接通过同名的字段来连接的 如果没有同名的字段返回笛卡尔积 会对结果进行整理，整理的规则如下 a) 连接字段保留一个 b) 连接字段放在最前面 c) 左外连接左边在前，右外连接右表在前 1.2.6 using() 用来指定连接字段。 using()也会对连接字段进行整理，整理方式和自然连接是一样的。 12345678910111213141516mysql&gt; select * from stuinfo inner join stumarks using(stuno); # using指定字段+--------+----------+--------+--------+---------+------------+---------+-------------+---------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress | examNo | writtenExam | labExam |+--------+----------+--------+--------+---------+------------+---------+-------------+---------+| s25303 | 李斯文 | 女 | 22 | 2 | 北京 | s271811 | 80 | 58 || s25302 | 李文才 | 男 | 31 | 3 | 上海 | s271813 | 50 | 90 || s25304 | 欧阳俊雄 | 男 | 28 | 4 | 天津 | s271815 | 65 | 50 || s25301 | 张秋丽 | 男 | 18 | 1 | 北京 | s271816 | 77 | 82 || s25318 | 争青小子 | 男 | 26 | 6 | 天津 | s271819 | 56 | 48 |+--------+----------+--------+--------+---------+------------+---------+-------------+---------+5 rows in set (0.00 sec)]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL基本操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-mv命令]]></title>
    <url>%2F2018%2F09%2F16%2FLinux%E5%91%BD%E4%BB%A4-mv%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[mv命令用来对文件或目录重新命名，或者将文件从一个目录移到另一个目录中。source表示源文件或目录，target表示目标文件或目录。如果将一个文件移到一个已经存在的目标文件中，则目标文件的内容将被覆盖。 mv命令可以用来将源文件移至一个目标文件中，或将一组文件移至一个目标目录中。源文件被移至目标文件有两种不同的结果： 如果目标文件是到某一目录文件的路径，源文件会被移到此目录下，且文件名不变。 如果目标文件不是目录文件，则源文件名（只能有一个）会变为此目标文件名，并覆盖己存在的同名文件。如果源文件和目标文件在同一个目录下，mv的作用就是改文件名。当目标文件是目录文件时，源文件或目录参数可以有多个，则所有的源文件都会被移至目标文件中。所有移到该目录下的文件都将保留以前的文件名。 注意事项：mv与cp的结果不同，mv好像文件“搬家”，文件个数并未增加。而cp对文件进行复制，文件个数增加了。 语法mv(选项)(参数) 选项12345678--backup=&lt;备份模式&gt;：若需覆盖文件，则覆盖前先行备份；-b：当文件存在时，覆盖前，为其创建一个备份；-f：若目标文件或目录与现有的文件或目录重复，则直接覆盖现有的文件或目录；-i：交互式操作，覆盖前先行询问用户，如果源文件与目标文件或目标目录中的文件同名，则询问用户是否覆盖目标文件。用户输入”y”，表示将覆盖目标文件；输入”n”，表示取消对源文件的移动。这样可以避免误将文件覆盖。--strip-trailing-slashes：删除源文件中的斜杠“/”；-S&lt;后缀&gt;：为备份文件指定后缀，而不使用默认的后缀；-t:--target-directory=&lt;目录&gt;,指定源文件要移动到目标目录；-u：当源文件比目标文件新或者目标文件不存在时，才执行移动操作。 参数 源文件：源文件列表。 目标文件：如果“目标文件”是文件名则在移动文件的同时，将其改名为“目标文件”；如果“目标文件”是目录名则将源文件移动到“目标文件”下。 常用范例1）文件改名 12345678910111213# ll总计 20drwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 10-25 17:46 test3drwxr-xr-x 2 root root 4096 10-25 17:56 test4drwxr-xr-x 3 root root 4096 10-25 17:56 test5-rw-r--r-- 1 root root 16 10-28 06:04 test.log# mv test.log test1.txt# ll总计 20drwxr-xr-x 6 root root 4096 10-27 01:58 scf-rw-r--r-- 1 root root 16 10-28 06:04 test1.txtdrwxrwxrwx 2 root root 4096 10-25 17:46 test3drwxr-xr-x 2 root root 4096 10-25 17:56 test4drwxr-xr-x 3 root root 4096 10-25 17:56 test5 2）移动文件 12345678910111213141516# ll总计 20drwxr-xr-x 6 root root 4096 10-27 01:58 scf-rw-r--r-- 1 root root 29 10-28 06:05 test1.txtdrwxrwxrwx 2 root root 4096 10-25 17:46 test3drwxr-xr-x 2 root root 4096 10-25 17:56 test4drwxr-xr-x 3 root root 4096 10-25 17:56 test5# mv test1.txt test3# ll总计 16drwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 2 root root 4096 10-28 06:09 test3drwxr-xr-x 2 root root 4096 10-25 17:56 test4drwxr-xr-x 3 root root 4096 10-25 17:56 test5# cd test3# ll总计 4-rw-r--r-- 1 root root 29 10-28 06:05 test1.txt 3）将文件log1.txt,log2.txt,log3.txt移动到目录test3中 12345678910111213141516171819202122232425262728293031# ll总计 28-rw-r--r-- 1 root root 8 10-28 06:15 log1.txt-rw-r--r-- 1 root root 12 10-28 06:15 log2.txt-rw-r--r-- 1 root root 13 10-28 06:16 log3.txtdrwxrwxrwx 2 root root 4096 10-28 06:09 test3# mv log1.txt log2.txt log3.txt test3# ll总计 16drwxrwxrwx 2 root root 4096 10-28 06:18 test3# cd test3/# ll总计 16-rw-r--r-- 1 root root 8 10-28 06:15 log1.txt-rw-r--r-- 1 root root 12 10-28 06:15 log2.txt-rw-r--r-- 1 root root 13 10-28 06:16 log3.txt-rw-r--r-- 1 root root 29 10-28 06:05 test1.txt# ll总计 20-rw-r--r-- 1 root root 8 10-28 06:15 log1.txt-rw-r--r-- 1 root root 12 10-28 06:15 log2.txt-rw-r--r-- 1 root root 13 10-28 06:16 log3.txtdrwxr-xr-x 2 root root 4096 10-28 06:21 logs-rw-r--r-- 1 root root 29 10-28 06:05 test1.txt# mv -t /opt/soft/test/test4/ log1.txt log2.txt log3.txt ]# cd ..# cd test4/# ll总计 12-rw-r--r-- 1 root root 8 10-28 06:15 log1.txt-rw-r--r-- 1 root root 12 10-28 06:15 log2.txt-rw-r--r-- 1 root root 13 10-28 06:16 log3.txt 4）将文件file1改名为file2，如果file2已经存在，则询问是否覆盖 12345678910111213# ll总计 12-rw-r--r-- 1 root root 8 10-28 06:15 log1.txt-rw-r--r-- 1 root root 12 10-28 06:15 log2.txt-rw-r--r-- 1 root root 13 10-28 06:16 log3.txt# cat log1.txt odfdfs# cat log2.txt ererwerwer# mv -i log1.txt log2.txt mv：是否覆盖“log2.txt”? y# cat log2.txt odfdfs 5）将文件file1改名为file2，即使file2存在，也是直接覆盖掉 12345678910111213141516171819202122# ll总计 8-rw-r--r-- 1 root root 8 10-28 06:15 log2.txt-rw-r--r-- 1 root root 13 10-28 06:16 log3.txt# cat log2.txt odfdfs# cat log3cat: log3: 没有那个文件或目录# ll总计 8-rw-r--r-- 1 root root 8 10-28 06:15 log2.txt-rw-r--r-- 1 root root 13 10-28 06:16 log3.txt# cat log2.txt odfdfs# cat log3.txt dfosdfsdfdss# mv -f log3.txt log2.txt # cat log2.txt dfosdfsdfdss# ll总计 4-rw-r--r-- 1 root root 13 10-28 06:16 log2.txt 说明： log3.txt的内容直接覆盖了log2.txt内容，-f 这是个危险的选项，使用的时候一定要保持头脑清晰，一般情况下最好不用加上它。 6）目录的移动 12345678910111213141516171819202122232425ll-rw-r--r-- 1 root root 13 10-28 06:16 log2.txt# ll-rw-r--r-- 1 root root 13 10-28 06:16 log2.txt# cd ..# lldrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 3 root root 4096 10-28 06:24 test3drwxr-xr-x 2 root root 4096 10-28 06:48 test4drwxr-xr-x 3 root root 4096 10-25 17:56 test5# cd test3# lldrwxr-xr-x 2 root root 4096 10-28 06:21 logs-rw-r--r-- 1 root root 29 10-28 06:05 test1.txt# cd ..# mv test4 test3# lldrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 4 root root 4096 10-28 06:54 test3drwxr-xr-x 3 root root 4096 10-25 17:56 test5# cd test3/# lldrwxr-xr-x 2 root root 4096 10-28 06:21 log-rw-r--r-- 1 root root 29 10-28 06:05 test1.txtdrwxr-xr-x 2 root root 4096 10-28 06:48 test4 说明： 如果目录dir2不存在，将目录dir1改名为dir2；否则，将dir1移动到dir2中。 7）移动当前文件夹下的所有文件到上一级目录 123456789101112# ll-rw-r--r-- 1 root root 25 10-28 07:02 log1.txt-rw-r--r-- 1 root root 13 10-28 06:16 log2.txt# mv * ../# ll# cd ..# ll-rw-r--r-- 1 root root 25 10-28 07:02 log1.txt-rw-r--r-- 1 root root 13 10-28 06:16 log2.txtdrwxr-xr-x 2 root root 4096 10-28 06:21 logs-rw-r--r-- 1 root root 29 10-28 06:05 test1.txtdrwxr-xr-x 2 root root 4096 10-28 07:02 test4 8）把当前目录的一个子目录里的文件移动到另一个子目录里 123456789101112131415161718192021222324# lldrwxr-xr-x 6 root root 4096 10-27 01:58 scfdrwxrwxrwx 4 root root 4096 10-28 07:02 test3drwxr-xr-x 3 root root 4096 10-25 17:56 test5# cd test3# ll-rw-r--r-- 1 root root 25 10-28 07:02 log1.txt-rw-r--r-- 1 root root 13 10-28 06:16 log2.txtdrwxr-xr-x 2 root root 4096 10-28 06:21 logs-rw-r--r-- 1 root root 29 10-28 06:05 test1.txtdrwxr-xr-x 2 root root 4096 10-28 07:02 test4# cd ..# mv test3/*.txt test5# cd test5# ll-rw-r--r-- 1 root root 25 10-28 07:02 log1.txt-rw-r--r-- 1 root root 13 10-28 06:16 log2.txt-rw-r--r-- 1 root root 29 10-28 06:05 test1.txtdrwxr-xr-x 2 root root 4096 10-25 17:56 test5-1# cd ..# cd test3/# lldrwxr-xr-x 2 root root 4096 10-28 06:21 logsdrwxr-xr-x 2 root root 4096 10-28 07:02 test4 9）文件被覆盖前做简单备份，前面加参数-b 123456789101112# ll-rw-r--r-- 1 root root 25 10-28 07:02 log1.txt-rw-r--r-- 1 root root 13 10-28 06:16 log2.txt-rw-r--r-- 1 root root 29 10-28 06:05 test1.txtdrwxr-xr-x 2 root root 4096 10-25 17:56 test5-1# mv log1.txt -b log2.txtmv：是否覆盖“log2.txt”? y# ll-rw-r--r-- 1 root root 25 10-28 07:02 log2.txt-rw-r--r-- 1 root root 13 10-28 06:16 log2.txt~-rw-r--r-- 1 root root 29 10-28 06:05 test1.txtdrwxr-xr-x 2 root root 4096 10-25 17:56 test5-1 说明： -b 不接受参数，mv会去读取环境变量VERSION_CONTROL来作为备份策略。 –backup该选项指定如果目标文件存在时的动作，共有四种备份策略： 1.CONTROL=none或off : 不备份。 2.CONTROL=numbered或t：数字编号的备份 3.CONTROL=existing或nil：如果存在以数字编号的备份，则继续编号备份m+1…n： 执行mv操作前已存在以数字编号的文件log2.txt.~1~，那么再次执行将产生log2.txt~2~，以次类推。如果之前没有以数字编号的文件，则使用下面讲到的简单备份。 4.CONTROL=simple或never：使用简单备份：在被覆盖前进行了简单备份，简单备份只能有一份，再次被覆盖时，简单备份也会被覆盖。 参考链接： http://www.cnblogs.com/peida/archive/2012/10/27/2743022.html http://man.linuxde.net/mv]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库基本操作11-排序分组联合]]></title>
    <url>%2F2018%2F09%2F16%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C11-%E6%8E%92%E5%BA%8F%E5%88%86%E7%BB%84%E8%81%94%E5%90%88%2F</url>
    <content type="text"><![CDATA[1.6.11 order by排序asc：升序【默认】 desc：降序 12345mysql&gt; select * from stu order by ch desc; # 语文成绩降序排列mysql&gt; select * from stu order by math asc; # 数学成绩升序排列mysql&gt; select * from stu order by math; # 默认升序排列 多列排序 12#年龄升序,成绩降序mysql&gt; select *,(ch+math) as &apos;总分&apos; from stu order by stuage asc,(ch+math) desc; 思考如下代码表示什么含义 1234select * from stu order by stuage desc,ch desc; #年龄降序，语文降序select * from stu order by stuage desc,ch asc; #年龄降序，语文升序select * from stu order by stuage,ch desc; #年龄升序、语文降序select * from stu order by stuage,ch; #年龄升序、语文升序 1.6.12 group by 【分组查询】将查询的结果分组，分组查询目的在于统计数据。 123456789101112131415161718192021# 按性别分组，显示每组的平均年龄mysql&gt; select avg(stuage) as &apos;年龄&apos;,stusex from stu group by stusex;+---------+--------+| 年龄 | stusex |+---------+--------+| 22.7500 | 女 || 25.4000 | 男 |+---------+--------+2 rows in set (0.00 sec)# 按地区分组，每个地区的平均年龄mysql&gt; select avg(stuage) as &apos;年龄&apos;,stuaddress from stu group by stuaddress;+---------+------------+| 年龄 | stuaddress |+---------+------------+| 31.0000 | 上海 || 21.3333 | 北京 || 27.0000 | 天津 || 23.0000 | 河北 || 23.0000 | 河南 |+---------+------------+5 rows in set (0.00 sec) 123脚下留心：1、如果是分组查询，查询字段必须是分组字段和聚合函数。2、查询字段是普通字段，只取第一个值 通过group_concat()函数将同一组的值连接起来显示 12345678mysql&gt; select group_concat(stuname),stusex from stu group by stusex;+-------------------------------------+--------+| group_concat(stuname) | stusex |+-------------------------------------+--------+| 李斯文,诸葛丽丽,梅超风,Tabm | 女 || 张秋丽,李文才,欧阳俊雄,争青小子,Tom | 男 |+-------------------------------------+--------+2 rows in set (0.00 sec) 123多学一招：【了解】1、分组后的结果默认会按升序排列显示2、也是可以使用desc实现分组后的降序 多列分组 123456789101112mysql&gt; select stuaddress,stusex,avg(stuage) from stu group by stuaddress,stusex;+------------+--------+-------------+| stuaddress | stusex | avg(stuage) |+------------+--------+-------------+| 上海 | 男 | 31.0000 || 北京 | 女 | 22.0000 || 北京 | 男 | 21.0000 || 天津 | 男 | 27.0000 || 河北 | 女 | 23.0000 || 河南 | 女 | 23.0000 |+------------+--------+-------------+6 rows in set (0.00 sec) 1.6.13 having条件123思考：数据库中的表是一个二维表，返回的结果是一张二维表，既然能在数据库的二维表中进行查询，能否在结果集的二维表上继续进行查询？答：可以，having条件就是在结果集上继续进行筛选。 例题 1234567891011121314151617181920212223mysql&gt; select * from stu where stusex=&apos;男&apos;; # 从数据库中查找+--------+----------+--------+--------+---------+------------+------+------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress | ch | math |+--------+----------+--------+--------+---------+------------+------+------+| s25301 | 张秋丽 | 男 | 18 | 1 | 北京 | 80 | NULL || s25302 | 李文才 | 男 | 31 | 3 | 上海 | 77 | 76 || s25304 | 欧阳俊雄 | 男 | 28 | 4 | 天津 | NULL | 74 || s25318 | 争青小子 | 男 | 26 | 6 | 天津 | 86 | 92 || s25320 | Tom | 男 | 24 | 8 | 北京 | 65 | 67 |+--------+----------+--------+--------+---------+------------+------+------+5 rows in set (0.00 sec)mysql&gt; select * from stu having stusex=&apos;男&apos;; # 从结果集中查找+--------+----------+--------+--------+---------+------------+------+------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress | ch | math |+--------+----------+--------+--------+---------+------------+------+------+| s25301 | 张秋丽 | 男 | 18 | 1 | 北京 | 80 | NULL || s25302 | 李文才 | 男 | 31 | 3 | 上海 | 77 | 76 || s25304 | 欧阳俊雄 | 男 | 28 | 4 | 天津 | NULL | 74 || s25318 | 争青小子 | 男 | 26 | 6 | 天津 | 86 | 92 || s25320 | Tom | 男 | 24 | 8 | 北京 | 65 | 67 |+--------+----------+--------+--------+---------+------------+------+------+5 rows in set (0.00 sec) 思考如下语句是否正确 having和where的区别： where是对原始数据进行筛选，having是对记录集进行筛选。 1.6.14 limit语法：limit 起始位置，显示长度 12345678910111213141516mysql&gt; select * from stu limit 0,2; # 从0的位置开始，取两条数据+--------+---------+--------+--------+---------+------------+------+------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress | ch | math |+--------+---------+--------+--------+---------+------------+------+------+| s25301 | 张秋丽 | 男 | 18 | 1 | 北京 | 80 | NULL || s25302 | 李文才 | 男 | 31 | 3 | 上海 | 77 | 76 |+--------+---------+--------+--------+---------+------------+------+------+2 rows in set (0.00 sec) mysql&gt; select * from stu limit 2,2; # 从2的位置开始，取两条数据+--------+----------+--------+--------+---------+------------+------+------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress | ch | math |+--------+----------+--------+--------+---------+------------+------+------+| s25303 | 李斯文 | 女 | 22 | 2 | 北京 | 55 | 82 || s25304 | 欧阳俊雄 | 男 | 28 | 4 | 天津 | NULL | 74 |+--------+----------+--------+--------+---------+------------+------+------+ 起始位置可以省略，默认是从0开始 12345678mysql&gt; select * from stu limit 2;+--------+---------+--------+--------+---------+------------+------+------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress | ch | math |+--------+---------+--------+--------+---------+------------+------+------+| s25301 | 张秋丽 | 男 | 18 | 1 | 北京 | 80 | NULL || s25302 | 李文才 | 男 | 31 | 3 | 上海 | 77 | 76 |+--------+---------+--------+--------+---------+------------+------+------+2 rows in set (0.00 sec) 例题：找出班级总分前三名 12345678mysql&gt; select *,(ch+math) total from stu order by total desc limit 0,3;+--------+----------+--------+--------+---------+------------+------+------+-------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress | ch | math | total |+--------+----------+--------+--------+---------+------------+------+------+-------+| s25318 | 争青小子 | 男 | 26 | 6 | 天津 | 86 | 92 | 178 || s25321 | Tabm | 女 | 23 | 9 | 河北 | 88 | 77 | 165 || s25302 | 李文才 | 男 | 31 | 3 | 上海 | 77 | 76 | 153 |+--------+----------+--------+--------+---------+------------+------+------+-------+ 多学一招：limit在update和delete语句中也是可以使用的。 1.6.15 查询语句中的选项查询语句中的选项有两个： 1、 all：显示所有数据 【默认】 2、 distinct：去除结果集中重复的数据 1234567891011mysql&gt; select distinct stuaddress from stu;+------------+| stuaddress |+------------+| 上海 || 天津 || 河南 || 河北 || 北京 |+------------+5 rows in set (0.00 sec) 1.7 union（联合）插入测试数据 12345678mysql&gt; create table GO1( -&gt; id int primary key, -&gt; name varchar(20));Query OK, 0 rows affected (0.06 sec)mysql&gt; insert into Go1 values (1,&apos;李白&apos;),(2,&apos;张秋丽&apos;);Query OK, 2 rows affected (0.02 sec)Records: 2 Duplicates: 0 Warnings: 0 1.7.1 union的使用作用：将多个select语句结果集纵向联合起来 1语法：select 语句 union [选项] select 语句 union [选项] select 语句 12345678910111213141516mysql&gt; select stuno,stuname from stu union select id,name from Go1;+--------+----------+| stuno | stuname |+--------+----------+| s25301 | 张秋丽 || s25302 | 李文才 || s25303 | 李斯文 || s25304 | 欧阳俊雄 || s25305 | 诸葛丽丽 || s25318 | 争青小子 || s25319 | 梅超风 || s25320 | Tom || s25321 | Tabm || 1 | 李白 || 2 | 张秋丽 |+--------+----------+ 例题：查询上海的男生和北京的女生 1234567891011121314151617mysql&gt; select stuname,stuaddress,stusex from stu where (stuaddress=&apos;上海&apos; and stusex=&apos;男&apos;) or (stuaddress=&apos;北京&apos; and stusex=&apos;女&apos;);+---------+------------+--------+| stuname | stuaddress | stusex |+---------+------------+--------+| 张秋丽 | 上海 | 男 || 梅超风 | 北京 | 女 |+---------+------------+--------+2 rows in set (0.00 sec)mysql&gt; select stuname,stuaddress,stusex from stu where stuaddress=&apos;上海&apos; and stusex=&apos;男&apos; union select stuname,stuaddress,stusex from stu where stuaddress=&apos;北京&apos; and stusex=&apos;女&apos;;+---------+------------+--------+| stuname | stuaddress | stusex |+---------+------------+--------+| 张秋丽 | 上海 | 男 || 梅超风 | 北京 | 女 |+---------+------------+--------+2 rows in set (0.02 sec) 1.7.2 union的选项union的选项有两个 1、 all：显示所有数据 2、 distinct：去除重复的数据【默认】 123456789101112131415mysql&gt; select name from go1 union select stuname from stu;+----------+| name |+----------+| 李白 || 张秋丽 || 李文才 || 李斯文 || 欧阳俊雄 || 诸葛丽丽 || 争青小子 || 梅超风 || Tom || Tabm |+----------+ 默认是去重复的 12345678910111213141516mysql&gt; select name from go1 union all select stuname from stu; # all不去重复记录+----------+| name |+----------+| 李白 || 张秋丽 || 张秋丽 || 李文才 || 李斯文 || 欧阳俊雄 || 诸葛丽丽 || 争青小子 || 梅超风 || Tom || Tabm |+----------+ 1.7.3 union的注意事项1、 union两边的select语句的字段个数必须一致 2、 union两边的select语句的字段名可以不一致，最终按第一个select语句的字段名。 3、 union两边的select语句中的数据类型可以不一致。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL基本操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-rmdir命令]]></title>
    <url>%2F2018%2F09%2F16%2FLinux%E5%91%BD%E4%BB%A4-rmdir%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[mdir命令。rmdir是常用的命令，该命令的功能是删除空目录，一个目录被删除之前必须是空的。（注意，rm - r dir命令可代替rmdir，但是有很大危险性。）删除某目录时也必须具有对父目录的写权限。 语法rmdir(选项)(参数) 选项12345-p或--parents：删除指定目录后，若该目录的上层目录已变成空目录，则将其一并删除；--ignore-fail-on-non-empty：此选项使rmdir命令忽略由于删除非空目录时导致的错误信息；-v或-verboes：显示命令的详细执行过程；--help：显示命令的帮助信息；--version：显示命令的版本信息。 参数目录列表：要删除的空目录列表。当删除多个空目录时，目录名之间使用空格隔开。 常用范例1）rmdir 不能删除非空目录 1234567891011121314151617181920212223242526272829303132# tree.|-- bin|-- doc| |-- info| `-- product|-- lib|-- logs| |-- info| `-- product`-- service `-- deploy |-- info `-- product12 directories, 0 files# rmdir docrmdir: doc: 目录非空# rmdir doc/info# rmdir doc/product# tree.|-- bin|-- doc|-- lib|-- logs| |-- info| `-- product`-- service `-- deploy |-- info `-- product10 directories, 0 files 2）rmdir -p 当子目录被删除后使它也成为空目录的话，则顺便一并删除 12345678910111213141516171819202122232425262728293031323334353637# tree.|-- bin|-- doc|-- lib|-- logs| `-- product`-- service `-- deploy |-- info `-- product10 directories, 0 files# rmdir -p logsrmdir: logs: 目录非空# tree.|-- bin|-- doc|-- lib|-- logs| `-- product`-- service `-- deploy |-- info `-- product9 directories, 0 files# rmdir -p logs/product# tree.|-- bin|-- doc|-- lib`-- service`-- deploy |-- info `-- product7 directories, 0 files 参考链接： http://www.cnblogs.com/peida/archive/2012/10/27/2742076.html http://man.linuxde.net/rmdir]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库基本操作10-查询语句]]></title>
    <url>%2F2018%2F09%2F16%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C10-%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%2F</url>
    <content type="text"><![CDATA[1.6 查询语句1语法：select [选项] 列名 [from 表名] [where 条件] [group by 分组] [order by 排序][having 条件] [limit 限制] 1.6.1 字段表达式12345678910111213mysql&gt; select &apos;锄禾日当午&apos;;+------------+| 锄禾日当午 |+------------+| 锄禾日当午 |+------------+mysql&gt; select 10*10;+-------+| 10*10 |+-------+| 100 |+-------+ 通过as给字段取别名 123456789101112131415mysql&gt; select &apos;锄禾日当午&apos; as content;+------------+| content |+------------+| 锄禾日当午 |+------------+1 row in set (0.00 sec)mysql&gt; select 10*10 as result;+--------+| result |+--------+| 100 |+--------+1 row in set (0.00 sec) 多学一招：as可以省略 1234567mysql&gt; select 10*10 result;+--------+| result |+--------+| 100 |+--------+1 row in set (0.00 sec) 1.6.2 from子句from：来自，from后面跟的是数据源。数据源可以有多个。返回笛卡尔积。 插入测试表 12345678910111213141516171819mysql&gt; create table t1( -&gt; id int, -&gt; name varchar(10) -&gt; );Query OK, 0 rows affected (0.05 sec)mysql&gt; create table t2( -&gt; field1 varchar(10), -&gt; field2 varchar(10) -&gt; );Query OK, 0 rows affected (0.00 sec)mysql&gt; insert into t1 values (1,&apos;tom&apos;),(2,&apos;berry&apos;);Query OK, 2 rows affected (0.00 sec)Records: 2 Duplicates: 0 Warnings: 0mysql&gt; insert into t2 values (&apos;333&apos;,&apos;333&apos;),(&apos;444&apos;,&apos;444&apos;);Query OK, 2 rows affected (0.02 sec)Records: 2 Duplicates: 0 Warnings: 0 测试多个数据源 12345678910mysql&gt; select * from t1,t2; # 返回笛卡尔积+------+-------+--------+--------+| id | name | field1 | field2 |+------+-------+--------+--------+| 1 | tom | 333 | 333 || 2 | berry | 333 | 333 || 1 | tom | 444 | 444 || 2 | berry | 444 | 444 |+------+-------+--------+--------+4 rows in set (0.00 sec) 1.6.3 dual表dual表是一个伪表。在有些特定情况下，没有具体的表的参与，但是为了保证select语句的完整又必须要一个表名，这时候就使用伪表。 123456mysql&gt; select 10*10 as result from dual; #dual表是用来保证select语句的完整性。+--------+| result |+--------+| 100 |+--------+ 1.6.4 where子句where后面跟的是条件，在数据源中进行筛选。返回条件为真记录 MySQL支持的运算符 &gt; 大于 &lt;小于 &gt;= &lt;= = != and 与 or 或 not 非 12mysql&gt; select * from stu where stusex=&apos;男&apos;; # 查找性别是男的记录mysql&gt; select * from stu where stuage&gt;=20; # 查找年龄不低于20的记录 思考：如下代码输出什么 12select * from stu where 1 # 返回所有数据库select * from stu where 0 #返回空记录 思考：如何查找北京和上海的学生 123456789mysql&gt; select * from stu where stuaddress=&apos;上海&apos; or stuaddress=&apos;北京&apos;;+--------+---------+--------+--------+---------+------------+------+------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress | ch | math |+--------+---------+--------+--------+---------+------------+------+------+| s25301 | 张秋丽 | 男 | 18 | 1 | 北京 | 80 | NULL || s25302 | 李文才 | 男 | 31 | 3 | 上海 | 77 | 76 || s25303 | 李斯文 | 女 | 22 | 2 | 北京 | 55 | 82 || s25320 | Tom | 男 | 24 | 8 | 北京 | 65 | 67 |+--------+---------+--------+--------+---------+------------+------+------+ 1.6.5 in | not in 上面的查询上海和北京的学生的SQL可以通过in语句来实现 1mysql&gt; select * from stu where stuaddress in (&apos;北京&apos;,&apos;上海&apos;); 练习： 1、查找学号是s25301,s25302,s25303的学生 1mysql&gt; select * from stu where stuno in (&apos;s25301&apos;,&apos;s25302&apos;,&apos;s25303&apos;); 2、查找年龄是18,19,20的学生 1mysql&gt; select * from stu where stuage in(18,19,20); 3、查找不是北京和上海的学生 1mysql&gt; select * from stu where stuaddress not in (&apos;北京&apos;,&apos;上海&apos;); 1.6.6 between…and|not between…and查找某个范围的记录 1、查找年龄在18~20之间的学生 123mysql&gt; select * from stu where stuage&gt;=18 and stuage&lt;=20; # 方法一mysql&gt; select * from stu where stuage between 18 and 20; # 方法二 2、查找年龄不在18~20之间的学生 12345mysql&gt; select * from stu where stuage&lt;18 or stuage&gt;20; #方法一mysql&gt; select * from stu where not (stuage&gt;=18 and stuage&lt;=20);mysql&gt; select * from stu where stuage not between 18 and 20; 1.6.7 is null | is not null 脚下留心：查询一个为空的字段不能用等于，必须用is null 查找缺考的学生 1234567mysql&gt; select * from stu where ch is null or math is null; # 查找缺考的人+--------+----------+--------+--------+---------+------------+------+------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress | ch | math |+--------+----------+--------+--------+---------+------------+------+------+| s25301 | 张秋丽 | 男 | 18 | 1 | 北京 | 80 | NULL || s25304 | 欧阳俊雄 | 男 | 28 | 4 | 天津 | NULL | 74 |+--------+----------+--------+--------+---------+------------+------+------+ 查找参加考试的学生 1mysql&gt; select * from stu where ch is not null and math is not null; 1.6.8 聚合函数 sum() 求和 avg() 求平均值 max() 求最大值 min() 求最小值 count() 求记录数 12345678910#求语文总分、语文平均分、语文最高分、语文最低分、总人数mysql&gt; select sum(ch) &apos;语文总分&apos;,avg(ch) &apos;语文平均分&apos;, max(ch) &apos;语文最高分&apos;,min(ch) &apos;语文最低分&apos;,count(*) &apos;总人数&apos; from stu;+----------+------------+------------+------------+--------+| 语文总分 | 语文平均分 | 语文最高分 | 语文最低分 | 总人数 |+----------+------------+------------+------------+--------+| 597 | 74.6250 | 88 | 55 | 9 |+----------+------------+------------+------------+--------+1 row in set (0.00 sec) 1.6.9 通配符 _ [下划线] 表示任意一个字符 % 表示任意字符 练习 1、满足“T_m”的有（A、C） A：Tom B：Toom C：Tam D：Tm E：Tmo 2、满足“T_m_”的有（B、C ） A:Tmom B:Tmmm C:T1m2 D:Tmm E:Tm 3、满足“张%”的是（A、B、C、D） A:张三 B：张三丰 C：张牙舞爪 D：张 E：小张 4、满足“%诺基亚%”的是（A、B、C、D） A：诺基亚2100 B：2100诺基亚 C：把我的诺基亚拿过来 D：诺基亚 1.6.10 模糊查询（like）12345678910111213141516# 查找姓张的同学mysql&gt; select * from stu where stuname like &apos;张%&apos;;+--------+---------+--------+--------+---------+------------+------+------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress | ch | math |+--------+---------+--------+--------+---------+------------+------+------+| s25301 | 张秋丽 | 男 | 18 | 1 | 北京 | 80 | NULL |+--------+---------+--------+--------+---------+------------+------+------+1 row in set (0.00 sec)#例题mysql&gt; select * from stu where stuname like &apos;T_m&apos;;+--------+---------+--------+--------+---------+------------+------+------+| stuNo | stuName | stuSex | stuAge | stuSeat | stuAddress | ch | math |+--------+---------+--------+--------+---------+------------+------+------+| s25320 | Tom | 男 | 24 | 8 | 北京 | 65 | 67 |+--------+---------+--------+--------+---------+------------+------+------+1 row in set (0.00 sec)]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL基本操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库设计思维与模式]]></title>
    <url>%2F2018%2F09%2F15%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1%E6%80%9D%E7%BB%B4%E4%B8%8E%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[1.1 今日目标 理解实体之间的关系 理解绘制E-R图 理解三范式 理解范式和性能的关系 1.2 数据库基本概念1、关系：两个表的公共字段 2、行：也称记录，也称实体 3、列：也称字段，也称属性 123就表结构而言，表分为行和列；就表数据而言，分为记录和字段；就面向对象而言，一个记录就是一个实体，一个字段就是一个属性。 4、数据冗余：相同的数据存储在不同的地方 1234脚下留心：1、冗余只能减少，不能杜绝。2、减少冗余的方法是分表3、为减少数据查找的麻烦，允许数据有一定的冗余 5、数据完整性：正确性+准确性=数据完整性 12正确性：数据类型正确准确性：数据范围要准确 思考：学生的年龄是整型，输入1000岁，正确性和准确性如何？ 答：正确的，但不准确 思考：年龄是整形的，收入了字符串，正确性和准确性如何？ 答：不正确 1.3 实体和实体之间的关系 1、一对一 2、一对多 （多对一） 3、多对多 1.3.1 一对多 1：N1、主表中的一条记录对应从表中的多条记录。 2、一对多和多对一是一样的 如何实现一对多？ 答：主键和非主键建关系 问题：说出几个一对多的关系？ 答：班级表和学生表、 班主表和学生表 1.3.2 一对一（1:1）1、主表中的一条记录对应从表中的一条记录 如何实现一对一？ 主键和主键建关系就能实现一对一。 123思考：一对一两个表完全可以用一个表实现，为什么还要分成两个表？答：在字段数量很多情况下，数据量也就很大，每次查询都需要检索大量数据，这样效率低下。我们可以将所有字段分成两个部分，“常用字段”和“不常用字段”，这样对大部分查询者来说效率提高了。【表的垂直分割】 1.3.3 多对多（N：M）主表中的一条记录对应从表中的多条记录，从表中的一条记录对应主表中的多条记录 班级和老师的关系 如何实现多对多？ 答：建立第三张表来保存关系。 问题：说出几个多对多的关系？ 1、科目表和学生表的关系 2、商品表和订单表 3、游戏目录表和玩家表 1.4 数据库设计的步骤 1.4.1 数据库设计具体步骤1、 收集信息：与该系统有关人员进行交流、坐谈，充分理解数据库需要完成的任务 2、 标识对象（实体－Entity）标识数据库要管理的关键对象或实体 3、 标识每个实体的属性（Attribute） 4、 标识对象之间的关系（Relationship） 5、 将模型转换成数据库 6、 规范化 1.4.2 绘制E-R图 E-R（Entity－Relationship）实体关系图 E-R图的语法 绘制E-R图 1.4.3 将E-R图转成表1、 实体转成表，属性转成字段 2、 如果没有合适的字段做主键，给表添加一个自动增长列做主键。 1.4.4 例题1、项目需求 12345BBS论坛的基本功能：用户注册和登录，后台数据库需要存放用户的注册信息和在线状态信息；用户发贴，后台数据库需要存放贴子相关信息，如贴子内容、标题等；用户可以对发帖进行回复；论坛版块管理：后台数据库需要存放各个版块信息，如版主、版块名称、贴子数等； 2、标识对象 ​ 参与的对象有：用户、发的帖子、跟帖、板块 3、标识对象的属性 4、建立关系，绘制E-R图 5、将E-R图转出表结构 1.5 数据规范化Codd博士定义了6个范式来规范化数据库，范式由小到大来约束，范式越高冗余越小，但表的个数也越多。实验证明，三范式是性价比最高的。 1.5.1 第一范式：确保每列原子性第一范式确保每个字段不可再分 思考：如下表设计是否合理？ 不合理。不满足第一范式，上课时间可以再分 思考：地址包含省、市、县、地区是否需要拆分？ 答：如果仅仅起地址的作用，不需要统计，可以不拆分；如果有按地区统计的功能需要拆分。 在实际项目中，建议拆分。 1.5.2 第二范式：非键字段必须依赖于键字段一个表只能描述一件事 思考：如下表设计是否合理？ 1.5.3 第三范式：消除传递依赖在所有的非键字段中，不能有传递依赖 下列设计是否满足第三范式？ 不满足，因为语文和数学确定了，总分就确定了。 123多学一招：上面的设计不满足第三范式，但是高考分数表就是这样设计的，为什么？答：高考分数峰值访问量非常大，这时候就是性能更重要。当性能和规范化冲突的时候，我们首选性能。这就是“反三范式”。 1.5.4 数据库设计的例题1、需求 123公司承担多个工程项目，每一项工程有：工程号、工程名称、施工人员等公司有多名职工，每一名职工有：职工号、姓名、性别、职务（工程师、技术员）等公司按照工时和小时工资率支付工资，小时工资率由职工的职务决定（例如，技术员的小时工资率与工程师不同） 2、工资表 3、将工资表转成数据库表 4、这个表存在的问题 ​ A：新人入职需要虚拟一个项目 ​ B：职务更改，小时工资率可能会忘记更改，造成数据不完整 ​ C：有人离职，删除记录后，工程也没有了 5、规范化表 ​ 第一步：这个表满足第一范式 ​ 第二步：这个表不是描述了一件事情 第三步：是否满足第三范式 更改如下：]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL基本操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-rm]]></title>
    <url>%2F2018%2F09%2F14%2FLinux%E5%91%BD%E4%BB%A4-rm%2F</url>
    <content type="text"><![CDATA[rm命令可以删除一个目录中的一个或多个文件或目录，也可以将某个目录及其下属的所有文件及其子目录均删除掉。对于链接文件，只是删除整个链接文件，而原有文件保持不变。 rm是一个危险的命令，使用的时候要特别当心，尤其对于新手，否则整个系统就会毁在这个命令（比如在/（根目录）下执行rm * -rf）。所以，我们在执行rm之前最好先确认一下在哪个目录，到底要删除什么东西，操作时保持高度清醒的头脑。 语法rm (选项)(参数) 选项123456-d：直接把欲删除的目录的硬连接数据删除成0，删除该目录；-f：强制删除文件或目录；-i：删除已有文件或目录之前先询问用户；-r或-R：递归处理，将指定目录下的所有文件与子目录一并处理；--preserve-root：不对根目录进行递归操作；-v：显示指令的详细执行过程。 参数文件：指定被删除的文件列表，如果参数中含有目录，则必须加上-r或者-R选项。 常用范例1）删除文件file，系统会先询问是否删除。 12# rm log.log rm：是否删除 一般文件 “log.log”? y 输入rm log.log命令后，系统会询问是否删除，输入y后就会删除文件，不想删除则数据n。 2）强行删除file，系统不再提示 1rm -f log1.log 3）删除任何.log文件；删除前逐一询问确认 1rm -i *.log 4）将 test1子目录及子目录中所有档案删除 1rm -r test1]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-mkdir]]></title>
    <url>%2F2018%2F09%2F14%2FLinux%E5%91%BD%E4%BB%A4-mkdir%2F</url>
    <content type="text"><![CDATA[mkdir命令用来创建目录。该命令创建由dirname命名的目录。如果在目录名的前面没有加任何路径名，则在当前目录下创建由dirname指定的目录；如果给出了一个已经存在的路径，将会在该目录下创建一个指定的目录。在创建目录时，应保证新建的目录与它所在目录下的文件没有重名。 语法cd (选项) (参数) 选项12345-Z：设置安全上下文，当使用SELinux时有效；-m&lt;目标属性&gt;或--mode&lt;目标属性&gt;建立目录的同时设置目录的权限；-p或--parents 若所要建立目录的上层目录目前尚未建立，则会一并建立上层目录；-v, --verbose 每次创建新目录都显示信息--version 显示版本信息。 常用范例1）创建一个空目录 1mkdir test1 2）递归创建多个目录 1mkdir -p test2/test22 3）创建权限为777的目录 1mkdir -m 777 test3 4）一个命令创建项目的目录结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#mkdir -vp scf/&#123;lib/,bin/,doc/&#123;info,product&#125;,logs/&#123;info,product&#125;,service/deploy/&#123;info,product&#125;&#125;mkdir: 已创建目录 “scf”mkdir: 已创建目录 “scf/lib”mkdir: 已创建目录 “scf/bin”mkdir: 已创建目录 “scf/doc”mkdir: 已创建目录 “scf/doc/info”mkdir: 已创建目录 “scf/doc/product”mkdir: 已创建目录 “scf/logs”mkdir: 已创建目录 “scf/logs/info”mkdir: 已创建目录 “scf/logs/product”mkdir: 已创建目录 “scf/service”mkdir: 已创建目录 “scf/service/deploy”mkdir: 已创建目录 “scf/service/deploy/info”mkdir: 已创建目录 “scf/service/deploy/product”# tree scf/scf/|-- bin|-- doc| |-- info| `-- product|-- lib|-- logs| |-- info| `-- product`-- service `-- deploy |-- info `-- product12 directories, 0 files]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库基本操作9]]></title>
    <url>%2F2018%2F09%2F13%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C9%2F</url>
    <content type="text"><![CDATA[1.15 数据完整性介绍 1.15.1 保证实体完整性1、 主键约束 2、 唯一约束 3、 自动增长列 1.15.2 保证域完整性1、 数据类型约束 2、 非空约束 3、 默认值约束 1.15.3 保证引用完整性1、外键约束：从表中的公共字段是主表的外键 1.16 引用完整性1.16.1 主表和从表两个表建立关系（两个表只要有公共字段就有关系），一个表称为主表，一个表称为从表。 外键约束可以实现： 1、 主表中没有的从表中不允许插入 2、 从表中有的主表中不允许删除 3、 不能更改主表中的值而导致从表中的记录孤立存在。 4、 先删除从表，再删除主表 1.16.2 外键（foreign key）1、 外键：从表中的公共字段，公共字段的名字可以不一样，但是数据类型必须一样。 2、 外键约束用来保证引用完整性 1.16.3 添加外键方法一：创建表的时候添加外键 12345678910create table stuinfo( stuno char(4) primary key, name varchar(10) not null);create table stumarks( stuid char(4) primary key, score tinyint unsigned, foreign key (stuid) references stuinfo(stuno)); 方法二：修改表的时候添加外键 1234567891011121314151617mysql&gt; create table stuinfo( -&gt; stuno char(4) primary key, -&gt; name varchar(10) not null -&gt; );Query OK, 0 rows affected (0.00 sec)mysql&gt; create table stumarks( -&gt; stuid char(4) primary key, -&gt; score tinyint unsigned -&gt; );Query OK, 0 rows affected (0.06 sec)语法： alter table 从表 add foreign key (从表的公共字段) references 主表(公共字段)mysql&gt; alter table stumarks add foreign key (stuid) references stuinfo(stuno);Query OK, 0 rows affected (0.06 sec)Records: 0 Duplicates: 0 Warnings: 0 脚下留心：要创建外键必须是innodb引擎，myisam不支持外键约束 1.16.4 查看外键 1.16.5 删除外键通过外键的名字删除外键 1语法：alter table 表名 drop foreign key 外键名 例题 123mysql&gt; alter table stumarks drop foreign key stumarks_ibfk_1;Query OK, 0 rows affected (0.02 sec)Records: 0 Duplicates: 0 Warnings: 0 。 1.17 外键操作1、 严格操作（前面讲的是严格操作） 2、 置空操作（set null）：如果主表记录删除或更新，从表置空 3、 级联操作（cascade）：如果主表记录删除或更新，从表级联 一般来说：主表删除的时候，从表置空操作，主表更新的时候，从表级联操作。 1语法：foreign key(外键) references 主表(关键字段)[主表删除是的动作][主表更新时候的动作] 例题 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950mysql&gt; create table stuinfo( -&gt; stuno char(4) primary key, -&gt; name varchar(10) not null -&gt; );Query OK, 0 rows affected (0.02 sec)mysql&gt; create table stumarks( -&gt; stuid int auto_increment primary key, -&gt; stuno char(4) , -&gt; score tinyint unsigned, -&gt; foreign key (stuno) references stuinfo(stuno) on delete set null on update cascade -&gt; );Query OK, 0 rows affected (0.00 sec)mysql&gt; insert into stuinfo values (&apos;s101&apos;,&apos;tom&apos;);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into stumarks values (null,&apos;s101&apos;,88);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from stuinfo;+-------+------+| stuno | name |+-------+------+| s101 | tom |+-------+------+1 row in set (0.00 sec)mysql&gt; update stuinfo set stuno=&apos;s102&apos; where stuno=&apos;s101&apos;; # 更新时级联Query OK, 1 row affected (0.00 sec)Rows matched: 1 Changed: 1 Warnings: 0mysql&gt; select * from stumarks;+-------+-------+-------+| stuid | stuno | score |+-------+-------+-------+| 1 | s102 | 88 |+-------+-------+-------+1 row in set (0.00 sec)mysql&gt; delete from stuinfo where stuno=&apos;s102&apos;; # 删除时置空Query OK, 1 row affected (0.02 sec)mysql&gt; select * from stumarks;+-------+-------+-------+| stuid | stuno | score |+-------+-------+-------+| 1 | NULL | 88 |+-------+-------+-------+1 row in set (0.00 sec) 1.18客户端介绍第一：命令行 第二：MySQL-Front和Navicat MySQL-Front]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL基本操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库基本操作8]]></title>
    <url>%2F2018%2F09%2F13%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C8%2F</url>
    <content type="text"><![CDATA[1.11.2 创建组合键 1.11.3 查看主键 1.11.3 删除主键 1.11.4 选择主键的原则1、 最少性：尽量选择一个字段做主键 2、 稳定性：尽量选择更新少的列做主键 3、 尽量选择数字型的列做主键 1.11.5 主键思考题1、在主键列输入的数值，允许为空吗? 不可以 2、 一个表可以有多个主键吗? 不可以 3、 在一个学校数据库中，如果一个学校内允许重名的学员，但是一个班级内不允许学员重名，可以组合班级和姓名两个字段一起来作为主键吗？ 可以 4、 标识列（自动增长列）允许为字符数据类型吗？ 不可以 5、 表中没有合适的列作为主键怎么办？ 添加自动增加列 6、 如果标识列A的初始值为1，增长量为1，则输入三行数据以后，再删除两行，下次再输入数据行的时候，标识值从多少开始？ 从4开始 1.12 列属性——唯一键特点： 1、不能重复，可以为空 2、一个表可以有多个唯一键 作用： 1、 保证数据不能重复。保证数据完整性 2、 加快数据访问 1.12.1 添加唯一键方法一：创建表的时候添加唯一键 1234567891011121314mysql&gt; create table t22( -&gt; id int primary key, -&gt; name varchar(20) unique, #通过unique添加唯一键 -&gt; addr varchar(100) unique -&gt; );Query OK, 0 rows affected (0.00 sec)mysql&gt; insert into t22 values (1,&apos;tom&apos;,&apos;上海&apos;);Query OK, 1 row affected (0.05 sec)mysql&gt; insert into t22 values (2,&apos;tom&apos;,&apos;北京&apos;); # name重复了，报错ERROR 1062 (23000): Duplicate entry &apos;tom&apos; for key &apos;name&apos;mysql&gt; insert into t22 values (2,&apos;berry&apos;,&apos;上海&apos;); # addr重复了 ERROR 1062 (23000): Duplicate entry &apos;上海&apos; for key &apos;addr&apos; 还有一种方法 123456789mysql&gt; create table t26( -&gt; id int, -&gt; name varchar(20), -&gt; addr varchar(20), -&gt; primary key(id), -&gt; unique (name), # 添加唯一键 -&gt; unique (addr) -&gt; );Query OK, 0 rows affected (0.06 sec) 方法二：修改表的时候添加唯一键 123456789mysql&gt; create table t23( -&gt; id int primary key, -&gt; name varchar(20) -&gt; );Query OK, 0 rows affected (0.02 sec)mysql&gt; alter table t23 add unique (name); # 添加一个唯一键Query OK, 0 rows affected (0.02 sec)Records: 0 Duplicates: 0 Warnings: 0 一次添加多个唯一键 12345678910mysql&gt; create table t24( -&gt; id int primary key, -&gt; name varchar(20), -&gt; addr varchar(20) -&gt; );Query OK, 0 rows affected (0.06 sec)mysql&gt; alter table t24 add unique(name),add unique(addr); Query OK, 0 rows affected (0.09 sec)Records: 0 Duplicates: 0 Warnings: 0 添加组合唯一键 12345678910mysql&gt; create table t25( -&gt; id int primary key, -&gt; name varchar(20), -&gt; addr varchar(20) -&gt; );Query OK, 0 rows affected (0.09 sec)mysql&gt; alter table t25 add unique(name,addr);Query OK, 0 rows affected (0.01 sec)Records: 0 Duplicates: 0 Warnings: 0 1.12.2查看唯一键123456789101112131415161718192021222324mysql&gt; show create table t26\G*************************** 1. row *************************** Table: t26Create Table: CREATE TABLE `t26` ( `id` int(11) NOT NULL DEFAULT &apos;0&apos;, `name` varchar(20) DEFAULT NULL, `addr` varchar(20) DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `name` (`name`), # 唯一键 UNIQUE KEY `addr` (`addr`) # 唯一键) ENGINE=InnoDB DEFAULT CHARSET=utf81 row in set (0.00 sec)mysql&gt; show create table t25\G*************************** 1. row *************************** Table: t25Create Table: CREATE TABLE `t25` ( `id` int(11) NOT NULL, `name` varchar(20) DEFAULT NULL, `addr` varchar(20) DEFAULT NULL, PRIMARY KEY (`id`), UNIQUE KEY `name` (`name`,`addr`) # 组合唯一键) ENGINE=InnoDB DEFAULT CHARSET=utf81 row in set (0.00 sec) 添加唯一键，给唯一键取名 1234567891011121314151617mysql&gt; create table t27( -&gt; name varchar(20) -&gt; );Query OK, 0 rows affected (0.03 sec)mysql&gt; alter table t27 add unique UQ_name(name);Query OK, 0 rows affected (0.00 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; show create table t27\G*************************** 1. row *************************** Table: t27Create Table: CREATE TABLE `t27` ( `name` varchar(20) DEFAULT NULL, UNIQUE KEY `UQ_name` (`name`) # 唯一键的名字是UQ_name) ENGINE=InnoDB DEFAULT CHARSET=utf81 row in set (0.00 sec) 1.12.3 删除唯一键通过唯一键的名字来删除唯一键 1语法：alter table 表名 drop index 唯一键名称 问题：主键和唯一键的区别？ 1、主键不能重复，不能为空，唯一键不能重复，可以为空 2、主键只有一个，唯一键可以有多个。 1.13列属性——备注（comment）为了程序员之间的相互交流 1.14 SQL注释单行注释：—或# 多行注释：/ /]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL基本操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库基本操作7]]></title>
    <url>%2F2018%2F09%2F13%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C7%2F</url>
    <content type="text"><![CDATA[1.6 数据类型——booleanMySQL不支持boolean类型，true和false在数据库中对应1和0。 1234567891011121314151617mysql&gt; create table t15( -&gt; field boolean -&gt; );Query OK, 0 rows affected (0.00 sec)mysql&gt; insert into t15 values (true),(false); # true和false在数据库中对应1和0Query OK, 2 rows affected (0.00 sec)Records: 2 Duplicates: 0 Warnings: 0mysql&gt; select * from t15;+-------+| field |+-------+| 1 || 0 |+-------+2 rows in set (0.00 sec) 1.7 关于数据类型的思考题 手机号码一般使用什么数据类型存储? char 电话号码使用什么数据类型 varchar 性别一般使用什么数据类型存储? char enum 学生年龄信息一般使用什么数据类型存储? tinyint 照片信息一般使用什么数据类型存储? binary 薪水一般使用什么数据类型存储? decimal 多学一招：一个字段到底选数字还是字符，取决于有没有计算的可能，如果没有计算的可能即使是数字也要用字符类型，比如手机号、QQ号，… 1.8 列属性——是否为空(null | not null)null：可以为空 not null：不可以为空 思考题 学员姓名允许为空吗? 非空 家庭地址允许为空吗? 非空 电子邮件信息允许为空吗? 可以为空 考试成绩允许为空吗? 可以为空 1.9 列属性——默认值（default）1、如果一个字段没有插入值，可以默认插入一个指定的值。 2、default关键字用来插入默认值 123456789101112131415161718mysql&gt; create table t16( -&gt; id int unsigned, -&gt; addr varchar(20) not null default &apos;地址不详&apos; -&gt; );Query OK, 0 rows affected (0.06 sec)mysql&gt; insert into t16 values (1,&apos;北京&apos;),(2,default);Query OK, 2 rows affected (0.00 sec)Records: 2 Duplicates: 0 Warnings: 0mysql&gt; select * from t16;+------+----------+| id | addr |+------+----------+| 1 | 北京 || 2 | 地址不详 |+------+----------+2 rows in set (0.00 sec) 1.10 列属性——自动增长（auto_increment）1、字段的值从1开始，每次递增1，特点就在字段中的数据不可能重复，适合为记录生成唯一的id 2、自动增长都是无符号整数。 3、在MySQL中，auto_increment必须是主键。但是主键不一定是自动增长的。 4、如果要给自动增长列插入数据，使用null关键字。 5、自动增长列上的数据被删除，默认情况下此记录的编号不再使用。 1.11 列属性——主键（primary key）主键：唯一标识表中记录的一个或一组列 主键的特点：不能重复，不能为空 一个表只能有一个主键，主键可以有多个字段组成。 主键的作用： 1、 保证数据完整性 2、 加快查询速度 1.11.1 添加主键方法一：创建表的时候添加主键 1234567891011121314151617181920212223242526mysql&gt; create table t17( -&gt; id varchar(5) primary key, # 创建主键 -&gt; name varchar(10) not null -&gt; );Query OK, 0 rows affected (0.00 sec)mysql&gt; insert into t17 values (&apos;s2531&apos;,&apos;tom&apos;),(&apos;s2532&apos;,&apos;berry&apos;);Query OK, 2 rows affected (0.00 sec)Records: 2 Duplicates: 0 Warnings: 0mysql&gt; select * from t17;+-------+-------+| id | name |+-------+-------+| s2531 | tom || s2532 | berry |+-------+-------+2 rows in set (0.00 sec)# 如果插入主键相同数据会报错mysql&gt; insert into t17 values (&apos;s2531&apos;,&apos;tom&apos;);ERROR 1062 (23000): Duplicate entry &apos;s2531&apos; for key &apos;PRIMARY&apos;# 主键不能插入null值mysql&gt; insert into t17 values (null,&apos;tom&apos;);ERROR 1048 (23000): Column &apos;id&apos; cannot be null 方法二：创建表的时候添加主键 123456789101112131415mysql&gt; create table t18( -&gt; id int, -&gt; name varchar(10), -&gt; primary key(id) -&gt; );Query OK, 0 rows affected (0.00 sec)mysql&gt; desc t18;+-------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+-------------+------+-----+---------+-------+| id | int(11) | NO | PRI | 0 | || name | varchar(10) | YES | | NULL | |+-------+-------------+------+-----+---------+-------+2 rows in set (0.00 sec) 方法三：更改表的时候添加主键 123456789101112131415161718mysql&gt; create table t20( -&gt; id int, -&gt; name varchar(10) -&gt; );Query OK, 0 rows affected (0.00 sec)mysql&gt; alter table t20 add primary key (id); # 更改表添加主键Query OK, 0 rows affected (0.08 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc t20;+-------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+-------------+------+-----+---------+-------+| id | int(11) | NO | PRI | 0 | || name | varchar(10) | YES | | NULL | |+-------+-------------+------+-----+---------+-------+2 rows in set (0.00 sec)]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL基本操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库基本操作6]]></title>
    <url>%2F2018%2F09%2F12%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C6%2F</url>
    <content type="text"><![CDATA[1.3 数据类型——枚举（enum）1、从集合中选择一个数据（单选） 123456789101112131415161718192021mysql&gt; create table t8( -&gt; name varchar(20), -&gt; sex enum(&apos;男&apos;,&apos;女&apos;,&apos;保密&apos;) # 枚举 -&gt; )charset=utf8;Query OK, 0 rows affected (0.06 sec)mysql&gt; insert into t8 values (&apos;tom&apos;,&apos;男&apos;);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t8 values (&apos;berry&apos;,&apos;女&apos;);Query OK, 1 row affected (0.05 sec)mysql&gt; insert into t8 values (&apos;rose&apos;,&apos;未知&apos;); # 报错，只能插入枚举值ERROR 1265 (01000): Data truncated for column &apos;sex&apos; at row 1mysql&gt; select * from t8;+-------+------+| name | sex |+-------+------+| tom | 男 || berry | 女 |+-------+------+ 2、MySQL的枚举类型是通过整数来管理的，第一个值是1，第二个值是2，以此类推。 1234567mysql&gt; select sex+0 from t8;+-------+| sex+0 |+-------+| 1 || 2 |+-------+ 1234567mysql&gt; select sex+0 from t8;+-------+| sex+0 |+-------+| 1 || 2 |+-------+ 3、既然枚举在数据库内部存储的是整数，那么可以直接插入数字 123456789101112mysql&gt; insert into t8 values (&apos;rose&apos;,3); # 可以直接插入数字Query OK, 1 row affected (0.00 sec)mysql&gt; select * from t8;+-------+------+| name | sex |+-------+------+| tom | 男 || berry | 女 || rose | 保密 |+-------+------+3 rows in set (0.00 sec) 枚举的优点： 1、 运行速度快（数字比字符串运算速度快） 2、 限制数据，保证数据完整性 3、 节省空间 123思考：已知枚举占用2个字节，请问最多有多少个枚举值？答：2个字节=16位，可以保存数字（0-65535），枚举是从1开始，所以枚举最多可以有65535个枚举值。 1.4 数据类型——集合（set）从集合中选择一些数据（多选） 12345678910111213141516mysql&gt; create table t9( -&gt; hobby set(&apos;爬山&apos;,&apos;读书&apos;,&apos;游泳&apos;,&apos;敲代码&apos;) -&gt; );Query OK, 0 rows affected (0.08 sec)mysql&gt; insert into t9 values (&apos;爬山&apos;);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t9 values (&apos;爬山,游泳&apos;);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t9 values (&apos;游泳,爬山&apos;); # 插入顺序不一样，但是显示的顺序是一样的Query OK, 1 row affected (0.02 sec)mysql&gt; insert into t9 values (&apos;爬山,游泳,开车&apos;); # 报错，插入集合中没有的选项会报错ERROR 1265 (01000): Data truncated for column &apos;hobby&apos; at row 1 每个集合的元素都分配一个固定的数字，分配的方式从左往右按2的0、1、2、…次方 123思考：已知集合占用8个字节，最多可以表示几个选项？答：8个字节=64位，一个位表示1个选项，最多可以表示64个选项。 1.5 数据类型——日期类型 数据类型 描述 datetime 日期时间，占用8个字节 date 日期 占用3个字节 time 时间 占用3个字节 timestamp 时间戳，占用4个字节 year 年份 占用1个字节 1、datetime 格式：年-月-日 小时:分钟:秒 12345678910111213141516171819202122mysql&gt; create table t10( -&gt; field datetime -&gt; );Query OK, 0 rows affected (0.02 sec)mysql&gt; insert into t10 values (&apos;2025-10-12 10:12:36&apos;);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t10 values (&apos;100-10-12 10:12:36&apos;);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t10 values (&apos;10000-10-12 10:12:36&apos;); #datetime保存范围是：1~9999年ERROR 1292 (22007): Incorrect datetime value: &apos;10000-10-12 10:12:36&apos; for column &apos;field&apos; at row 1mysql&gt; select * from t10;+---------------------+| field |+---------------------+| 2025-10-12 10:12:36 || 0100-10-12 10:12:36 |+---------------------+2 rows in set (0.00 sec) 2、date 日期格式 1234567891011121314mysql&gt; create table t11( -&gt; field date -&gt; );Query OK, 0 rows affected (0.00 sec)mysql&gt; insert into t11 values (&apos;2025-10-12&apos;);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from t11;+------------+| field |+------------+| 2025-10-12 |+------------+ 3、timestamp：时间戳 timestamp类型和 datetime类型在表现上是一样的。他们的区别：datetime是从1到9999，而timestamp从1970年~2038年，2038年01月19日11:14:07秒以后就超出timestamp范围了。 1234567891011121314151617181920212223mysql&gt; create table t12( -&gt; field timestamp -&gt; );Query OK, 0 rows affected (0.00 sec)mysql&gt; insert into t12 values (&apos;1975-5-5 12:12:12&apos;);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t12 values (&apos;1969-5-5 12:12:12&apos;); # 超出范围ERROR 1292 (22007): Incorrect datetime value: &apos;1969-5-5 12:12:12&apos; for column &apos;field&apos; at row 1mysql&gt; insert into t12 values (&apos;2038-1-19 11:14:07&apos;);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t12 values (&apos;2038-1-19 11:14:08&apos;); # 超出范围ERROR 1292 (22007): Incorrect datetime value: &apos;2038-1-19 11:14:08&apos; for column &apos;field&apos; at row 1mysql&gt; select * from t12;+---------------------+| field |+---------------------+| 1975-05-05 12:12:12 || 2038-01-19 11:14:07 |+---------------------+ 4、year 因为只占用1个字节，最多只能表示255个年份，范围是1901-2155之间的年份 123456789101112131415mysql&gt; create table t13( -&gt; field year -&gt; );Query OK, 0 rows affected (0.06 sec)mysql&gt; insert into t13 values (2025);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t13 values (1900); # 超出范围ERROR 1264 (22003): Out of range value for column &apos;field&apos; at row 1mysql&gt; insert into t13 values (2155);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t13 values (2156); # 超出范围ERROR 1264 (22003): Out of range value for column &apos;field&apos; at row 1 5、time 表示时间或时间间隔，范围是-838:59:59~838:59:59 1234567891011121314151617mysql&gt; create table t14( -&gt; field time -&gt; );Query OK, 0 rows affected (0.00 sec)mysql&gt; insert into t14 values (&apos;12:12:12&apos;);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t14 values (&apos;212:12:12&apos;);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t14 values (&apos;838:59:59&apos;);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into t14 values (&apos;839:00:00&apos;); # 操作范围ERROR 1292 (22007): Incorrect time value: &apos;839:00:00&apos; for column &apos;field&apos; at row 1mysql&gt; 多学一招：time支持以天的方式插入 123456789101112mysql&gt; insert into t14 values (&apos;10 10:10:10&apos;);Query OK, 1 row affected (0.02 sec)mysql&gt; select * from t14;+-----------+| field |+-----------+| 12:12:12 || 212:12:12 || 838:59:59 || 250:10:10 |+-----------+]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL基本操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库基本操作5]]></title>
    <url>%2F2018%2F09%2F12%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C5%2F</url>
    <content type="text"><![CDATA[1.1 数据类型——值类型1.1.1 整型 类型 字节 范围 tinyint 1 -128~127 smallint 2 -32768~32767 mediumint 3 -8388608~8388607 int 4 -2^31^~2^31^-1 bigint 8 -2^63^~2^63^-1 1、无符号整数（unsigned）：无符号数没有负数，正数部分是有符号的两倍。 例题 12345678910111213141516mysql&gt; create table stu( -&gt; id smallint unsigned auto_increment primary key comment &apos;主键&apos;, -&gt; age tinyint unsigned not null comment &apos;年龄&apos;, -&gt; money bigint unsigned comment &apos;存款&apos; -&gt; );Query OK, 0 rows affected (0.06 sec)mysql&gt; desc stu;+-------+----------------------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+-------+----------------------+------+-----+---------+----------------+| id | smallint(5) unsigned | NO | PRI | NULL | auto_increment || age | tinyint(3) unsigned | NO | | NULL | || money | bigint(20) unsigned | YES | | NULL | |+-------+----------------------+------+-----+---------+----------------+3 rows in set, 3 warnings (0.00 sec) 2、整型支持显示宽度（最小的显示位数） 比如int(5)，如果数值的位数小于5位，前面加上前导0。比如输入12，显示00012；大于5位就不添加前导0。 1脚下留心：必须结合zerofill才起作用 123456789101112131415161718192021222324252627mysql&gt; create table stu( -&gt; id int(5), -&gt; age int(5) zerofill # 填充前导0 -&gt; );Query OK, 0 rows affected (0.02 sec)mysql&gt; desc stu;+-------+--------------------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+--------------------------+------+-----+---------+-------+| id | int(5) | YES | | NULL | || age | int(5) unsigned zerofill | YES | | NULL | |+-------+--------------------------+------+-----+---------+-------+2 rows in set (0.02 sec)mysql&gt; insert into stu values (1,11);mysql&gt; insert into stu values (1111111,2222222);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from stu;+---------+---------+| id | age |+---------+---------+| 1 | 00011 || 1111111 | 2222222 | # 注意：age填充了前导0+---------+---------+2 rows in set (0.00 sec) 1.1.2 浮点型（保存近似值小数） 浮点型 占用字节 范围 float（单精度） 4 -3.4E+38~3.4E+38 double（双精度） 8 -1.8E+308~1.8E+308 1、浮点数声明: float(M,D) double(M,D) M：总位数 D：小数位数 例题； 12345678910111213141516mysql&gt; create table t1( -&gt; num1 float(5,2), #总位数是5，小数位数是2，那么整数位数是3， -&gt; num2 double(4,1) -&gt; );Query OK, 0 rows affected (0.08 sec)mysql&gt; insert into t1 values (1.23,1.23); #如果精度超出了允许的范围，会四舍五入Query OK, 1 row affected (0.00 sec)mysql&gt; select * from t1;+------+------+| num1 | num2 |+------+------+| 1.23 | 1.2 | #如果精度超出了允许的范围，会四舍五入+------+------+1 row in set (0.00 sec) 2、浮点的精度可能会丢失【精度指的是小数】 1.1.3 定点数语法：decimal(M,D) 123456789101112131415mysql&gt; create table t4( -&gt; num decimal(20,19) -&gt; );Query OK, 0 rows affected (0.00 sec)mysql&gt; insert into t4 values (1.1234567890123456789);Query OK, 1 row affected (0.01 sec)mysql&gt; select * from t4;+-----------------------+| num |+-----------------------+| 1.1234567890123456789 |+-----------------------+1 row in set (0.00 sec) 123多学一招：1、定点数是变长的，大致每9个数字用4个字节来存储。定点数之所以能保存精确的小数，因为整数和小数是分开存储的。占用的资源比浮点数要多。2、定点数和浮点数都支持显示宽度和无符号数。 1.2 数据类型——字符型 数据类型 描述 长度 char(长度) 定长 最大255 varchar(长度) 变长 最大65535 tinytext 大段文本 2^8^-1=255 text 大段文本 2^16^-1=65535 mediumtext 大段文本 2^24^-1 longtext 大段文本 2^32^-1 1、char(10)和varchar(10)的区别？ 答：相同点：它们最多只能保存10个字符； ​ 不同点：char不回收多余的字符，varchar会回收多余的字符。 ​ char效率高，浪费空间，varchar节省空间，效率比char低。 2、char的最大长度是255。 3、varchar理论长度是65535字节,实际根本达不到。具体长度与字符编码有关。 4、一个记录的总长度不能超过65535个字节。 5、大块文本（text）不计算在总长度中,一个大块文本只占用10个字节来保存文本的地址。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL基本操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库基本操作4]]></title>
    <url>%2F2018%2F09%2F12%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C4%2F</url>
    <content type="text"><![CDATA[1.6 SQL分类DDL（data definition language）数据库定义语言create、alter、drop、shopDML（data manipulation language）数据操纵语言select、update、insert、deleteDCL（Data Control Language）数据库控制语言,是用来设置或更改数据库用户或角色权限的语句1.7 数据表的文件介绍一个数据库对应一个文件夹 一个表对应一个或多个文件 引擎是myisam，一个表对应三个文件 引擎是innodb,一个表对应一个表结构文件 所有的innodb引擎的数据统一的存放在data\ibdata1文件中。如果数据量很大，MySQL会自动的创建ibdata2，ibdata3，…，目的就是为了便于管理。 引擎是memory，数据存储在内存中，重启服务数据丢失，但是读取速度非常快。 1.8 字符集字符集：字符在保存和传输时对应的二进制编码集合。 创建测试数据库 12345mysql&gt; create table stu( -&gt; id int primary key, -&gt; name varchar(20) -&gt; );Query OK, 0 rows affected (0.00 sec) 插入中文报错 分析原因： 客户端通过GBK发送的命令 但是，服务用utf8解释命令 设置服务器，用gbk字符编码接受客户端发来的命令 测试：插入中文，成功 查询数据，发现数据乱码 原因：以utf返回的结果，客户端用gbk来接受 解决：服务器用gbk返回数据 再次测试，查询数据 总结：客户端编码、character_set_client、character_set_results三个编码的值一致即可操作中文。 多学一招：我们只要设置“set names 字符编码”，就可以更改character_set_client、character_set_results的值。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL基本操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库基本操作3]]></title>
    <url>%2F2018%2F09%2F11%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C3%2F</url>
    <content type="text"><![CDATA[1.5 数据操作创建测试表 1234567mysql&gt; create table stu( -&gt; id int auto_increment primary key comment &apos;主键&apos;, -&gt; name varchar(20) not null, -&gt; addr varchar(50) default &apos;地址不详&apos;, -&gt; score int comment &apos;成绩&apos; -&gt; );Query OK, 0 rows affected (0.01 sec) 1.5.1 插入数据插入一条数据1语法：insert into 表名 (字段名, 字段名,…) values (值1, 值1,…) 例题一：插入数据 12mysql&gt; insert into stu (id,name,addr,score) values (1,&apos;tom&apos;,&apos;上海&apos;,88);Query OK, 1 row affected (0.11 sec) 例题二：插入的字段可以和表的字段顺序不一致。值的顺序必须和插入字段的顺序一致。 12mysql&gt; insert into stu (name,score,addr,id) values (&apos;berry&apos;,77,&apos;北京&apos;,2);Query OK, 1 row affected (0.00 sec) 例题三：可以插入部分字段，但是，非空字段必须插入 1mysql&gt; insert into stu (id,name,addr) values (3,&apos;ketty&apos;,&apos;上海&apos;); 例题四：自动增长字段不用插入，数据库会自动插入增长的数字 12mysql&gt; insert into stu (name,addr) values (&apos;rose&apos;,&apos;北京&apos;);Query OK, 1 row affected (0.00 sec) 例题五：自动增长列的值插入null即可 12mysql&gt; insert into stu (id,name,addr,score) values (null,&apos;李白&apos;,&apos;上海&apos;,66);Query OK, 1 row affected (0.00 sec) 例题六：插入值的顺序和个数与表字段的顺序和个数一致，插入的字段可以省略 12mysql&gt; insert into stu values (null,&apos;杜甫&apos;,&apos;北京&apos;,null);Query OK, 1 row affected (0.00 sec) 例题七：通过default关键字插入默认值 1mysql&gt; insert into stu values (null,&apos;李清照&apos;,default,66); 12脚下留心：1、插入字段的顺序与值的顺序必须一致 插入多条数据123mysql&gt; insert into stu values (null,&apos;辛弃疾&apos;,default,66),(null,&apos;岳飞&apos;,&apos;河南&apos;,77);Query OK, 2 rows affected (0.00 sec)Records: 2 Duplicates: 0 Warnings: 0 1.5.2 更新数据语法： 1update 表名 set 字段=值 [where 条件] 例题一：将1号学生的地址改成山东 1mysql&gt; update stu set addr=&apos;山东&apos; where id=1 例题二：将ketty的成绩改为99 1mysql&gt; update stu set score=99 where name=&apos;ketty&apos;; 例题三：将berry地址改成上海，成绩改成66 1mysql&gt; update stu set addr=&apos;上海&apos;,score=66 where name=&apos;berry&apos;; 例题四：将上海的学生成绩改为60 1mysql&gt; update stu set score=60 where addr=&apos;上海&apos;; 例题五：条件可以省略，如果省略，更改所有数据（将所有数据的地址改为湖南，成绩改为70） 1mysql&gt; update stu set addr=&apos;湖南&apos;,score=70; 例题六：将2、3的学生成绩改为65 1mysql&gt; update stu set score=65 where id=2 or id=3; 1.5.3 删除数据语法 1delete from 表名 [where 条件] 例题一：删除学号是1号的学生 1mysql&gt; delete from stu where id=1; 例题二：删除成绩小于等于65分的 1mysql&gt; delete from stu where score&lt;=65; 例题三：删除表中所有记录 1mysql&gt; delete from stu; 1.5.4 清空表语法： 1truncate table 表名 例题 12mysql&gt; truncate table stu;Query OK, 0 rows affected (0.00 sec) 123脚下留心：delete from 表和truncate table 表区别？delete from 表：遍历表记录，一条一条的删除truncate table：将原表销毁，再创建一个同结构的新表。就清空表而言，这种方法效率高。 1.5.5查询表语法： 1select 列名 from 表 例题： 1234567891011121314151617181920212223mysql&gt; select name,score from stu;+------+-------+| name | score |+------+-------+| rose | 88 |+------+-------+1 row in set (0.00 sec)mysql&gt; select id,name,addr,score from stu;+----+------+------+-------+| id | name | addr | score |+----+------+------+-------+| 1 | rose | 上海 | 88 |+----+------+------+-------+1 row in set (0.00 sec)mysql&gt; select * from stu; # *表示所有字段+----+------+------+-------+| id | name | addr | score |+----+------+------+-------+| 1 | rose | 上海 | 88 |+----+------+------+-------+1 row in set (0.00 sec)]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL基本操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库基本操作2]]></title>
    <url>%2F2018%2F09%2F11%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C2%2F</url>
    <content type="text"><![CDATA[1.4 表的操作1.4.1 显示所有表语法： 1show tables 1.4.2 创建表语法： 1234create table [if not exists] 表名( 字段名 数据类型 [null|not null] [auto_increment] [primary key] [comment], 字段名 数据类型 [default]…)engine=存储引擎 单词 123456null | not null 空|非空default 默认值auto_increment 自动增长primary key 主键comment 备注engine 引擎 innodb myisam memory 引擎是决定数据存储的方式 创建简单的表 123456789101112131415161718192021mysql&gt; create database study;Query OK, 1 row affected (0.00 sec)mysql&gt; use study;Database changedmysql&gt; show tables;Empty set (0.05 sec)# 创建表mysql&gt; create table stu( -&gt; id int, -&gt; name varchar(30) -&gt; );Query OK, 0 rows affected (0.13 sec)# 查看创建的表mysql&gt; show tables;+------------------+| Tables_in_itcast |+------------------+| stu |+------------------+ 创建复杂的表 12345678910mysql&gt; set names gbk; # 设置字符编码Query OK, 0 rows affected (0.05 sec)mysql&gt; create table if not exists teacher( -&gt; id int auto_increment primary key comment &apos;主键&apos;, -&gt; name varchar(20) not null comment &apos;姓名&apos;, -&gt; phone varchar(20) comment &apos;电话号码&apos;, -&gt; `add` varchar(100) default &apos;地址不详&apos; comment &apos;地址&apos; -&gt; )engine=innodb;Query OK, 0 rows affected (0.09 sec) 多学一招：create table 数据库名.表名，用于给指定的数据库创建表 1234mysql&gt; create table data.stu( #给data数据库中创建stu表 -&gt; id int, -&gt; name varchar(10));Query OK, 0 rows affected (0.00 sec) 1.4.3 显示创建表的语句语法： 1show create table 表名 显示创建teacher表的语句 1234567891011121314151617181920mysql&gt; show create table teacher;+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| Table | Create Table |+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| teacher | CREATE TABLE `teacher` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &apos;主键&apos;, `name` varchar(20) NOT NULL COMMENT &apos;姓名&apos;, `phone` varchar(20) DEFAULT NULL COMMENT &apos;电话号码&apos;, `add` varchar(100) DEFAULT &apos;地址不详&apos; COMMENT &apos;地址&apos;, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 | 将两个字段竖着排列 show create table 表名\G 1234567891011mysql&gt; show create table teacher\G;*************************** 1. row *************************** Table: teacherCreate Table: CREATE TABLE `teacher` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &apos;主键&apos;, `name` varchar(20) NOT NULL COMMENT &apos;姓名&apos;, `phone` varchar(20) DEFAULT NULL COMMENT &apos;电话号码&apos;, `add` varchar(100) DEFAULT &apos;地址不详&apos; COMMENT &apos;地址&apos;, PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf81 row in set (0.00 sec) 1.4.4 查看表结构语法： 1desc[ribe] 表名 查看teacher表的结构 123456789101112131415161718192021mysql&gt; describe teacher;+-------+--------------+------+-----+----------+----------------+| Field | Type | Null | Key | Default | Extra |+-------+--------------+------+-----+----------+----------------+| id | int(11) | NO | PRI | NULL | auto_increment || name | varchar(20) | NO | | NULL | || phone | varchar(20) | YES | | NULL | || add | varchar(100) | YES | | 地址不详 | |+-------+--------------+------+-----+----------+----------------+4 rows in set (0.08 sec)mysql&gt; desc teacher;+-------+--------------+------+-----+----------+----------------+| Field | Type | Null | Key | Default | Extra |+-------+--------------+------+-----+----------+----------------+| id | int(11) | NO | PRI | NULL | auto_increment || name | varchar(20) | NO | | NULL | || phone | varchar(20) | YES | | NULL | || add | varchar(100) | YES | | 地址不详 | |+-------+--------------+------+-----+----------+----------------+4 rows in set (0.01 sec) 1.4.5 删除表语法： 1drop table [if exists] 表1，表2,… 删除表 12mysql&gt; drop table stu;Query OK, 0 rows affected (0.08 sec) 如果删除一个不存在的表就会报错，删除的时候可以判断一下，存在就删除。 12345mysql&gt; drop table stu;ERROR 1051 (42S02): Unknown table &apos;stu&apos;mysql&gt; drop table if exists stu;Query OK, 0 rows affected, 1 warning (0.00 sec) 可以一次删除多个表 12mysql&gt; drop table a1,a2;Query OK, 0 rows affected (0.00 sec) 1.4.6 修改表1语法：alter table 表名 1、添加字段：alter table 表名add [column] 字段名 数据类型 [位置] 例题一：添加字段 123456789101112131415mysql&gt; alter table teacher add age int;Query OK, 0 rows affected (0.09 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc teacher;+-------+--------------+------+-----+----------+----------------+| Field | Type | Null | Key | Default | Extra |+-------+--------------+------+-----+----------+----------------+| id | int(11) | NO | PRI | NULL | auto_increment || name | varchar(20) | NO | | NULL | || phone | varchar(20) | YES | | NULL | || add | varchar(100) | YES | | 地址不详 | || age | int(11) | YES | | NULL | |+-------+--------------+------+-----+----------+----------------+5 rows in set (0.00 sec) 例题二：在第一个位置上添加字段 123456789101112131415mysql&gt; alter table teacher add email varchar(30) first;Query OK, 0 rows affected (0.00 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc teacher;+-------+--------------+------+-----+----------+----------------+| Field | Type | Null | Key | Default | Extra |+-------+--------------+------+-----+----------+----------------+| email | varchar(30) | YES | | NULL | || id | int(11) | NO | PRI | NULL | auto_increment || name | varchar(20) | NO | | NULL | || phone | varchar(20) | YES | | NULL | || add | varchar(100) | YES | | 地址不详 | || age | int(11) | YES | | NULL | |+-------+--------------+------+-----+----------+----------------+ 例题三：在指定的字段后添加字段 1234567891011121314151617mysql&gt; alter table teacher add sex varchar(2) after name;Query OK, 0 rows affected (0.00 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; desc teacher;+-------+--------------+------+-----+----------+----------------+| Field | Type | Null | Key | Default | Extra |+-------+--------------+------+-----+----------+----------------+| email | varchar(30) | YES | | NULL | || id | int(11) | NO | PRI | NULL | auto_increment || name | varchar(20) | NO | | NULL | || sex | varchar(2) | YES | | NULL | || phone | varchar(20) | YES | | NULL | || add | varchar(100) | YES | | 地址不详 | || age | int(11) | YES | | NULL | |+-------+--------------+------+-----+----------+----------------+7 rows in set (0.00 sec) 2、删除字段：alter table 表 drop [column] 字段名 123mysql&gt; alter table teacher drop email;Query OK, 0 rows affected (0.06 sec)Records: 0 Duplicates: 0 Warnings: 0 3、修改字段(改名改类型)：alter table 表 change [column] 原字段名 新字段名 数据类型 … 将字段sex改为xingbie，数据类型为int 123mysql&gt; alter table teacher change sex xingbie int;Query OK, 0 rows affected (0.00 sec)Records: 0 Duplicates: 0 Warnings: 0 4、修改字段（不改名）:alter table 表 modify 字段名 字段属性… 将性别的数据类型改为varchar(2) 123mysql&gt; alter table teacher modify xingbie varchar(2);Query OK, 0 rows affected (0.00 sec)Records: 0 Duplicates: 0 Warnings: 0 5、修改引擎：alter table 表名 engine=引擎名 123mysql&gt; alter table teacher engine=myisam;Query OK, 0 rows affected (0.05 sec)Records: 0 Duplicates: 0 Warnings: 0 6、修改表名：alter table 表名 rename to 新表名 12345678910mysql&gt; alter table teacher rename to stu;Query OK, 0 rows affected (0.00 sec)mysql&gt; show tables;+------------------+| Tables_in_itcast |+------------------+| stu |+------------------+1 row in set (0.00 sec) 1.4.7 复制表1语法一：create table 新表 select 字段 from 旧表 特点：不能复制父表的主键，能够复制父表的数据 12345678910111213141516171819202122mysql&gt; create table stu1 select * from stu;Query OK, 1 row affected (0.06 sec)Records: 1 Duplicates: 0 Warnings: 0mysql&gt; select * from stu1; # 查看数据复制到新表中+----+------+------+-------+| id | name | addr | score |+----+------+------+-------+| 1 | rose | 上海 | 88 |+----+------+------+-------+1 row in set (0.00 sec)mysql&gt; desc stu1; # 主键没有复制+-------+-------------+------+-----+----------+-------+| Field | Type | Null | Key | Default | Extra |+-------+-------------+------+-----+----------+-------+| id | int(11) | NO | | 0 | || name | varchar(20) | NO | | NULL | || addr | varchar(50) | YES | | 地址不详 | || score | int(11) | YES | | NULL | |+-------+-------------+------+-----+----------+-------+4 rows in set (0.00 sec) 1语法二：create table 新表 like 旧表 特点：只能复制表结构，不能复制表数据 123456789101112131415Query OK, 0 rows affected (0.00 sec)mysql&gt; select * from stu2; # 数据没有复制Empty set (0.01 sec)mysql&gt; desc stu2; # 主键复制了+-------+-------------+------+-----+----------+----------------+| Field | Type | Null | Key | Default | Extra |+-------+-------------+------+-----+----------+----------------+| id | int(11) | NO | PRI | NULL | auto_increment || name | varchar(20) | NO | | NULL | || addr | varchar(50) | YES | | 地址不详 | || score | int(11) | YES | | NULL | |+-------+-------------+------+-----+----------+----------------+4 rows in set (0.00 sec)]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL基本操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库基本操作1]]></title>
    <url>%2F2018%2F09%2F11%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C1%2F</url>
    <content type="text"><![CDATA[1.1 连接服务器通过命令行面板连接 1234host：主机 -husername：用户名 -upassword：密码 -pport：端口 -P 1多学一招：如果MySQL服务器在本地，IP地址可以省略；如果MySQL服务器用的是3306端口，-P也是可以省略 1.2关闭连接12345方法一：exit方法二：quit方法三：\q 1脚下留心：MySQL中的命令后面要加分号，windows命令行的命令后面不用加分号。 1.3数据库的操作1.3.1 显示数据库123456789101112语法：show databases mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || test |+--------------------+4 rows in set (0.11 sec) 安装MySQL后，MySQL自带了4个数据库 information_schema：存储了MySQL服务器管理数据库的信息。 performance_schema：MySQL5.5新增的表，用来保存数据库服务器性能的参数 mysql：MySQL系统数据库，保存的登录用户名，密码，以及每个用户的权限等等 test：给用户学习和测试的数据库。 1.3.2 创建数据库1语法：create database [if not exists] `数据名` [字符编码] 创建数据库： 12mysql&gt; create database stu;Query OK, 1 row affected (0.09 sec) 如果创建的数据库已存在，就会报错 12mysql&gt; create database stu;ERROR 1007 (HY000): Can&apos;t create database &apos;stu&apos;; database exists 解决：创建数据库的时候判断一下数据库是否存在，如果不存在再创建 12mysql&gt; create database if not exists stu;Query OK, 1 row affected, 1 warning (0.00 sec) 如果数据库名是关键字和特殊字符要报错 解决：在特殊字符、关键字行加上反引号 12mysql&gt; create database `create`;Query OK, 1 row affected (0.05 sec) 1多学一招：为了创建数据库时万无一失，我们可以在所有的数据库名上加上反引号 创建数据库的时候可以指定字符编码 12345mysql&gt; create database teacher charset=gbk;Query OK, 1 row affected (0.01 sec)gbk 简体中文gb2312： 简体中文utf8： 通用字符编码 1脚下留心：创建数据库如果不指定字符编码，默认和MySQL服务器的字符编码是一致的。 1.3.3 删除数据库1语法：drop database [if exists] 数据库名 删除数据库 12mysql&gt; drop database teacher;Query OK, 0 rows affected (0.00 sec) 如果删除的数据库不存在，会报错 123mysql&gt; drop database teacher;ERROR 1008 (HY000): Can&apos;t drop database &apos;teacher&apos;; database doesn&apos;t existmysql&gt; 解决：删除之前判断一下，如果存在就删除 12mysql&gt; drop database if exists teacher;Query OK, 0 rows affected, 1 warning (0.00 sec) 1.3.4 显示创建数据库的SQL语句1语法：show create database 数据库名 123456789101112131415mysql&gt; show create database stu;+----------+--------------------------------------------------------------+| Database | Create Database |+----------+--------------------------------------------------------------+| stu | CREATE DATABASE `stu` /*!40100 DEFAULT CHARACTER SET utf8 */ |+----------+--------------------------------------------------------------+1 row in set (0.01 sec)mysql&gt; show create database teacher;+----------+-----------------------------------------------------------------+| Database | Create Database |+----------+-----------------------------------------------------------------+| teacher | CREATE DATABASE `teacher` /*!40100 DEFAULT CHARACTER SET gbk */ |+----------+-----------------------------------------------------------------+1 row in set (0.00 sec) 1.3.5 修改数据库修改数据库的字符编码 语法： 1alter database 数据库名 charset=字符编码 例题 12345678910mysql&gt; alter database teacher charset=utf8;Query OK, 1 row affected (0.00 sec)mysql&gt; show create database teacher;+----------+------------------------------------------------------------------+| Database | Create Database |+----------+------------------------------------------------------------------+| teacher | CREATE DATABASE `teacher` /*!40100 DEFAULT CHARACTER SET utf8 */ |+----------+------------------------------------------------------------------+1 row in set (0.00 sec) 1.3.6 选择数据库语法： 1use 数据库名 选择数据库 12mysql&gt; use stu;Database changed]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL基本操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图文讲解MySQL安装教程]]></title>
    <url>%2F2018%2F09%2F11%2F%E5%9B%BE%E6%96%87%E8%AE%B2%E8%A7%A3MySQL%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[今天来讲解一下MySQL的安装教程 1：首先获取一个安装文件，如下图，可以走网上或者某些途径拿到 2：然后双击安装，如图 3：选择安装模式，这里我们选择自定义安装 4：选择数据库以及数据库文件安装位置 在这里我们数据库和数据库文件直接选择安装在C盘下面 5：下面一些很多都可以直接点Next选择默认的配置 这里的并发数一般选择系统的默认给我们设置的 端口也选择系统默认的，一般不用改 这里的编码方式选择utf8 服务名称可以自己设置，一般默认，下面的选项是添加环境变量，如果不勾选安装完自己也可以添加一下 这里初始化一下密码 然后点击安装等待4个勾全部勾上点击完成6：MySQL目录 这里是安装的目录，里面的data就是之前选择数据文件安装的路径，我们放在一起所以在这里，你可以自己设置路径7：启动/关闭MySQL服务方法一：在服务面板中启动或关闭控制面板项——管理工具——服务，选择相应服务，右键执行操作 方法二：通过命令行启动\关闭net start 服务名： 启动MySQL服务 net stop 服务器： 关闭MySQL服务 注意：必须通过管理员身份启动命令行这样数据库就安装好了，然后就可以进入数据库操作了]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL安装教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-pwd命令]]></title>
    <url>%2F2018%2F09%2F10%2FLinux%E5%91%BD%E4%BB%A4-pwd%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Linux中用 pwd 命令来查看”当前工作目录“的完整路径。 简单得说，每当你在终端进行操作时，你都会有一个当前工作目录。 在不太确定当前位置时，就会使用pwd来判定当前目录在文件系统内的确切位置。 语法1pwd（选项） 选项1234-L 目录连接链接时，输出连接路径-P 输出物理路径--help：显示帮助信息；--version：显示版本信息。 常用范例显示当前位置 1pwd]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-cd命令]]></title>
    <url>%2F2018%2F09%2F10%2FLinux%E5%91%BD%E4%BB%A4-cd%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Linux cd 命令可以说是Linux中最基本的命令语句，其他的命令语句要进行操作，都是建立在使用 cd 命令上的。所以，学习Linux 常用命令，首先就要学好 cd 命令的使用方法技巧。 语法1cd (选项) (参数) 选项123-p 如果要切换到的目标目录是一个符号连接，直接切换到符号连接指向的目标目录-L 如果要切换的目标目录是一个符号的连接，直接切换到字符连接名代表的目录，而非符号连接所指向的目标目录。- 当仅实用&quot;-&quot;一个选项时，当前工作目录将被切换到环境变量&quot;OLDPWD&quot;所表示的目录。 常用范例1）进入系统根目录 1cd / 2）进入当前用户主目录 12cd ~cd 3）跳转到指定目录 1cd /opt/soft 4）返回进入此目录之前所在的目录 1cd -]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux命令-ls]]></title>
    <url>%2F2018%2F09%2F10%2FLinux%E5%91%BD%E4%BB%A4-ls%2F</url>
    <content type="text"><![CDATA[ls命令时linux下最常用的命令。ls命令就是list的缩写，缺省下ls用来打印出当前目录的清单，如果ls指定其他目录，那么就会显示指定目录里的文件及文件夹清单。 通过ls命令不仅可以查看linux文件夹包含的文件，而且可以查看文件权限（包括目录、文件夹、文件权限），查看目录信息等等。 语法1ls (选项) （参数） 选项常用选项 123456789-a：显示所有档案及目录（ls内定将档案名或目录名称为“.”的视为影藏，不会列出）；-l：以长格式显示目录下的内容列表。输出的信息从左到右依次包括文件名，文件类型、权限模式、硬连接数、所有者、组、文件大小和文件的最后修改时间等；-h:–human-readable 以容易理解的格式列出文件大小 (例如 1K 234M 2G);-s：显示文件和目录的大小，以区块为单位；-t：用文件和目录的更改时间排序；-r：以文件名反序排列并输出目录内容列表；-d：仅显示目录名，而不显示目录下的内容列表。显示符号链接文件本身，而不显示其所指向的目录列表；-R：递归处理，将指定目录下的所有文件及子目录一并处理；--color[=WHEN]：使用不同的颜色高亮显示不同类型的。 其他选项 123456789101112131415-A：显示除影藏文件“.”和“..”以外的所有文件列表；-C：多列显示输出结果。这是默认选项；-F：在每个输出项后追加文件的类型标识符，具体含义：“*”表示具有可执行权限的普通文件，“/”表示目录，“@”表示符号链接，“|”表示命令管道FIFO，“=”表示sockets套接字。当文件为普通文件时，不输出任何标识符；-b：将文件中的不可输出的字符以反斜线“”加字符编码的方式输出；-f：此参数的效果和同时指定“aU”参数相同，并关闭“lst”参数的效果；-i：显示文件索引节点号（inode）。一个索引节点代表一个文件；-k：以KB（千字节）为单位显示文件大小；-m：用“,”号区隔每个文件和目录的名称；--n：以用户识别码和群组识别码替代其名称；-L：如果遇到性质为符号链接的文件或目录，直接列出该链接所指向的原始文件或目录；-c：与“-lt”选项连用时，按照文件状态时间排序输出目录内容，排序的依据是文件的索引节点中的ctime字段,与“-l”选项连用时，则排序的一句是文件的状态改变时间；--file-type：与“-F”选项的功能相同，但是不显示“*”；--full-time：列出完整的日期与时间；–-help 显示此帮助信息–-version 显示版本信息 参数目录：指定要显示列表的目录，也可以是具体的文件。 常用范例1）列出/opt文件夹下的所有文件和目录的详细资料 1# ls -l -R /opt/ 在使用 ls 命令时要注意命令的格式：在命令提示符后，首先是命令的关键字，接下来是命令参数，在命令参数之前要有一短横线“-”，所有的命令参数都有特定的作用，自己可以根据需要选用一个或者多个参数，在命令参数的后面是命令的操作对象。在以上这条命令“ ls -l -R /home/peidachang”中，“ls” 是命令关键字，“-l -R”是参数，“ /home/peidachang”是命令的操作对象。在这条命令中，使用到了两个参数，分别为“l”和“R”，当然，你也可以把他们放在一起使用，如下所示： 1ls -lR /opt 2）列出当前目录中所有以“t”开头的目录的详细内容，可以使用如下命令： 1#ls -l t* 可以查看当前目录下文件名以“t”开头的所有文件的信息。其实，在命令格式中，方括号内的内容都是可以省略的，对于命令ls而言，如果省略命令参数和操作对象，直接输入“ ls ”，则将会列出当前工作目录的内容清单。 3）列出目前工作目录下所有名称是s 开头的档案，愈新的排愈后面，可以使用如下命令： 1#ls -ltr s* 4）列出目前工作目录下所有档案及目录;目录于名称后加”/“, 可执行档于名称后加”*“ 1# ls -AF 5）计算当前目录下的文件数和目录数 12ls -l * |grep &quot;^-&quot;|wc -l ---文件个数 ls -l * |grep &quot;^d&quot;|wc -l ---目录个数 6）在ls中列出文件的绝对路径 1ls | sed &quot;s:^:`pwd`/:&quot; 7）列出当前目录下的所有文件（包括隐藏文件）的绝对路径， 对目录不做递归 1find $PWD -maxdepth 1 | xargs ls -ld 8）递归列出当前目录下的所有文件（包括隐藏文件）的绝对路径 1find $PWD | xargs ls -ld 9）指定文件时间输出格式 12ls -tl --time-style=full-iso ls -ctl --time-style=long-iso 10）列出文件并标记颜色分类 1ls --color=auto 颜色蓝色—–目录绿色—–可执行文件白色—–一般性文件，如文本文件，配置文件等红色—–压缩文件或归档文件浅蓝色—-链接文件红色闪烁—-链接文件存在问题黄色—–设备文件青黄色—-管道文件 参考链接： http://man.linuxde.net/ls http://www.9usb.net/201005/linux-ls.html http://www.cnblogs.com/peida/archive/2012/10/23/2734829.html]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Docker镜像]]></title>
    <url>%2F2018%2F09%2F10%2F%E4%BD%BF%E7%94%A8Docker%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[镜像（image）是Docker三大核心概念中最为重要的，自Docker诞生之日起“镜像”就是相关社区最为热门的关键词。 Docker运行容器前需要本地存在对应的镜像，如果镜像没保存在本地，Docker会尝试从默认镜像仓库下载（默认使用Docker Hub公共注册服务器中的仓库），用户也可以通过配置，使用自定义的镜像仓库。 获取镜像镜像是运行容器的前提，官方的Docker Hub网站已经提供了数十万个镜像供大家开放下载。 可以使用docker pull命令直接从Docker Hub镜像源来下载镜像。改命令的格式为docker pull NAME[:TAG]。其中，NAME是镜像仓库的名称（用来区分镜像），TAG是镜像的标签（往往用来表示版本信息）。通常情况下，描述一个镜像需要包括“名称+标签”信息。 例如，获取一个Ubuntu14.04系统的基础镜像可以使用如下的命令： 1docker pull ubuntu:14.04 对于docker镜像来说，如果不显示指定TAG，则默认会选择latest标签，这会下载仓库中最新版本的镜像。 从稳定性上考虑，不要在生产环境中忽略镜像的标签信息或使用默认的latest标记的镜像。 使用images命令列出镜像123docker images REPOSITORY TAG IMAGE ID CREATED SIZEdocker.io/tomcat latest 08f8166740f8 3 months ago 366.7 MB 使用tag命令添加镜像标签1docker tag docker.io/tomcat:latest mytomcat:latest 使用inspect命令查看详细信息12345678910111213docker inspect docker.io/tomcat:latest [ &#123; &quot;Id&quot;: &quot;sha256:08f8166740f822b79f1306648591c1013105ccb5dca0a15320c54e991e0f9538&quot;, &quot;RepoTags&quot;: [ &quot;docker.io/tomcat:latest&quot; ], &quot;RepoDigests&quot;: [], &quot;Parent&quot;: &quot;&quot;, &quot;Comment&quot;: &quot;&quot;, &quot;Created&quot;: &quot;2017-05-05T23:55:12.275453692Z&quot;, &quot;Container&quot;: &quot;172a4b1f8457373887172ff3f36c15708bbda7430c43963796ffd98f3dd6345e&quot;,..... 返回的是一个JSON格式的信息，如果我们只要其中一项内容时，可以使用参数-f来指定，例如，获取镜像的Os 12docker inspect -f &#123;&#123;&quot;.Architecture&quot;&#125;&#125; docker.io/tomcat:latest amd64 删除镜像使用标签删除镜像 使用docker rmi 命令可以删除镜像，命令格式为dockerrmi IMAGE [IMAGE…],其中IMAGE可以为标签或ID。 如果想强行删除镜像，可以使用-f参数。 创建镜像创建镜像的方法主要有三种：基于已有镜像的容器创建、基于本地模板导入、基于Dockerfile创建。 基于已有镜像的容器创建该方法主要是使用docker commit命令。命令格式为dockercommit [OPTIONS] CONTAINER [REPOSRITORY[:TAG]]，主要信息包括： -a，–author=””：作者信息 - -c，–change=[]：提交时执行Dockerfile指令，包括CMD|ENTERYPOINT|ENV|EXPOSE|LABEL|ONBUILD|USER|VOLUME|WORKDIR等； -m，–message=“”：提交消息； -p ，–pause=true：提交时暂停容器执行。 基于本地模板导入略 搜索镜像使用docker search命令可以搜索远端仓库中共享的镜像，默认搜索官方仓库中的镜像。用法为docker search TERM，支持的参数主要包括 –automated=true|false:仅显示自动创建的镜像，默认为否； –no-trunc=true|false:输出信息不截断显示，默认为否； -s，–stars=X:指定仅显示评价为指定星级以上的镜像，默认为0，即输出所有镜像。 存出和载入镜像用户可以使用docker save和docker load命令来存出和载入镜像。 存出镜像如果要导出镜像到本地，可以使用docker save名利，例如，导出本地的tomcat:latest镜像为tomcat.tart，如下所示： 1docker save -o /images/tomcat.tart tomcat:latest 之后，用户就可以通过复制tomcat.tar文件将改镜像分享给他人。 载入镜像可以使用docker load将导出的tar文件再导入本地镜像库，例如从tomcat.tar 导入镜像到本地镜像列表，如下所示： 12docker load --input tomcat.tar 或docker load &lt; tomcat.tar 这将导入镜像及其相关的元数据信息（包括标签等）。导入成功后，可以使用docker images命令进行查看。 上传镜像可以使用docker push 命令上传镜像到仓库，默认上传到DockerHub官方仓库（需要登录）。命令格式为： docker push NAME[:TAG] 用户在Docker Hub网站注册后可以上传自制的镜像。例如用户user上传本地的test：latest镜像，可以先添加新的标签user/test:latest，然后用docker push命令上传镜像： 12docker tag test:latest user/test:latestdocker push user/test:latest 第一次上传时，会提示输入登录信息或进行注册。]]></content>
      <categories>
        <category>git使用及讲解</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识容器与Docker]]></title>
    <url>%2F2018%2F09%2F10%2F%E5%88%9D%E8%AF%86%E5%AE%B9%E5%99%A8%E4%B8%8EDocker%2F</url>
    <content type="text"><![CDATA[如果说主机时代大家拼的是单个服务器物理性能（如CPU主频和内存），那么在云时代，最为看重的则是凭借虚拟化技术所构建的集群处理能力。 伴随着信息技术的飞速发展，虚拟化技术早已经广泛应用到各种关键场景中。从20世纪60年代IBM推出的大型主机虚拟化，到后来以Xen、KVM为代表的虚拟机虚拟化，再到现在以Docker为代表的的容器技术，虚拟化技术自身也在不断进行创新和突破。 什么是DockerDocker开源背景Docker是基于Go语言实现的开源容器项目，诞生于2013年年初，最初发起者是dotCloud公司。 Docker项目已加入了linux基金会，并遵循Apache2.0协议，全部开源代码均在https://github.com/docker/docker上进行维护。在linux基金会最近一次关于“最受欢迎的云计算开源项目”的调查中，Docker仅次于2010年发起的Openstack项目，并仍处于上升趋势。 Docker的构想是要实现“Build，Ship and Run Any App，Anywhere”，即通过对应用的封装（Packaging）、分发（Distribution）、部署（Deployment）、运行（Runtime）生命周期进行管理，达到应用组件“一次封装，到处运行”的目的。 linux容器技术—巨人的肩膀跟着大部分新兴技术的诞生一样，Docker也并非“从石头缝里蹦出来的”，而是站在前人的肩膀上，其中最重要的就是linux容器（linux Containers，LXC）技术。 从Linux容器到Docker简单地讲，可以将Docker容器理解为一种轻量级的沙盒（sandbox）。每个容器内运行着一个应用，不同的容器相互隔离，容器之间也可以通过网络互相通信。容器的创建和停止都十分快速，几乎跟创建和终止原生应用一致；另外，容器自身对系统资源的额外需求也十分有限，远远低于传统虚拟机。很多时候，甚至直接把容器当作应用本身也没有任何问题。 有理由相信，Docker技术会进一步成熟，将会成为更受欢迎的容器虚拟化技术实现，并在云计算和DevOps等领域得到更广泛的应用。 Docker容器虚拟化的好处Docker提供了一种聪明的方式，通过容器来打包应用，解耦应用和运行平台。意味着迁移的时候，只需要在新的服务器上启动需要的容器就可以了，无论新旧服务器是否同一类型的平台。这无疑将节约大量的宝贵时间，并降低部署过程出现问题的风险。 Docker在开发和运维中的优势更快的交付和部署 更高效的资源利用 更轻松的迁移和扩展 更简单的更新管理 Docker与虚拟机的比较 特性 容器 虚拟机 启动速度 秒级 分钟级 性能 接近原生 较弱 内存代价 很小 较多 硬盘使用 一般为MB 一般为GB 运行密度 单机支持上千个容器 一般为几十个 隔离性 安全隔离 完全隔离 迁移性 优秀 一般 Docker与虚拟化]]></content>
      <categories>
        <category>git使用及讲解</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通过blockchain-go分析区块链交易原理]]></title>
    <url>%2F2018%2F09%2F09%2F%E9%80%9A%E8%BF%87blockchain-go%E5%88%86%E6%9E%90%E5%8C%BA%E5%9D%97%E9%93%BE%E4%BA%A4%E6%98%93%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[1.背景在去中心化的区块链中进行交易（转账）是怎么实现的呢？本篇通过blockchain_go来分析一下。需要进行交易，首先就需要有交易的双方以及他们的认证机制，其次是各自的资金账户规则。在分布式账本系统里面，需要有机制能够准确验证一个用户身份以及对账户资金的精确计算，不能出现一丁点差错。在区块链中交易通过Transaction表示，自己的账户余额并不是在每个节点上保存各个用户的一个最终数字，而是通过历史交易信息计算而来（历史交易不可篡改），其中的关键机制是UTXO。 2.身份认证在区块链身份认证是采用RSA非对称加密体系完成，每个用户在会拥有一个“钱包”，钱包是通过安全的椭圆曲线加密算法生成，其中包括一对公私钥。私钥自己保留不能暴露，用作加密，签名等，公钥公开给所有人，用于信息验证等。只要是用私钥签名的信息，就可以通过配对的公钥解码认证，不可抵赖。在blockchain_go中，钱包实现如下： 1234567891011121314151617181920212223// Wallet stores private and public keystype Wallet struct &#123; PrivateKey ecdsa.PrivateKey PublicKey []byte&#125;// NewWallet creates and returns a Walletfunc NewWallet() *Wallet &#123; private, public := newKeyPair() wallet := Wallet&#123;private, public&#125; return &amp;wallet&#125;func newKeyPair() (ecdsa.PrivateKey, []byte) &#123; curve := elliptic.P256() //椭圆曲线 private, err := ecdsa.GenerateKey(curve, rand.Reader) //生成私钥 if err != nil &#123; log.Panic(err) &#125; pubKey := append(private.PublicKey.X.Bytes(),private.PublicKey.Y.Bytes()...) //合成公钥 return *private, pubKey&#125; 钱包最重要的功能就是为用户提供身份认证和加解密的公私钥对。 3.什么是Transaction区块链中的Transaction（交易）就是一批输入和输出的集合，比如A通过交易给B10个代币（token），那么交易就是A输入10代币，输出变成B得到10代币，这样A就减少10代币，B增加10代币，再将这个交易信息存储到区块链中固化后，A和B在区块链中的账号状态就发生了永久性不可逆的变化。 在blockchain_go中transaction的定义如下： 123456789101112131415161718// TXInput represents a transaction inputtype TXInput struct &#123; Txid []byte Vout int Signature []byte PubKey []byte&#125;// TXOutput represents a transaction outputtype TXOutput struct &#123; Value int PubKeyHash []byte&#125;type Transaction struct &#123; ID []byte //交易唯一ID Vin []TXInput //交易输入序列 Vout []TXOutput //交易输出序列&#125; 从定义可以看到Transaction就是输入和输出的集合，输入和输出的关系如下图： 其中tx0，tx1，tx2等是独立的交易，每个交易通过输入产生输出，下面重点看看一个交易的输入和输出单位是怎么回事。 先看输出TXOutput： Value : 表示这个输出中的代币数量PubKeyHash : 存放了一个用户的公钥的hash值，表示这个输出里面的Value是属于哪个用户的输入单元TXInput: Txid : 交易ID（这个输入使用的是哪个交易的输出）Vout : 该输入单元指向本次交易输出数组的下标，通俗讲就是，这个输入使用的是Txid中的第几个输出。Signature : 输入发起方（转账出去方）的私钥签名本Transaction，表示自己认证了这个输入TXInput。PubKey : 输入发起方的公钥通俗来讲，一个TXInput结构表示 : 1我要使用哪个交易(Txid)的哪个输出数组（Transaction.Vout）的下标(Vout)作为我本次输入的代币数值（TXOutput.Value) 因为交易的输入其实是需要指明要输入多少代币（Value），但是TXInput中并没有直接的代币字段，而唯一有代币字段的是在TXOuput中，所以这里使用的方式是在TXInput中指明了自己需要使用的代币在哪个TXOutput中。 TXInput中的Signature字段是发起用户对本次交易输入的签名，PubKey存放了用户的公钥，用于之前的验证（私钥签名，公钥验证）。 3.什么是UTXOUTXO 是 Unspent Transaction Output 的缩写，意指“为花费的交易输出”，是中本聪最早在比特币中采用的一种技术方案。因为比特币中没有账户的概念，也就没有保存用户余额数值的机制。因为区块链中的历史交易都是被保存且不可修改的，而每一个交易（如前所述的Transaction）中又保存了“谁转移了多少给谁”的信息，所以要计算用户账户余额，只需要遍历所有交易进行累计即可。 从第三节的交易图可以看到，每笔交易的输入TXInput都是使用的是其他交易的输出TXOutput（只有输出中保存了该输出是属于哪个用户，价值多少）。如果一笔交易的输出被另外一个交易的输入引用了（TXInput中的Vout指向了该TXOutput），那么这笔输出就是“已花费”。如果一笔交易的输出没有被任何交易的输入引用，那么就是“未花费”。分析上图的tx3交易： tx3有3个输入： input 0 ：来自tx0的output0，花费了这个tx0.output0.input 1 ：来自tx1的output1，花费了这个tx1.output1.input 2 ：来自了tx2的output0，花费了这个tx2.output0.tx3有2个输出： output 0 ：没有被任何后续交易引用，表示“未花费”。output 1 ：被tx4的input1引用，表示已经被花费。因为每一个output都包括一个value和一个公钥身份，所以遍历所有区块中的交易，找出其中所有“未花费”的输出，就可以计算出用户的账户余额。 4.查找未花费的Output如果一个账户需要进行一次交易，把自己的代币转给别人，由于没有一个账号系统可以直接查询余额和变更，而在utxo模型里面一个用户账户余额就是这个用户的所有utxo（未花费的输出）记录的合集，因此需要查询用户的转账额度是否足够，以及本次转账需要消耗哪些output（将“未花费”的output变成”已花费“的output），通过遍历区块链中每个区块中的每个交易中的output来得到结果。 下面看看怎么查找一个特定用户的utxo，utxo_set.go相关代码如下： 123456789101112131415161718192021222324252627282930// FindSpendableOutputs finds and returns unspent outputs to reference in inputsfunc (u UTXOSet) FindSpendableOutputs(pubkeyHash []byte, amount int) (int, map[string][]int) &#123; unspentOutputs := make(map[string][]int) accumulated := 0 db := u.Blockchain.db err := db.View(func(tx *bolt.Tx) error &#123; b := tx.Bucket([]byte(utxoBucket)) c := b.Cursor() for k, v := c.First(); k != nil; k, v = c.Next() &#123; txID := hex.EncodeToString(k) outs := DeserializeOutputs(v) for outIdx, out := range outs.Outputs &#123; if out.IsLockedWithKey(pubkeyHash) &amp;&amp; accumulated &lt; amount &#123; accumulated += out.Value unspentOutputs[txID] = append(unspentOutputs[txID], outIdx) &#125; &#125; &#125; return nil &#125;) if err != nil &#123; log.Panic(err) &#125; return accumulated, unspentOutputs&#125; FindSpendableOutputs查找区块链上pubkeyHash账户的utxo集合，直到这些集合的累计未花费金额达到需求的amount为止。 blockchain_go中使用嵌入式key-value数据库boltdb存储区块链和未花费输出等信息，其中utxoBucket是所有用户未花费输出的bucket，其中的key表示交易ID，value是这个交易中未被引用的所有output的集合。所以通过遍历查询本次交易需要花费的output，得到Transaction的txID和这个output在Transaction中的输出数组中的下标组合unspentOutputs。 另外一个重点是utxobucket中保存的未花费输出结合是关于所有账户的，要查询特定账户需要对账户进行判断，因为TXOutput中有pubkeyhash字段，用来表示该输出属于哪个用户，此处采用out.IsLockedWithKey(pubkeyHash)判断特定output是否是属于给定用户。 5.新建Transaction需要发起一笔交易的时候，需要新建一个Transaction，通过交易发起人的钱包得到足够的未花费输出，构建出交易的输入和输出，完成签名即可，blockchain_go中的实现如下： 1234567891011121314151617181920212223242526272829303132333435363738// NewUTXOTransaction creates a new transactionfunc NewUTXOTransaction(wallet *Wallet, to string, amount int, UTXOSet *UTXOSet) *Transaction &#123; var inputs []TXInput var outputs []TXOutput pubKeyHash := HashPubKey(wallet.PublicKey) acc, validOutputs := UTXOSet.FindSpendableOutputs(pubKeyHash, amount) if acc &lt; amount &#123; log.Panic(&quot;ERROR: Not enough funds&quot;) &#125; // Build a list of inputs for txid, outs := range validOutputs &#123; txID, err := hex.DecodeString(txid) if err != nil &#123; log.Panic(err) &#125; for _, out := range outs &#123; input := TXInput&#123;txID, out, nil, wallet.PublicKey&#125; inputs = append(inputs, input) &#125; &#125; // Build a list of outputs from := fmt.Sprintf(&quot;%s&quot;, wallet.GetAddress()) outputs = append(outputs, *NewTXOutput(amount, to)) if acc &gt; amount &#123; outputs = append(outputs, *NewTXOutput(acc-amount, from)) // a change &#125; tx := Transaction&#123;nil, inputs, outputs&#125; tx.ID = tx.Hash() UTXOSet.Blockchain.SignTransaction(&amp;tx, wallet.PrivateKey) return &amp;tx&#125; 函数参数： wallet : 用户钱包参数，存储用户的公私钥，用于交易的签名和验证。to : 交易转账的目的地址（转账给谁）。amount : 需要交易的代币额度。UTXOSet : uxto集合，查询用户的未花费输出。查询需要的未花费输出： 1acc, validOutputs := UTXOSet.FindSpendableOutputs(pubKeyHash, amount) 因为用户的总金额是通过若干未花费输出累计起来的，而每个output所携带金额不一而足，所以每次转账可能需要消耗多个不同的output，而且还可能涉及找零问题。以上查询返回了一批未花费输出列表validOutputs和他们总共的金额acc. 找出来的未花费输出列表就是本次交易的输入，并将输出结果构造output指向目的用户，并检查是否有找零，将找零返还。 如果交易顺利完成，转账发起人的“未花费输出”被消耗掉变成了花费状态，而转账接收人to得到了一笔新的“未花费输出”，之后他自己需要转账时，查询自己的未花费输出，即可使用这笔钱。 最后需要对交易进行签名，表示交易确实是由发起人本人发起（私钥签名），而不是被第三人冒充。 6.Transaction的签名和验证6.1 签名交易的有效性需要首先建立在发起人签名的基础上，防止他人冒充转账或者发起人抵赖，blockchain_go中交易签名实现如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// SignTransaction signs inputs of a Transactionfunc (bc *Blockchain) SignTransaction(tx *Transaction, privKey ecdsa.PrivateKey) &#123; prevTXs := make(map[string]Transaction) for _, vin := range tx.Vin &#123; prevTX, err := bc.FindTransaction(vin.Txid) if err != nil &#123; log.Panic(err) &#125; prevTXs[hex.EncodeToString(prevTX.ID)] = prevTX &#125; tx.Sign(privKey, prevTXs)&#125;// Sign signs each input of a Transactionfunc (tx *Transaction) Sign(privKey ecdsa.PrivateKey, prevTXs map[string]Transaction) &#123; if tx.IsCoinbase() &#123; return &#125; for _, vin := range tx.Vin &#123; if prevTXs[hex.EncodeToString(vin.Txid)].ID == nil &#123; log.Panic(&quot;ERROR: Previous transaction is not correct&quot;) &#125; &#125; txCopy := tx.TrimmedCopy() for inID, vin := range txCopy.Vin &#123; prevTx := prevTXs[hex.EncodeToString(vin.Txid)] txCopy.Vin[inID].Signature = nil txCopy.Vin[inID].PubKey = prevTx.Vout[vin.Vout].PubKeyHash dataToSign := fmt.Sprintf(&quot;%x\n&quot;, txCopy) r, s, err := ecdsa.Sign(rand.Reader, &amp;privKey, []byte(dataToSign)) if err != nil &#123; log.Panic(err) &#125; signature := append(r.Bytes(), s.Bytes()...) tx.Vin[inID].Signature = signature txCopy.Vin[inID].PubKey = nil &#125;&#125; 交易输入的签名信息是放在TXInput中的signature字段，其中需要包括用户的pubkey，用于之后的验证。需要对每一个输入做签名。 6.2 验证交易签名是发生在交易产生时，交易完成后，Transaction会把交易广播给邻居。节点在进行挖矿时，会整理一段时间的所有交易信息，将这些信息打包进入新的区块，成功加入区块链以后，这个交易就得到了最终的确认。但是在挖矿节点打包交易前，需要对交易的有效性做验证，以防虚假数据，验证实现如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879// MineBlock mines a new block with the provided transactionsfunc (bc *Blockchain) MineBlock(transactions []*Transaction) *Block &#123; var lastHash []byte var lastHeight int for _, tx := range transactions &#123; // TODO: ignore transaction if it&apos;s not valid if bc.VerifyTransaction(tx) != true &#123; log.Panic(&quot;ERROR: Invalid transaction&quot;) &#125; &#125; ... ... ... return block&#125;// VerifyTransaction verifies transaction input signaturesfunc (bc *Blockchain) VerifyTransaction(tx *Transaction) bool &#123; if tx.IsCoinbase() &#123; return true &#125; prevTXs := make(map[string]Transaction) for _, vin := range tx.Vin &#123; prevTX, err := bc.FindTransaction(vin.Txid) if err != nil &#123; log.Panic(err) &#125; prevTXs[hex.EncodeToString(prevTX.ID)] = prevTX &#125; return tx.Verify(prevTXs)&#125;// Verify verifies signatures of Transaction inputsfunc (tx *Transaction) Verify(prevTXs map[string]Transaction) bool &#123; if tx.IsCoinbase() &#123; return true &#125; for _, vin := range tx.Vin &#123; if prevTXs[hex.EncodeToString(vin.Txid)].ID == nil &#123; log.Panic(&quot;ERROR: Previous transaction is not correct&quot;) &#125; &#125; txCopy := tx.TrimmedCopy() curve := elliptic.P256() for inID, vin := range tx.Vin &#123; prevTx := prevTXs[hex.EncodeToString(vin.Txid)] txCopy.Vin[inID].Signature = nil txCopy.Vin[inID].PubKey = prevTx.Vout[vin.Vout].PubKeyHash r := big.Int&#123;&#125; s := big.Int&#123;&#125; sigLen := len(vin.Signature) r.SetBytes(vin.Signature[:(sigLen / 2)]) s.SetBytes(vin.Signature[(sigLen / 2):]) x := big.Int&#123;&#125; y := big.Int&#123;&#125; keyLen := len(vin.PubKey) x.SetBytes(vin.PubKey[:(keyLen / 2)]) y.SetBytes(vin.PubKey[(keyLen / 2):]) dataToVerify := fmt.Sprintf(&quot;%x\n&quot;, txCopy) rawPubKey := ecdsa.PublicKey&#123;Curve: curve, X: &amp;x, Y: &amp;y&#125; if ecdsa.Verify(&amp;rawPubKey, []byte(dataToVerify), &amp;r, &amp;s) == false &#123; return false &#125; txCopy.Vin[inID].PubKey = nil &#125; return true&#125; 可以看到验证的时候也是每个交易的每个TXInput都单独进行验证，和签名过程很相似，需要构造相同的交易数据txCopy，验证时会用到签名设置的TxInput.PubKeyHash生成一个原始的PublicKey，将前面的signature分拆后通过ecdsa.Verify进行验证。 7.总结 以上简单分析和整理了blockchain_go中的交易和UTXO机制的实现过程，加深了区块链中的挖矿，交易和转账的基础技术原理的理解。 转载自http://www.bugclosed.com/post/38]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go的几种死锁情况分析]]></title>
    <url>%2F2018%2F09%2F09%2Fgo%E7%9A%84%E5%87%A0%E7%A7%8D%E6%AD%BB%E9%94%81%E6%83%85%E5%86%B5%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[在go语言中用channel通信稍不注意就会发生死锁情况，下面我们来看一下几种常见的死锁情况 第一种：同一个goroutine中，使用同一个 channel 读写。123456package mainfunc main()&#123; ch:=make(chan int) //这就是在main程里面发生的死锁情况 ch&lt;-6 // 这里会发生一直阻塞的情况，执行不到下面一句 &lt;-ch&#125; 这是最简单的死锁情况 看运行结果 第二种：2个 以上的go程中， 使用同一个 channel 通信。 读写channel 先于 go程创建。123456789package mainfunc main()&#123; ch:=make(chan int) ch&lt;-666 //这里一直阻塞，运行不到下面 go func ()&#123; &lt;-ch //这里虽然创建了子go程用来读出数据，但是上面会一直阻塞运行不到下面 &#125;()&#125; 这里如果想不成为死锁那匿名函数go程就要放到ch&lt;-666这条语句前面 看运行结果 还是同样的错误，死锁。 第三种：2个以上的go程中，使用多个 channel 通信。 A go 程 获取channel 1 的同时，尝试使用channel 2， 同一时刻，B go 程 获取channel 2 的同时，尝试使用channel 112345678910111213141516171819package mainfunc main() &#123; ch1 := make(chan int) ch2 := make(chan int) go func() &#123; //匿名子go程 for &#123; select &#123; //这里互相等对方造成死锁 case &lt;-ch1: //这里ch1有数据读出才会执行下一句 ch2 &lt;- 777 &#125; &#125; &#125;() for &#123; //主go程 select &#123; case &lt;-ch2 : //这里ch2有数据读出才会执行下一句 ch1 &lt;- 999 &#125; &#125;&#125; 第三种是互相等对方造成死锁 第四种： 在go语言中， channel 和 读写锁、互斥锁 尽量避免交叉混用。——“隐形死锁”。如果必须使用。推荐借助“条件变量”12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package mainimport ( &quot;runtime&quot; &quot;math/rand&quot; &quot;time&quot; &quot;fmt&quot; &quot;sync&quot;)// 使用读写锁var rwMutex2 sync.RWMutexfunc readGo2(idx int, in &lt;-chan int) &#123; // 读go程 for &#123; time.Sleep(time.Millisecond * 500) // 放大实验现象// 一个go程可以读 无限 次。 rwMutex2.RLock() // 读模式加 读写锁 num := &lt;-in // 从 公共的 channel 中获取数据 fmt.Printf(&quot;%dth 读 go程，读到：%d\n&quot;, idx, num) rwMutex2.RUnlock() // 解锁 读写锁 &#125;&#125;func writeGo2(idx int, out chan&lt;- int) &#123; for &#123; // 一个go程可以写 无限 次。 // 生产一个随机数 num := rand.Intn(500) rwMutex2.Lock() // 写模式加 读写锁 out &lt;- num fmt.Printf(&quot;-----%dth 写 go程，写入：%d\n&quot;, idx, num) rwMutex2.Unlock() // 解锁 读写锁 //time.Sleep(time.Millisecond * 200) // 放大实验现象 &#125;&#125;func main() &#123; // 播种随机数种子。 rand.Seed(time.Now().UnixNano()) // 创建 模拟公共区的 channel ch := make(chan int, 5) for i:=0; i&lt;5; i++ &#123; // 同时创建 N 个 读go程 go readGo2(i+1, ch) &#125; for i:=0; i&lt;5; i++ &#123; // 同时创建 N 个 写go程 go writeGo2(i+1, ch) &#125; for &#123; // 防止 主 go 程 退出 runtime.GC() &#125;&#125; 这是一种隐形的死锁，我们来看一下结果 注意这几种的死锁情况]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git详解-git实战]]></title>
    <url>%2F2018%2F09%2F09%2Fgit%E8%AF%A6%E8%A7%A3-git%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[本章介绍开始使用 Git 前的相关知识。我们会先了解一些版本控制工具的历史背景，然后试着让 Git 在你的系统上跑起来，直到最后配置好，可以正常开始开发工作。读完本章，你就会明白为什么 Git 会如此流行，为什么你应该立即开始使用它。 关于版本控制什么是版本控制？我真的需要吗？版本控制是一种记录若干文件内容变化，以便将来查阅特定版本修订情况的系统。在本书所展示的例子中，我们仅对保存着软件源代码的文本文件作版本控制管理，但实际上，你可以对任何类型的文件进行版本控制。 如果你是位图形或网页设计师，可能会需要保存某一幅图片或页面布局文件的所有修订版本（这或许是你非常渴望拥有的功能）。采用版本控制系统 （VCS）是个明智的选择。有了它你就可以将某个文件回溯到之前的状态，甚至将整个项目都回退到过去某个时间点的状态。你可以比较文件的变化细节，查出最 后是谁修改了哪个地方，从而导致出现怪异问题，又是谁在何时报告了某个功能缺陷等等。使用版本控制系统通常还意味着，就算你乱来一气把整个项目中的文件改 的改删的删，你也照样可以轻松恢复到原先的样子。但额外增加的工作量却微乎其微。 本地版本控制系统 许多人习惯用复制整个项目目录的方式来保存不同的版本，或许还会改名加上备份时间以示区别。这么做唯一的好处就是简单。不过坏处也不少：有时候会混淆所在的工作目录，一旦弄错文件丢了数据就没法撤销恢复。 为了解决这个问题，人们很久以前就开发了许多种本地版本控制系统，大多都是采用某种简单的数据库来记录文件的历次更新差异 其中最流行的一种叫做 rcs，现今许多计算机系统上都还看得到它的踪影。甚至在流行的 Mac OS X 系统上安装了开发者工具包之后，也可以使用 rcs 命令。它的工作原理基本上就是保存并管理文件补丁（patch）。文件补丁是一种特定格式的文本文件，记录着对应文件修订前后的内容变化。所以，根据每次 修订后的补丁，rcs 可以通过不断打补丁，计算出各个版本的文件内容。 集中化的版本控制系统 接下来人们又遇到一个问题，如何让在不同系统上的开发者协同工作？于是，集中化的版本控制系统（ Centralized Version Control Systems，简称 CVCS ）应运而生。这类系统，诸如 CVS，Subversion 以及 Perforce 等，都有一个单一的集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。多年以来，这 已成为版本控制系统的标准做法。 这种做法带来了许多好处，特别是相较于老式的本地 VCS 来说。现在，每个人都可以在一定程度上看到项目中的其他人正在做些什么。而管理员也可以轻松掌控每个开发者的权限，并且管理一个 CVCS 要远比在各个客户端上维护本地数据库来得轻松容易。 事分两面，有好有坏。这么做最显而易见的缺点是中央服务器的单点故障。如果宕机一小时，那么在这一小时内，谁都无法提交更新，也就无法协同工作。要 是中央服务器的磁盘发生故障，碰巧没做备份，或者备份不够及时，就还是会有丢失数据的风险。最坏的情况是彻底丢失整个项目的所有历史更改记录，而被客户端 提取出来的某些快照数据除外，但这样的话依然是个问题，你不能保证所有的数据都已经有人事先完整提取出来过。本地版本控制系统也存在类似问题，只要整个项 目的历史记录被保存在单一位置，就有丢失所有历史更新记录的风险。 分布式版本控制系统 于是分布式版本控制系统（ Distributed Version Control System，简称 DVCS ）面世了。在这类系统中，像 Git，Mercurial，Bazaar 以及 Darcs 等，客户端并不只提取最新版本的文件快照，而是把原始的代码仓库完整地镜像下来。这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜 像出来的本地仓库恢复。因为每一次的提取操作，实际上都是一次对代码仓库的完整备份。 进一步，许多这类系统都可以指定和若干不同的远端代码仓库进行交互。籍此，你就可以在同一个项目中，分别和不同工作小组的人相互协作。你可以根据需要设定不同的协作流程，比如层次模型式的工作流，而这在以前的集中式系统中是无法实现的。 Git 基础 那么，简单地说，Git 究竟是怎样的一个系统呢？请注意，接下来的内容非常重要，若是理解了 Git 的思想和基本工作原理，用起来就会知其所以然，游刃有余。在开始学习 Git 的时候，请不要尝试把各种概念和其他版本控制系统（诸如 Subversion 和 Perforce 等）相比拟，否则容易混淆每个操作的实际意义。Git 在保存和处理各种信息的时候，虽然操作起来的命令形式非常相近，但它与其他版本控制系统的做法颇为不同。理解这些差异将有助于你准确地使用 Git 提供的各种工具。 直接记录快照，而非差异比较 Git 和其他版本控制系统的主要差别在于，Git 只关心文件数据的整体是否发生变化，而大多数其他系统则只关心文件内容的具体差异。这类系统 （CVS，Subversion，Perforce，Bazaar 等等）每次记录有哪些文件作了更新，以及都更新了哪些行的什么内容 Git 并不保存这些前后变化的差异数据。实际上，Git 更像是把变化的文件作快照后，记录在一个微型的文件系统中。每次提交更新时，它会纵览一遍所有文件的指纹信息并对文件作一快照，然后保存一个指向这次快照 的索引。为提高性能，若文件没有变化，Git 不会再次保存，而只对上次保存的快照作一链接。Git 的工作方式如图所示 这是 Git 同其他系统的重要区别。它完全颠覆了传统版本控制的套路，并对各个环节的实现方式作了新的设计。Git 更像是个小型的文件系统，但它同时还提供了许多以此为基础的超强工具，而不只是一个简单的 VCS。稍后在第三章讨论 Git 分支管理的时候，我们会再看看这样的设计究竟会带来哪些好处。 近乎所有操作都是本地执行 在 Git 中的绝大多数操作都只需要访问本地文件和资源，不用连网。但如果用 CVCS 的话，差不多所有操作都需要连接网络。因为 Git 在本地磁盘上就保存着所有当前项目的历史更新，所以处理起来速度飞快。 举个例子，如果要浏览项目的历史更新摘要，Git 不用跑到外面的服务器上去取数据回来，而直接从本地数据库读取后展示给你看。所以任何时候你都可以马上翻阅，无需等待。如果想要看当前版本的文件和一个月 前的版本之间有何差异，Git 会取出一个月前的快照和当前文件作一次差异运算，而不用请求远程服务器来做这件事，或是把老版本的文件拉到本地来作比较。 用 CVCS 的话，没有网络或者断开 VPN 你就无法做任何事情。但用 Git 的话，就算你在飞机或者火车上，都可以非常愉快地频繁提交更新，等到了有网络的时候再上传到远程仓库。同样，在回家的路上，不用连接 VPN 你也可以继续工作。换作其他版本控制系统，这么做几乎不可能，抑或非常麻烦。比如 Perforce，如果不连到服务器，几乎什么都做不了（译注：默认无法发出命令p4 edit file 开始编辑文件，因为 Perforce 需要联网通知系统声明该文件正在被谁修订。但实际上手工修改文件权限可以绕过这个限制，只是完成后还是无法提交更新。）；如果是 Subversion 或 CVS，虽然可以编辑文件，但无法提交更新，因为数据库在网络上。看上去好像这些都不是什么大问题，但实际体验过之后，你就会惊喜地发现，这其实是会带来很大不同的。 时刻保持数据完整性 在保存到 Git 之前，所有数据都要进行内容的校验和（checksum）计算，并将此结果作为数据的唯一标识和索引。换句话说，不可能在你修改了文件或目录之后，Git 一无所知。这项特性作为 Git 的设计哲学，建在整体架构的最底层。所以如果文件在传输时变得不完整，或者磁盘损坏导致文件数据缺失，Git 都能立即察觉。 Git 使用 SHA-1 算法计算数据的校验和，通过对文件的内容或目录的结构计算出一个 SHA-1 哈希值，作为指纹字符串。该字串由 40 个十六进制字符（0-9 及 a-f）组成，看起来就像是： 124b9da6552252987aa493b52f8696cd6d3b00373 Git 的工作完全依赖于这类指纹字串，所以你会经常看到这样的哈希值。实际上，所有保存在 Git 数据库中的东西都是用此哈希值来作索引的，而不是靠文件名。 多数操作仅添加数据 常用的 Git 操作大多仅仅是把数据添加到数据库。因为任何一种不可逆的操作，比如删除数据，都会使回退或重现历史版本变得困难重重。在别的 VCS 中，若还未提交更新，就有可能丢失或者混淆一些修改的内容，但在 Git 里，一旦提交快照之后就完全不用担心丢失数据，特别是养成定期推送到其他仓库的习惯的话。 这种高可靠性令我们的开发工作安心不少，尽管去做各种试验性的尝试好了，再怎样也不会弄丢数据。至于 Git 内部究竟是如何保存和恢复数据的，我们会在第九章讨论 Git 内部原理时再作详述。 文件的三种状态 好，现在请注意，接下来要讲的概念非常重要。对于任何一个文件，在 Git 内都只有三种状态：已提交（committed），已修改（modified）和已暂存（staged）。已提交表示该文件已经被安全地保存在本地数据库 中了；已修改表示修改了某个文件，但还没有提交保存；已暂存表示把已修改的文件放在下次提交时要保存的清单中。 由此我们看到 Git 管理项目时，文件流转的三个工作区域：Git 的工作目录，暂存区域，以及本地仓库。 每个项目都有一个 Git 目录（译注：如果 git clone 出来的话，就是其中 .git 的目录；如果git clone --bare 的话，新建的目录本身就是 Git 目录。），它是 Git 用来保存元数据和对象数据库的地方。该目录非常重要，每次克隆镜像仓库的时候，实际拷贝的就是这个目录里面的数据。 从项目中取出某个版本的所有文件和目录，用以开始后续工作的叫做工作目录。这些文件实际上都是从 Git 目录中的压缩对象数据库中提取出来的，接下来就可以在工作目录中对这些文件进行编辑。 所谓的暂存区域只不过是个简单的文件，一般都放在 Git 目录中。有时候人们会把这个文件叫做索引文件，不过标准说法还是叫暂存区域。 Git 工作流程如下 1. 在工作目录中修改某些文件。 2. 对修改后的文件进行快照，然后保存到暂存区域。 3. 提交更新，将保存在暂存区域的文件快照永久转储到 Git 目录中。 所以，我们可以从文件所处的位置来判断状态：如果是 Git 目录中保存着的特定版本文件，就属于已提交状态；如果作了修改并已放入暂存区域，就属于已暂存状态；如果自上次取出后，作了修 改但还没有放到暂存区域，就 是已修改状态。到第二章的时候，我们会进一步了解其中细节，并学会如何根据文件状态实施后续操作，以及怎样跳过暂存直接提交。 在正式使用前，我们还需要弄清楚Git的三种重要模式，分别是已提交、已修改、已暂存 已提交(committed):表示数据文件已经顺利提交到Git数据库中。 已修改(modified):表示数据文件已经被修改，但未被保存到Git数据库中。 已暂存(staged):表示数据文件已经被修改，并会在下次提交时提交到Git数据库中。 实战 先创建一个工程的目录mkdir test_project cd test_project git init 初始化git工作目录（git init –bare功能相同） git init的结果（这个隐藏的git目录里面的内容和–bare创建的相同） git init –bare 路径 4.touch readme 创建一个文件 5.git status 查看状态 第一次查看，这个文件还没有添加到暂存区的 6.git add readme 将readme文件添加到暂存区 既然有添加，那就有删除（此处说的是暂存区的操作，不会删除文件） git rm –cached readme 7.git status 再次查看暂存区的状态 将readme添加到暂存区后的状态 8.git commit -m “first commit” 提交到自己的中央仓库（init就是创建自己的中央仓库） 9.git log查看日志（相当与svn的提交日志） 到目前为止自己本地仓库就提交结束了 之后就是提交到远程仓库了 10.git remote –v 查看本地存储的远程仓库信息，如果是clone出来的工程这个结果如下 origin 表示的是远程仓库的别名（默认为origin，也可以自己起，fetch更新类似于update，push推数据相当于commit） 如果不是clone的工程，就不会有任何结果，要自己添加，命令如下： git remote add test ssh://root@10.0.0.5/usr/GitData/DingDang/.git 11.做完这步然后就是远程推数据了（必须保证本地仓库里面有提交，注意是本地仓库而不是暂存区） git push test 到此自己创建的文件就推到了远程的git仓库了 12.还有一个功能比较重要，本地仓库的版本回退 git reset –hard HEAD^ #还原历史提交版本上一次 git reset –hard 版本号 #就是上图黄色的部分，仅需要前7位即可 如果回退过头了，log是看不到未来的版本号的，想看可以用git reflog查看 转载链接Git详解之-Git实战]]></content>
      <categories>
        <category>git使用及讲解</category>
      </categories>
      <tags>
        <tag>git工具使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git分支详解]]></title>
    <url>%2F2018%2F09%2F08%2Fgit%E5%88%86%E6%94%AF%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[几乎每一种版本控制系统都以某种形式支持分支。使用分支意味着你可以从开发主线上分离开来，然后在不影响主线的同时继续工作。在很多版本控制系统中，这是个昂贵的过程，常常需要创建一个源代码目录的完整副本，对大型项目来说会花费很长时间。 有人把 Git 的分支模型称为“必杀技特性”，而正是因为它，将 Git 从版本控制系统家族里区分出来。Git 有何特别之处呢？Git 的分支可谓是难以置信的轻量级，它的新建操作几乎可以在瞬间完成，并且在不同分支间切换起来也差不多一样快。和许多其他版本控制系统不同，Git 鼓励在工作流程中频繁使用分支与合并，哪怕一天之内进行许多次都没有关系。理解分支的概念并熟练运用后，你才会意识到为什么 Git 是一个如此强大而独特的工具，并从此真正改变你的开发方式。 何谓分支 为了理解 Git 分支的实现方式，我们需要回顾一下 Git 是如何储存数据的。或许你还记得第一章的内容，Git 保存的不是文件差异或者变化量，而只是一系列文件快照。 在 Git 中提交时，会保存一个提交（commit）对象，该对象包含一个指向暂存内容快照的指针，包含本次提交的作者等相关附属信息，包含零个或多个指向该提交对 象的父对象指针：首次提交是没有直接祖先的，普通提交有一个祖先，由两个或多个分支合并产生的提交则有多个祖先。 为直观起见，我们假设在工作目录中有三个文件，准备将它们暂存后提交。暂存操作会对每一个文件计算校验和（即第一章中提到的 SHA-1 哈希字串），然后把当前版本的文件快照保存到 Git 仓库中（Git 使用 blob 类型的对象存储这些快照），并将校验和加入暂存区域： 121 $ git add README test.rb LICENSE2 $ git commit -m &apos;initial commit of my project&apos; 当使用 git commit 新建一个提交对象前，Git 会先计算每一个子目录（本例中就是项目根目录）的校验和，然后在 Git 仓库中将这些目录保存为树（tree）对象。之后 Git 创建的提交对象，除了包含相关提交信息以外，还包含着指向这个树对象（项目根目录）的指针，如此它就可以在将来需要的时候，重现此次快照的内容了。 现在，Git 仓库中有五个对象：三个表示文件快照内容的 blob 对象；一个记录着目录树内容及其中各个文件对应 blob 对象索引的 tree 对象；以及一个包含指向 tree 对象（根目录）的索引和其他提交信息元数据的 commit 对象。概念上来说，仓库中的各个对象保存的数据和相互关系看起来如图所示： 作些修改后再次提交，那么这次的提交对象会包含一个指向上次提交对象的指针（译注：即下图中的 parent 对象）。两次提交后，仓库历史会变成下图： 现在来谈分支。Git 中的分支，其实本质上仅仅是个指向 commit 对象的可变指针。Git 会使用 master 作为分支的默认名字。在若干次提交后，你其实已经有了一个指向最后一次提交对象的 master 分支，它在每次提交的时候都会自动向前移动。 那么，Git 又是如何创建一个新的分支的呢？答案很简单，创建一个新的分支指针。比如新建一个 testing 分支，可以使用 git branch 命令： 1 $ git branch testing 这会在当前 commit 对象上新建一个分支指针 那么，Git 是如何知道你当前在哪个分支上工作的呢？其实答案也很简单，它保存着一个名为 HEAD 的特别指针。请注意它和你熟知的许多其他版本控制系统（比如 Subversion 或 CVS）里的 HEAD 概念大不相同。在 Git 中，它是一个指向你正在工作中的本地分支的指针（译注：将 HEAD 想象为当前分支的别名。）。运行git branch 命令，仅仅是建立了一个新的分支，但不会自动切换到这个分支中去，所以在这个例子中，我们依然还在 master 分支里工作 要切换到其他分支，可以执行 git checkout 命令。我们现在转换到新建的 testing 分支： 1 $ git checkout testing 这样 HEAD 就指向了 testing 分支 这样的实现方式会给我们带来什么好处呢？好吧，现在不妨再提交一次： 12 $ vim test.rb $ git commit -a -m &apos;made a change&apos; 非常有趣，现在 testing 分支向前移动了一格，而 master 分支仍然指向原先 git checkout 时所在的 commit 对象。现在我们回到 master 分支看看： 1 $ git checkout master 这条命令做了两件事。它把 HEAD 指针移回到 master 分支，并把工作目录中的文件换成了 master 分支所指向的快照内容。也就是说，现在开始所做的改动，将始于本项目中一个较老的版本。它的主要作用是将 testing 分支里作出的修改暂时取消，这样你就可以向另一个方向进行开发。 我们作些修改后再次提交： 12 $ vim test.rb $ git commit -a -m &apos;made other changes&apos; 现在我们的项目提交历史产生了分叉，因为刚才我们创建了一个分支，转换到其中进行了一些工作，然后又回到原来的主分支进行了另外一些工作。这些改变分别孤立在不同的分支里：我们可以 在不同分支里反复切换，并在时机成熟时把它们合并到一起。而所有这些工作，仅仅需要branch 和 checkout 这两条命令就可以完成 由于 Git 中的分支实际上仅是一个包含所指对象校验和（40 个字符长度 SHA-1 字串）的文件，所以创建和销毁一个分支就变得非常廉价。说白了，新建一个分支就是向一个文件写入 41 个字节（外加一个换行符）那么简单，当然也就很快了。 这和大多数版本控制系统形成了鲜明对比，它们管理分支大多采取备份所有项目文件到特定目录的方式，所以根据项目文件数量和大小不同，可能花费的时间 也会有相当大的差别，快则几秒，慢则数分钟。而 Git 的实现与项目复杂度无关，它永远可以在几毫秒的时间内完成分支的创建和切换。同时，因为每次提交时都记录了祖先信息（译注：即parent 对象），将来要合并分支时，寻找恰当的合并基础（译注：即共同祖先）的工作其实已经自然而然地摆在那里了，所以实现起来非常容易。Git 鼓励开发者频繁使用分支，正是因为有着这些特性作保障。 接下来看看，我们为什么应该频繁使用分支。 分支的新建与合并 现在让我们来看一个简单的分支与合并的例子，实际工作中大体也会用到这样的工作流程： 1. 开发某个网站。 2. 为实现某个新的需求，创建一个分支。 3. 在这个分支上开展工作。 假设此时，你突然接到一个电话说有个很严重的问题需要紧急修补，那么可以按照下面的方式处理： 1. 返回到原先已经发布到生产服务器上的分支。 2. 为这次紧急修补建立一个新分支，并在其中修复问题。 3. 通过测试后，回到生产服务器所在的分支，将修补分支合并进来，然后再推送到生产服务器上。 4. 切换到之前实现新需求的分支，继续工作。 分支的新建与切换 首先，我们假设你正在项目中愉快地工作，并且已经提交了几次更新 现在，你决定要修补问题追踪系统上的 #53 问题。顺带说明下，Git 并不同任何特定的问题追踪系统打交道。这里为了说明要解决的问题，才把新建的分支取名为 iss53。要新建并切换到该分支，运行git checkout 并加上 -b参数： 12 $ git checkout -b iss53 Switched to a new branch &quot;iss53&quot; 这相当于执行下面这两条命令： 12 $ git branch iss53 $ git checkout iss53 接着你开始尝试修复问题，在提交了若干次更新后，iss53 分支的指针也会随着向前推进，因为它就是当前分支（换句话说，当前的 HEAD 指针正指向 iss53）： 12$ vim index.html$ git commit -a -m &apos;added a new footer [issue 53]&apos; 现在你就接到了那个网站问题的紧急电话，需要马上修补。有了 Git ，我们就不需要同时发布这个补丁和 iss53里作出的修改，也不需要在创建和发布该补丁到服务器之前花费大力气来复原这些修改。唯一需要的仅仅是切换回master 分支。 不过在此之前，留心你的暂存区或者工作目录里，那些还没有提交的修改，它会和你即将检出的分支产生冲突从而阻止 Git 为你切换分支。切换分支的时候最好保持一个清洁的工作区域。稍后会介绍几个绕过这种问题的办法（分别叫做 stashing 和 commit amending）。目前已经提交了所有的修改，所以接下来可以正常转换到master 分支： 12 $ git checkout master Switched to branch &quot;master&quot; 此时工作目录中的内容和你在解决问题 #53 之前一模一样，你可以集中精力进行紧急修补。这一点值得牢记：Git 会把工作目录的内容恢复为检出某分支时它所指向的那个提交对象的快照。它会自动添加、删除和修改文件以确保目录的内容和你当时提交时完全一样。 接下来，你得进行紧急修补。我们创建一个紧急修补分支 hotfix 来开展工作，直到搞定： 123456$ git checkout -b &apos;hotfix&apos;Switched to a new branch &quot;hotfix&quot;$ vim index.html$ git commit -a -m &apos;fixed the broken email address&apos;[hotfix]: created 3a0874c: &quot;fixed the broken email address&quot; 1 files changed, 0 insertions(+), 1 deletions(-) 有必要作些测试，确保修补是成功的，然后回到 master 分支并把它合并进来，然后发布到生产服务器。用 git merge 命令来进行合并： 123456$ git checkout master$ git merge hotfixUpdating f42c576..3a0874cFast forward README | 1 - 1 files changed, 0 insertions(+), 1 deletions(-) 请注意，合并时出现了“Fast forward”的提示。由于当前 master 分支所在的提交对象是要并入的 hotfix 分支的直接上游，Git 只需把master 分支指针直接右移。换句话说，如果顺着一个分支走下去可以到达另一个分支的话，那么 Git 在合并两者时，只会简单地把指针右移，因为这种单线的历史分支不存在任何需要解决的分歧，所以这种合并过程可以称为快进（Fast forward）。 现在最新的修改已经在当前 master 分支所指向的提交对象中了，可以部署到生产服务器上去了 在那个超级重要的修补发布以后，你想要回到被打扰之前的工作。由于当前 hotfix 分支和 master 都指向相同的提交对象，所以hotfix 已经完成了历史使命，可以删掉了。使用 git branch 的 -d 选项执行删除操作： 12 $ git branch -d hotfix Deleted branch hotfix (3a0874c). 现在回到之前未完成的 #53 问题修复分支上继续工作（图 3-15）： 123456$ git checkout iss53Switched to branch &quot;iss53&quot;$ vim index.html$ git commit -a -m &apos;finished the new footer [issue 53]&apos;[iss53]: created ad82d7a: &quot;finished the new footer [issue 53]&quot; 1 files changed, 1 insertions(+), 0 deletions(-) 不用担心之前 hotfix 分支的修改内容尚未包含到 iss53 中来。如果确实需要纳入此次修补，可以用git merge master 把 master 分支合并到 iss53；或者等 iss53 完成之后，再将iss53分支中的更新并入 master。 分支的合并 在问题 #53 相关的工作完成之后，可以合并回 master 分支。实际操作同前面合并 hotfix 分支差不多，只需回到master 分支，运行 git merge 命令指定要合并进来的分支： 12345$ git checkout master$ git merge iss53Merge made by recursive. README | 1 + 1 files changed, 1 insertions(+), 0 deletions(-) 请注意，这次合并操作的底层实现，并不同于之前 hotfix 的并入方式。因为这次你的开发历史是从更早的地方开始分叉的。由于当前 master 分支所指向的提交对象（C4）并不是 iss53 分支的直接祖先，Git 不得不进行一些额外处理。就此例而言，Git 会用两个分支的末端（C4 和 C5）以及它们的共同祖先（C2）进行一次简单的三方合并计算。图 3-16 用红框标出了 Git 用于合并的三个提交对象： 这次，Git 没有简单地把分支指针右移，而是对三方合并后的结果重新做一个新的快照，并自动创建一个指向它的提交对象（C6）。这个提交对象比较特殊，它有两个祖先（C4 和 C5）。 值得一提的是 Git 可以自己裁决哪个共同祖先才是最佳合并基础；这和 CVS 或 Subversion（1.5 以后的版本）不同，它们需要开发者手工指定合并基础。所以此特性让 Git 的合并操作比其他系统都要简单不少。 既然之前的工作成果已经合并到 master 了，那么 iss53 也就没用了。你可以就此删除它，并在问题追踪系统里关闭该问题。 1 $ git branch -d iss53 Checkout 历史版本从某个历史版本创建新的分支在 Git 中从当前分支创建并检出新分支的命令是 1git checkout -b name-of-new-branch 这个命令实际上是 1git checkout -b name-of-new-branch current-branch 的简写形式。也就是说，当我们不指定 checkout 起点时，Git 默认从当前活动分支开始创建新的分支。 Git 的每个提交都有一个 SHA1 散列值（Hash 值）作为 ID。我们可以在 checkout 命令中使用这些 ID 作为起点。比如： 1git checkout -b name-of-new-branch 169d2dc 这样，Git 的活动分支会切换到 name-of-new-branch 这个分支上，而它的内容与 169d2dc 这个分支一致。 注意：SHA1 的散列值有 40 个字母，相当长。所以 Git 允许我们在不引起歧义的情况下，使用散列值的前几位作为缩写。 提示：你也可以用 git branch name-of-new-branch 169d2dc 来创建一个历史分支，而不切换到该分支。 将某个历史版本 checkout 到工作区首先说明，这样做会产生一个分离的 HEAD 指针，所以个人不推荐这么做。 如果我们工作在 master 分支上，希望 checkout 到 dev 分支上，我们会这么做： 1git checkout dev 这里 dev 实际上是一个指针的别名，其本质也是一个 SHA1 散列值。所以，我们很自然地可以用 1git checkout &lt;sha1-of-a-commit&gt; 将某个历史版本 checkout 到工作区。 将某个文件的历史版本 checkout 到工作区大多数时候，我们可能只需要对某一个文件做细小的修补，因此只 checkout 该文件就行了，并不需要操作整个 commit 或分支。 上一节我们介绍了如何将某个历史版本完整地 checkout 到工作区。实际上，我们只需要在上一节的命令之后加上需要 checkout 的文件即可。 1git checkout &lt;sha1-of-a-commit&gt; &lt;/path/to/your/file&gt; 当然，有时候你需要将某个文件的历史版本 checkout 出来，并以一个新的名字保存。这时候可以这么做： 遇到冲突时的分支合并 有时候合并操作并不会如此顺利。如果在不同的分支中都修改了同一个文件的同一部分，Git 就无法干净地把两者合到一起（译注：逻辑上说，这种问题只能由人来裁决。）。如果你在解决问题 #53 的过程中修改了hotfix 中修改的部分，将得到类似下面的结果： 1234$ git merge iss53Auto-merging index.htmlCONFLICT (content): Merge conflict in index.htmlAutomatic merge failed; fix conflicts and then commit the result. Git 作了合并，但没有提交，它会停下来等你解决冲突。要看看哪些文件在合并时发生冲突，可以用 git status 查阅： 123456789101112[master*]$ git statusindex.html: needs merge# On branch master# Changed but not updated:# (use &quot;git add ...&quot; to update what will be committed)# (use &quot;git checkout -- ...&quot; to discard changes in working directory)## unmerged: index.html# 任何包含未解决冲突的文件都会以未合并（unmerged）的状态列出。Git 会在有冲突的文件里加入标准的冲突解决标记，可以通过它们来手工定位并解决这些冲突。可以看到此文件包含类似下面这样的部分： 12345&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD:index.htmlcontact : email.support@github.com=======please contact us at support@github.com&gt;&gt;&gt;&gt;&gt;&gt;&gt; iss53:index.html 可以看到 ======= 隔开的上半部分，是 HEAD（即 master 分支，在运行merge 命令时所切换到的分支）中的内容，下半部分是在 iss53 分支中的内容。解决冲突的办法无非是二者选其一或者由你亲自整合到一起。比如你可以通过把这段内容替换为下面这样来解决： please contact us at email.support@github.com 这个解决方案各采纳了两个分支中的一部分内容，而且我还删除了 &lt;&lt;&lt;&lt;&lt;&lt;&lt;，======= 和 &gt;&gt;&gt;&gt;&gt;&gt;&gt; 这些行。在解决了所有文件里的所有冲突后，运行 git add 将把它们标记为已解决状态（译注：实际上就是来一次快照保存到暂存区域。）。因为一旦暂存，就表示冲突已经解决。如果你想用一个有图形界面的工具来解决这些问题，不妨运行git mergetool，它会调用一个可视化的合并工具并引导你解决所有冲突： 12345678$ git mergetoolmerge tool candidates: kdiff3 tkdiff xxdiff meld gvimdiff opendiff emerge vimdiffMerging the files: index.htmlNormal merge conflict for &apos;index.html&apos;: &#123;local&#125;: modified &#123;remote&#125;: modifiedHit return to start merge resolution tool (opendiff): 如果不想用默认的合并工具（Git 为我默认选择了 opendiff，因为我在 Mac 上运行了该命令），你可以在上方”merge tool candidates”里找到可用的合并工具列表，输入你想用的工具名。我们将在第七章讨论怎样改变环境中的默认值。 退出合并工具以后，Git 会询问你合并是否成功。如果回答是，它会为你把相关文件暂存起来，以表明状态为已解决。 再运行一次 git status 来确认所有冲突都已解决： 1234567891011$ git status# On branch master# Changes to be committed:# (use &quot;git reset HEAD ...&quot; to unstage)## modified: index.html# 如果觉得满意了，并且确认所有冲突都已解决，也就是进入了暂存区，就可以用 git commit 来完成这次合并提交。提交的记录差不多是这样： 123456789Merge branch &apos;iss53&apos;Conflicts: index.html## It looks like you may be committing a MERGE.# If this is not correct, please remove the file# .git/MERGE_HEAD# and try again. 如果想给将来看这次合并的人一些方便，可以修改该信息，提供更多合并细节。比如你都作了哪些改动，以及这么做的原因。有时候裁决冲突的理由并不直接或明显，有必要略加注解。 分支的管理 到目前为止，你已经学会了如何创建、合并和删除分支。除此之外，我们还需要学习如何管理分支，在日后的常规工作中会经常用到下面介绍的管理命令。 git branch 命令不仅仅能创建和删除分支，如果不加任何参数，它会给出当前所有分支的清单： 1234$ git branch iss53* master testing 注意看 master 分支前的 * 字符：它表示当前所在的分支。也就是说，如果现在提交更新，master 分支将随着开发进度前移。若要查看各个分支最后一个提交对象的信息，运行git branch -v： 1234$ git branch -v iss53 93b412c fix javascript issue* master 7a98805 Merge branch &apos;iss53&apos; testing 782fd34 add scott to the author list in the readmes 要从该清单中筛选出你已经（或尚未）与当前分支合并的分支，可以用 --merge 和 --no-merged选项（Git 1.5.6 以上版本）。比如用git branch --merge 查看哪些分支已被并入当前分支（译注：也就是说哪些分支是当前分支的直接上游。）： 123$ git branch --merged iss53* master 之前我们已经合并了 iss53，所以在这里会看到它。一般来说，列表中没有 * 的分支通常都可以用 git branch -d 来删掉。原因很简单，既然已经把它们所包含的工作整合到了其他分支，删掉也不会损失什么。 另外可以用 git branch --no-merged 查看尚未合并的工作： 12$ git branch --no-merged testing 它会显示还未合并进来的分支。由于这些分支中还包含着尚未合并进来的工作成果，所以简单地用 git branch -d删除该分支会提示错误，因为那样做会丢失数据： 123$ git branch -d testingerror: The branch &apos;testing&apos; is not an ancestor of your current HEAD.If you are sure you want to delete it, run &apos;git branch -D testing&apos;. 不过，如果你确实想要删除该分支上的改动，可以用大写的删除选项 -D 强制执行，就像上面提示信息中给出的那样。 利用分支进行开发的工作流程 现在我们已经学会了新建分支和合并分支，可以（或应该）用它来做点什么呢？在本节，我们会介绍一些利用分支进行开发的工作流程。而正是由于分支管理的便捷，才衍生出了这类典型的工作模式，你可以根据项目的实际情况选择一种用用看。 长期分支 由于 Git 使用简单的三方合并，所以就算在较长一段时间内，反复多次把某个分支合并到另一分支，也不是什么难事。也就是说，你可以同时拥有多个开放的分支，每个分支用于完成特定的任务，随着开发的推进，你可以随时把某个特性分支的成果并到其他分支中。 许多使用 Git 的开发者都喜欢用这种方式来开展工作，比如仅在 master 分支中保留完全稳定的代码，即已经发布或即将发布的代码。与此同时，他们还有一个名为develop 或 next 的平行分支，专门用于后续的开发，或仅用于稳定性测试 — 当然并不是说一定要绝对稳定，不过一旦进入某种稳定状态，便可以把它合并到master 里。这样，在确保这些已完成的特性分支（短期分支，比如之前的 iss53 分支）能够通过所有测试，并且不会引入更多错误之后，就可以并到主干分支中，等待下一次的发布。 本质上我们刚才谈论的，是随着提交对象不断右移的指针。稳定分支的指针总是在提交历史中落后一大截，而前沿分支总是比较靠前 或者把它们想象成工作流水线，或许更好理解一些，经过测试的提交对象集合被遴选到更稳定的流水线 你可以用这招维护不同层次的稳定性。某些大项目还会有个 proposed（建议）或 pu（proposed updates，建议更新）分支，它包含着那些可能还没有成熟到进入next 或 master 的内容。这么做的目的是拥有不同层次的稳定性：当这些分支进入到更稳定的水平时，再把它们合并到更高层分支中去。再次说明下，使用多个长期分支的做法并非必需，不过一般来说，对于特大型项目或特复杂的项目，这么做确实更容易管理。 特性分支 在任何规模的项目中都可以使用特性（Topic）分支。一个特性分支是指一个短期的，用来实现单一特性或与其相关工作的分支。可能你在以前的版本控 制系统里从未做过类似这样的事情，因为通常创建与合并分支消耗太大。然而在 Git 中，一天之内建立、使用、合并再删除多个分支是常见的事。 我们在上节的例子里已经见过这种用法了。我们创建了 iss53 和 hotfix 这两个特性分支，在提交了若干更新后，把它们合并到主干分支，然后删除。该技术允许你迅速且完全的进行语境切换 — 因为你的工作分散在不同的流水线里，每个分支里的改变都和它的目标特性相关，浏览代码之类的事情因而变得更简单了。你可以把作出的改变保持在特性分支中几 分钟，几天甚至几个月，等它们成熟以后再合并，而不用在乎它们建立的顺序或者进度。 现在我们来看一个实际的例子。请看下图，由下往上，起先我们在 master 工作到 C1，然后开始一个新分支 iss91 尝试修复 91 号缺陷，提交到 C6 的时候，又冒出一个解决该问题的新办法，于是从之前 C4 的地方又分出一个分支iss91v2，干到 C8 的时候，又回到主干 master 中提交了 C9 和 C10，再回到 iss91v2 继续工作，提交 C11，接着，又冒出个不太确定的想法，从 master 的最新提交 C10 处开了个新的分支dumbidea 做些试验。 现在，假定两件事情：我们最终决定使用第二个解决方案，即 iss91v2 中的办法；另外，我们把 dumbidea 分支拿给同事们看了以后，发现它竟然是个天才之作。所以接下来，我们准备抛弃原来的iss91 分支（实际上会丢弃 C5 和 C6），直接在主干中并入另外两个分支。最终的提交历史将变成下图 请务必牢记这些分支全部都是本地分支，这一点很重要。当你在使用分支及合并的时候，一切都是在你自己的 Git 仓库中进行的 — 完全不涉及与服务器的交互。 远程分支 远程分支（remote branch）是对远程仓库中的分支的索引。它们是一些无法移动的本地分支；只有在 Git 进行网络交互时才会更新。远程分支就像是书签，提醒着你上次连接远程仓库时上面各分支的位置。 我们用 (远程仓库名)/(分支名) 这样的形式表示远程分支。比如我们想看看上次同 origin 仓库通讯时master的样子，就应该查看 origin/master 分支。如果你和同伴一起修复某个问题，但他们先推送了一个iss53 分支到远程仓库，虽然你可能也有一个本地的 iss53 分支，但指向服务器上最新更新的却应该是 origin/iss53 分支。 可能有点乱，我们不妨举例说明。假设你们团队有个地址为 git.ourcompany.com 的 Git 服务器。如果你从这里克隆，Git 会自动为你将此远程仓库命名为origin，并下载其中所有的数据，建立一个指向它的 master 分支的指针，在本地命名为 origin/master，但你无法在本地更改其数据。接着，Git 建立一个属于你自己的本地master 分支，始于 origin 上 master 分支相同的位置，你可以就此开始工作 一次 Git 克隆会建立你自己的本地分支 master 和远程分支 origin/master，它们都指向 origin/master 分支的最后一次提交。 如果你在本地 master 分支做了些改动，与此同时，其他人向 git.ourcompany.com 推送了他们的更新，那么服务器上的master 分支就会向前推进，而于此同时，你在本地的提交历史正朝向不同方向发展。不过只要你不和服务器通讯，你的 origin/master 指针仍然保持原位不会移动 在本地工作的同时有人向远程仓库推送内容会让提交历史开始分流。 可以运行 git fetch origin 来同步远程服务器上的数据到本地。该命令首先找到 origin 是哪个服务器（本例为git.ourcompany.com），从上面获取你尚未拥有的数据，更新你本地的数据库，然后把 origin/master 的指针移到它最新的位置上 为了演示拥有多个远程分支（在不同的远程服务器上）的项目是如何工作的，我们假设你还有另一个仅供你的敏捷开发小组使用的内部服务器 git.team1.ourcompany.com。可以用第二章中提到的git remote add 命令把它加为当前项目的远程分支之一。我们把它命名为 teamone，以便代替原始的 Git 地址 现在你可以用 git fetch teamone 来获取小组服务器上你还没有的数据了。由于当前该服务器上的内容是你 origin 服务器上的子集，Git 不会下载任何数据，而只是简单地创建一个名为teamone/master 的分支，指向 teamone 服务器上 master 分支所在的提交对象31b8e 推送本地分支 要想和其他人分享某个本地分支，你需要把它推送到一个你拥有写权限的远程仓库。你的本地分支不会被自动同步到你引入的远程服务器上，除非你明确执行推送操作。换句话说，对于无意分享的分支，你尽管保留为私人分支好了，而只推送那些协同工作要用到的特性分支。 如果你有个叫 serverfix 的分支需要和他人一起开发，可以运行 git push (远程仓库名) (分支名)： 1234567$ git push origin serverfixCounting objects: 20, done.Compressing objects: 100% (14/14), done.Writing objects: 100% (15/15), 1.74 KiB, done.Total 15 (delta 5), reused 0 (delta 0)To git@github.com:schacon/simplegit.git * [new branch] serverfix -&gt; serverfix 这其实有点像条捷径。Git 自动把 serverfix 分支名扩展为 refs/heads/serverfix:refs/heads/serverfix，意为“取出我在本地的 serverfix 分支，推送到远程仓库的 serverfix 分支中去”。我们将在第九章进一步介绍refs/heads/ 部分的细节，不过一般使用的时候都可以省略它。也可以运行 git push origin serverfix:serferfix 来实现相同的效果，它的意思是“上传我本地的 serverfix 分支到远程仓库中去，仍旧称它为 serverfix 分支”。通过此语法，你可以把本地分支推送到某个命名不同的远程分支：若想把远程分支叫作awesomebranch，可以用 git push origin serverfix:awesomebranch 来推送数据。 接下来，当你的协作者再次从服务器上获取数据时，他们将得到一个新的远程分支 origin/serverfix： 1234567$ git fetch originremote: Counting objects: 20, done.remote: Compressing objects: 100% (14/14), done.remote: Total 15 (delta 5), reused 0 (delta 0)Unpacking objects: 100% (15/15), done.From git@github.com:schacon/simplegit * [new branch] serverfix -&gt; origin/serverfix 值得注意的是，在 fetch 操作下载好新的远程分支之后，你仍然无法在本地编辑该远程仓库中的分支。换句话说，在本例中，你不会有一个新的serverfix 分支，有的只是一个你无法移动的 origin/serverfix 指针。 如果要把该内容合并到当前分支，可以运行 git merge origin/serverfix。如果想要一份自己的 serverfix来开发，可以在远程分支的基础上分化出一个新的分支来： 123$ git checkout -b serverfix origin/serverfixBranch serverfix set up to track remote branch refs/remotes/origin/serverfix.Switched to a new branch &quot;serverfix&quot; 这会切换到新建的 serverfix 本地分支，其内容同远程分支 origin/serverfix 一致，这样你就可以在里面继续开发了。 跟踪远程分支 从远程分支 checkout 出来的本地分支，称为跟踪分支(tracking branch)。跟踪分支是一种和远程分支有直接联系的本地分支。在跟踪分支里输入git push，Git 会自行推断应该向哪个服务器的哪个分支推送数据。反过来，在这些分支里运行 git pull 会获取所有远程索引，并把它们的数据都合并到本地分支中来。 在克隆仓库时，Git 通常会自动创建一个名为 master 的分支来跟踪 origin/master。这正是git push 和 git pull 一开始就能正常工作的原因。当然，你可以随心所欲地设定为其它跟踪分支，比如origin 上除了 master之外的其它分支。刚才我们已经看到了这样的一个例子：git checkout -b [分支名] [远程名]/[分支名]。如果你有 1.6.2 以上版本的 Git，还可以用--track 选项简化： 123$ git checkout --track origin/serverfixBranch serverfix set up to track remote branch refs/remotes/origin/serverfix.Switched to a new branch &quot;serverfix&quot; 要为本地分支设定不同于远程分支的名字，只需在前个版本的命令里换个名字： 123$ git checkout -b sf origin/serverfixBranch sf set up to track remote branch refs/remotes/origin/serverfix.Switched to a new branch &quot;sf&quot; 现在你的本地分支 sf 会自动向 origin/serverfix 推送和抓取数据了。 删除远程分支如果不再需要某个远程分支了，比如搞定了某个特性并把它合并进了远程的 master 分支（或任何其他存放稳定代码的地方），可以用这个非常无厘头的语法来删除它：git push [远程名] :[分支名]。如果想在服务器上删除serverfix 分支，运行下面的命令： 123$ git push origin :serverfixTo git@github.com:schacon/simplegit.git - [deleted] serverfix 咚！服务器上的分支没了。你最好特别留心这一页，因为你一定会用到那个命令，而且你很可能会忘掉它的语法。有种方便记忆这条命令的方法：记住我们不久前见过的 git push [远程名] [本地分支]:[远程分支] 语法，如果省略 [本地分支]，那就等于是在说“在这里提取空白然后把它变成[远程分支]”。 分支的衍合 把一个分支整合到另一个分支的办法有两种：merge 和 rebase（译注：rebase 的翻译暂定为“衍合”，大家知道就可以了。）。在本章我们会学习什么是衍合，如何使用衍合，为什么衍合操作如此富有魅力，以及我们应该在什么情况下使用衍合。 基本的衍合操作开发进程分叉到两个不同分支，又各自提交了更新。 之前介绍过，最容易的整合分支的方法是 merge 命令，它会把两个分支最新的快照（C3 和 C4）以及二者最新的共同祖先（C2）进行三方合并，合并的结果是产生一个新的提交对象（C5）。 其实，还有另外一个选择：你可以把在 C3 里产生的变化补丁在 C4 的基础上重新打一遍。在 Git 里，这种操作叫做衍合（rebase）。有了 rebase 命令，就可以把在一个分支里提交的改变移到另一个分支里重放一遍。 在上面这个例子中，运行： 1234$ git checkout experiment$ git rebase masterFirst, rewinding head to replay your work on top of it...Applying: added staged command 它的原理是回到两个分支最近的共同祖先，根据当前分支（也就是要进行衍合的分支 experiment）后续的历次提交对象（这里只有一个 C3），生成一系列文件补丁，然后以基底分支（也就是主干分支master）最后一个提交对象（C4）为新的出发点，逐个应用之前准备好的补丁文件，最后会生成一个新的合并提交对象（C3’），从而改写 experiment 的提交历史，使它成为 master 分支的直接下游，如图 现在回到 master 分支，进行一次快进合并 现在的 C3’ 对应的快照，其实和普通的三方合并，即上个例子中的 C5 对应的快照内容一模一样了。虽然最后整合得到的结果没有任何区别，但衍合能产生一个更为整洁的提交历史。如果视察一个衍合过的分支的历史记录，看起来会更 清楚：仿佛所有修改都是在一根线上先后进行的，尽管实际上它们原本是同时并行发生的。 一般我们使用衍合的目的，是想要得到一个能在远程分支上干净应用的补丁 — 比如某些项目你不是维护者，但想帮点忙的话，最好用衍合：先在自己的一个分支里进行开发，当准备向主项目提交补丁的时候，根据最新的origin/master 进行一次衍合操作然后再提交，这样维护者就不需要做任何整合工作（译注：实际上是把解决分支补丁同最新主干代码之间冲突的责任，化转为由提交补丁的人来解决。），只需根据你提供的仓库地址作一次快进合并，或者直接采纳你提交的补丁。 请注意，合并结果中最后一次提交所指向的快照，无论是通过衍合，还是三方合并，都会得到相同的快照内容，只不过提交历史不同罢了。衍合是按照每行的修改次序重演一遍修改，而合并是把最终结果合在一起。 有趣的衍合 衍合也可以放到其他分支进行，并不一定非得根据分化之前的分支。以图 3-31 的历史为例，我们为了给服务器端代码添加一些功能而创建了特性分支 server，然后提交 C3 和 C4。然后又从 C3 的地方再增加一个client 分支来对客户端代码进行一些相应修改，所以提交了 C8 和 C9。最后，又回到 server 分支提交了 C10。 假设在接下来的一次软件发布中，我们决定先把客户端的修改并到主线中，而暂缓并入服务端软件的修改（因为还需要进一步测试）。这个时候，我们就可以把基于 server 分支而非 master 分支的改变（即 C8 和 C9），跳过 server 直接放到master 分支中重演一遍，但这需要用 git rebase 的 --onto 选项指定新的基底分支master： 1$ git rebase --onto master server client 这好比在说：“取出 client 分支，找出 client 分支和 server 分支的共同祖先之后的变化，然后把它们在master 上重演一遍”。是不是有点复杂？不过它的结果如图 3-32 所示，非常酷（译注：虽然 client 里的 C8, C9 在 C3 之后，但这仅表明时间上的先后，而非在 C3 修改的基础上进一步改动，因为server 和 client 这两个分支对应的代码应该是两套文件，虽然这么说不是很严格，但应理解为在 C3 时间点之后，对另外的文件所做的 C8，C9 修改，放到主干重演。）： 现在可以快进 master 分支了： 12$ git checkout master$ git merge client 现在我们决定把 server 分支的变化也包含进来。我们可以直接把 server 分支衍合到 master，而不用手工切换到 server 分支后再执行衍合操作 — git rebase [主分支] [特性分支]命令会先取出特性分支server，然后在主分支 master 上重演： 1$ git rebase master server 于是，server 的进度应用到 master 的基础上， 然后就可以快进主干分支 master 了： 12$ git checkout master$ git merge server 现在 client 和 server 分支的变化都已经集成到主干分支来了，可以删掉它们了。最终我们的提交历史会变成图 3-35 的样子： 12$ git branch -d client$ git branch -d server 衍合的风险 呃，奇妙的衍合也并非完美无缺，要用它得遵守一条准则： 一旦分支中的提交对象发布到公共仓库，就千万不要对该分支进行衍合操作。 如果你遵循这条金科玉律，就不会出差错。否则，人民群众会仇恨你，你的朋友和家人也会嘲笑你，唾弃你。 在进行衍合的时候，实际上抛弃了一些现存的提交对象而创造了一些类似但不同的新的提交对象。如果你把原来分支中的提交对象发布出去，并且其他人更新下载后在其基础上开展工作，而稍后你又用git rebase 抛弃这些提交对象，把新的重演后的提交对象发布出去的话，你的合作者就不得不重新合并他们的工作，这样当你再次从他们那里获取内容时，提交历史就会变得一团糟。 下面我们用一个实际例子来说明为什么公开的衍合会带来问题。假设你从一个中央服务器克隆然后在它的基础上搞了一些开发，提交历史 现在，某人在 C1 的基础上做了些改变，并合并他自己的分支得到结果 C6，推送到中央服务器。当你抓取并合并这些数据到你本地的开发分支中后，会得到合并结果 C7，历史提交会变成 接下来，那个推送 C6 上来的人决定用衍合取代之前的合并操作；继而又用 git push --force 覆盖了服务器上的历史，得到 C4’。而之后当你再从服务器上下载最新提交后，会得到： 下载更新后需要合并，但此时衍合产生的提交对象 C4’ 的 SHA-1 校验值和之前 C4 完全不同，所以 Git 会把它们当作新的提交对象处理，而实际上此刻你的提交历史 C7 中早已经包含了 C4 的修改内容，于是合并操作会把 C7 和 C4’ 合并为 C8 C8 这一步的合并是迟早会发生的，因为只有这样你才能和其他协作者提交的内容保持同步。而在 C8 之后，你的提交历史里就会同时包含 C4 和 C4’，两者有着不同的 SHA-1 校验值，如果用git log 查看历史，会看到两个提交拥有相同的作者日期与说明，令人费解。而更糟的是，当你把这样的历史推送到服务器后，会再次把这些衍合后的提交引入到中央服务 器，进一步困扰其他人（译注：这个例子中，出问题的责任方是那个发布了 C6 后又用衍合发布 C4’ 的人，其他人会因此反馈双重历史到共享主干，从而混淆大家的视听。）。 如果把衍合当成一种在推送之前清理提交历史的手段，而且仅仅衍合那些尚未公开的提交对象，就没问题。如果衍合那些已经公开的提交对象，并且已经有人基于这些提交对象开展了后续开发工作的话，就会出现叫人沮丧的麻烦。 小结 读到这里，你应该已经学会了如何创建分支并切换到新分支，在不同分支间转换，合并本地分支，把分支推送到共享服务器上，使用共享分支与他人协作，以及在分享之前进行衍合。 转载链接Git详解三Git分支]]></content>
      <categories>
        <category>git使用及讲解</category>
      </categories>
      <tags>
        <tag>git工具使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络协议-最全的网络协议图]]></title>
    <url>%2F2018%2F09%2F08%2F%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE-%E6%9C%80%E5%85%A8%E7%9A%84%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[转载自http://www.52im.net 图片较大，建议单击放大或者下载后查看]]></content>
      <categories>
        <category>网络编程协议</category>
      </categories>
      <tags>
        <tag>通信协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go语言并发讲解，虚拟内存讲解]]></title>
    <url>%2F2018%2F09%2F08%2Fgo%E8%AF%AD%E8%A8%80%E5%B9%B6%E5%8F%91%E8%AE%B2%E8%A7%A3%EF%BC%8C%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E8%AE%B2%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[并行和并发今天我们来讲一下在计算机编程中并行和并发的意思并行(parallel)：指在同一时刻，有多条指令在多个处理器上同时执行。并发(concurrency)：指在同一时刻只能有一条指令执行，但多个进程指令被快速的轮换执行，使得在宏观上具有多个进程同时执行的效果，但在微观上并不是同时执行的，只是把时间分成若干段，通过cpu时间片轮转使多个进程快速交替的执行。如果字面意思上不好理解的话那么我们可以来看一个例子就可以很轻松的理解出大师曾以咖啡机的例子来解释并行和并发的区别。 并行是两个队列同时使用两台咖啡机 （真正的多任务）并发是两个队列交替使用一台咖啡机 （ 假 的多任务）在计算机上想要实现并行该怎么办，那么我们就要像图中一样增加硬件设备，那么计算机就要增加cpu，但是计算机的cpu是有限的，所以我们就要设计并发来实现在计算机cpu不变的情况下增加运算能力，这就是并发的字面意思。 常见并发编程技术进程并发程序和进程程序，是指编译好的二进制文件，在磁盘上，不占用系统资源(内存、打开的文件、设备、锁….)进程，是一个抽象的概念，与操作系统原理联系紧密。进程是活跃的程序，占用系统资源。在内存中执行。(程序运行起来，产生一个进程)程序 → 剧本(纸) 进程 → 戏 (舞台、演员、灯光、道具…)同一个剧本可以在多个舞台同时上演。同样，同一个程序也可以加载为不同的进程(彼此之间互不影响)如：同时开两个终端。各自都有一个bash但彼此ID不同。在windows系统下，通过查看“任务管理器”，可以查看相应的进程。包括我们在基础班写的“飞机大战”等程序，运行起来后也可以在“任务管理器”中查看到。运行起来的程序就是一个进程。 进程状态进程基本的状态有5种。分别为初始态，就绪态，运行态，挂起态与终止态。其中初始态为进程准备阶段，常与就绪态结合来看。 进程并发在使用进程 实现并发时会出现什么问题呢？1：系统开销比较大，占用资源比较多，开启进程数量比较少。2：在unix/linux系统下，还会产生“孤儿进程”和“僵尸进程”。在操作系统运行过程中，可以产生很多的进程。在unix/linux系统中，正常情况下，子进程是通过父进程fork创建的，子进程再创建新的进程。并且父进程永远无法预测子进程到底什么时候结束。 当一个进程完成它的工作终止之后，它的父进程需要调用系统调用取得子进程的终止状态。孤儿进程孤儿进程: 父进程先于子进程结束，则子进程成为孤儿进程，子进程的父进程成为init进程，称为init进程领养孤儿进程。僵尸进程僵尸进程: 进程终止，父进程尚未回收，子进程残留资源（PCB）存放于内核中，变成僵尸（Zombie）进程。Windows下的进程和Linux下的进程是不一样的，它比较懒惰，从来不执行任何东西，只是为线程提供执行环境。然后由线程负责执行包含在进程的地址空间中的代码。当创建一个进程的时候，操作系统会自动创建这个进程的第一个线程，成为主线程。 线程并发什么是线程LWP：light weight process 轻量级的进程，本质仍是进程 (Linux下)进程：独立地址空间，拥有PCB 线程：有独立的PCB，但没有独立的地址空间(共享) 区别：在于是否共享地址空间。独居(进程)；合租(线程)。线程：最小的执行单位进程：最小分配资源单位，可看成是只有一个线程的进程。Windows系统下，可以直接忽略进程的概念，只谈线程。因为线程是最小的执行单位，是被系统独立调度和分派的基本单位。而进程只是给线程提供执行环境。线程同步同步即协同步调，按预定的先后次序运行。线程同步，指一个线程发出某一功能调用时，在没有得到结果之前，该调用不返回。同时其它线程为保证数据一致性，不能调用该功能。举例1： 银行存款 5000。柜台，折：取3000；提款机，卡：取 3000。剩余：2000举例2： 内存中100字节，线程T1欲填入全1， 线程T2欲填入全0。但如果T1执行了50个字节失去cpu，T2执行，会将T1写过的内容覆盖。当T1再次获得cpu继续 从失去cpu的位置向后写入1，当执行结束，内存中的100字节，既不是全1，也不是全0。产生的现象叫做“与时间有关的错误”(time related)。为了避免这种数据混乱，线程需要同步。“同步”的目的，是为了避免数据混乱，解决与时间有关的错误。实际上，不仅线程间需要同步，进程间、信号间等等都需要同步机制。因此，所有“多个控制流，共同操作一个共享资源”的情况，都需要同步。 协程并发协程：coroutine。也叫轻量级线程。与传统的系统级线程和进程相比，协程最大的优势在于“轻量级”。可以轻松创建上万个而不会导致系统资源衰竭。而线程和进程通常很难超过1万个。这也是协程别称“轻量级线程”的原因。一个线程中可以有任意多个协程，但某一时刻只能有一个协程在运行，多个协程分享该线程分配到的计算机资源。多数语言在语法层面并不直接支持协程，而是通过库的方式支持，但用库的方式支持的功能也并不完整，比如仅仅提供协程的创建、销毁与切换等能力。如果在这样的轻量级线程中调用一个同步 IO 操作，比如网络通信、本地文件读写，都会阻塞其他的并发执行轻量级线程，从而无法真正达到轻量级线程本身期望达到的目标。在协程中，调用一个任务就像调用一个函数一样，消耗的系统资源最少！但能达到进程、线程并发相同的效果。在一次并发任务中，进程、线程、协程均可以实现。从系统资源消耗的角度出发来看，进程相当多，线程次之，协程最少。 Go并发Go 在语言级别支持协程，叫goroutine。Go 语言标准库提供的所有系统调用操作（包括所有同步IO操作），都会出让CPU给其他goroutine。这让轻量级线程的切换管理不依赖于系统的线程和进程，也不需要依赖于CPU的核心数量。有人把Go比作21世纪的C语言。第一是因为Go语言设计简单，第二，21世纪最重要的就是并行程序设计，而Go从语言层面就支持并行。同时，并发程序的内存管理有时候是非常复杂的，而Go语言提供了自动垃圾回收机制。Go语言为并发编程而内置的上层API基于顺序通信进程模型CSP(communicating sequential processes)。这就意味着显式锁都是可以避免的，因为Go通过相对安全的通道发送和接受数据以实现同步，这大大地简化了并发程序的编写。Go语言中的并发程序主要使用两种手段来实现。goroutine和channel。 Goroutine什么是Goroutinegoroutine是Go语言并行设计的核心，有人称之为go程。 goroutine说到底其实就是协程，它比线程更小，十几个goroutine可能体现在底层就是五六个线程，Go语言内部帮你实现了这些goroutine之间的内存共享。执行goroutine只需极少的栈内存(大概是4~5KB)，当然会根据相应的数据伸缩。也正因为如此，可同时运行成千上万个并发任务。goroutine比thread更易用、更高效、更轻便。一般情况下，一个普通计算机跑几十个线程就有点负载过大了，但是同样的机器却可以轻松地让成百上千个goroutine进行资源竞争。 Goroutine的创建只需在函数调⽤语句前添加 go 关键字，就可创建并发执⾏单元。开发⼈员无需了解任何执⾏细节，调度器会自动将其安排到合适的系统线程上执行。在并发编程中，我们通常想将一个过程切分成几块，然后让每个goroutine各自负责一块工作，当一个程序启动时，主函数在一个单独的goroutine中运行，我们叫它main goroutine。新的goroutine会用go语句来创建。而go语言的并发设计，让我们很轻松就可以达成这一目的。示例代码： 1234567891011121314151617181920package mainimport ( &quot;fmt&quot; &quot;time&quot;)func main()&#123; go func()&#123; //加一个go关键字就创建了一个子go程 for i:=0;i&lt;5;i++&#123; fmt.Println(&quot;这里是一个匿名函数go进程&quot;) //打印一次 time.Sleep(time.Second) //延时一秒 &#125; &#125;() for i:=0;i&lt;5;i++&#123; fmt.Println(&quot;main&quot;) time.Sleep(time.Second) //延时一秒 &#125;&#125; 打印结果如下 这就是go的并发，只需要在前面加一个关键字就可以了，在这里子go程和主go程相互夺取cpu，cpu用来分配时间轮转片，轮到谁就执行，在这个程序里面因为都延时了1秒，所以运行一次就会给对方运行 一个程序要想运行，必须要有“进行地址（虚拟地址）空间”， 所有系统都一样 下面我们来看一下虚拟内存的讲解，我们先来看一张图 我们图中是拿了一个512M的物理内存来举例子，电脑假设为32位的电脑，在32位电脑上，我们运行一个程序图中左边都有两个go程序，都会分配4G的空间给程序，分别为代码区，只读数据区，数据区，未初始化数据区，堆区，栈区，内核区，但是我们的物理内存只有512M，那么两个程序占8G，那么够用吗，其实是够的，这是为什么？，这是因为在程序运行是CPU中的MMU会映射一个虚拟内存出来，你可以当作想象吧，我们平时看到的内存条什么的物理内存都会有一个物理地址，但是我们是不能直接拿物理地址来操作程序什么的，我们平时操作的地址都是CPU映射出来的虚拟内存的虚拟地址，图中我们在虚拟内存中声明了两个变量，这样CPU通过MMU来在真实的物理内存上面映射两个物理地址，类似于将物理地址和虚拟地址链接一下，然后等程序运行完毕，直接将虚拟内存释放，这样的话运行程序在32位电脑上虚拟出来的4G都是虚拟出来的。 如果想要了解更多的硬件问题推荐一下《计算机组成原理》这本书可以去了解一下]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>内存讲解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络协议]]></title>
    <url>%2F2018%2F09%2F08%2F%E7%BD%91%E7%BB%9C%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[协议从应用的角度出发，协议可理解为“规则”，是数据传输和数据的解释的规则。假设，A、B双方欲传输文件。规定： 第一次，传输文件名，接收方接收到文件名，应答OK给传输方； 第二次，发送文件的尺寸，接收方接收到该数据再次应答一个OK； 第三次，传输文件内容。同样，接收方接收数据完成后应答OK表示文件内容接收成功。由此，无论A、B之间传递何种文件，都是通过三次数据传输来完成。A、B之间形成了一个最简单的数据传输规则。双方都按此规则发送、接收数据。A、B之间达成的这个相互遵守的规则即为协议。 这种仅在A、B之间被遵守的协议称之为原始协议。当此协议被更多的人采用，不断的增加、改进、维护、完善。最终形成一个稳定的、完整的文件传输协议，被广泛应用于各种文件传输过程中。该协议就成为一个标准协议。最早的ftp协议就是由此衍生而来。 典型协议 传输层 常见协议有TCP/UDP协议。 应用层 常见的协议有HTTP协议，FTP协议。 网络层 常见协议有IP协议、ICMP协议、IGMP协议。 网络接口层 常见协议有ARP协议、RARP协议。 TCP传输控制协议（Transmission Control Protocol）是一种面向连接的、可靠的、基于字节流的传输层通信协议。 UDP用户数据报协议（User Datagram Protocol）是OSI参考模型中一种无连接的传输层协议，提供面向事务的简单不可靠信息传送服务。 HTTP超文本传输协议（Hyper Text Transfer Protocol）是互联网上应用最为广泛的一种网络协议。 FTP文件传输协议（File Transfer Protocol） IP协议是因特网互联协议（Internet Protocol） ICMP协议是Internet控制报文协议（Internet Control Message Protocol）它是TCP/IP协议族的一个子协议，用于在IP主机、路由器之间传递控制消息。 IGMP协议是 Internet 组管理协议（Internet Group Management Protocol），是因特网协议家族中的一个组播协议。该协议运行在主机和组播路由器之间。 ARP协议是正向地址解析协议（Address Resolution Protocol），通过已知的IP，寻找对应主机的MAC地址。 RARP是反向地址转换协议，通过MAC地址确定IP地址。 分层模型网络分层架构为了减少协议设计的复杂性，大多数网络模型均采用分层的方式来组织。每一层都有自己的功能，就像建筑物一样，每一层都靠下一层支持。每一层利用下一层提供的服务来为上一层提供服务，本层服务的实现细节对上层屏蔽。 越下面的层，越靠近硬件；越上面的层，越靠近用户。至于每一层叫什么名字，对应编程而言不重要，但面试的时候，面试官可能会问每一层的名字。业内普遍的分层方式有两种。OSI七层模型 和TCP/IP四层模型。可以通过背诵两个口诀来快速记忆：OSI七层模型：物、数、网、传、会、表、应TCP/IP四层模型：链、网、传、应 物理层：主要定义物理设备标准，如网线的接口类型、光纤的接口类型、各种传输介质的传输速率等。它的主要作用是传输比特流（就是由1、0转化为电流强弱来进行传输，到达目的地后再转化为1、0，也就是我们常说的数模转换与模数转换）。这一层的数据叫做比特。 数据链路层：定义了如何让格式化数据以帧为单位进行传输，以及如何让控制对物理介质的访问。这一层通常还提供错误检测和纠正，以确保数据的可靠传输。如：串口通信中使用到的115200、8、N、1 网络层：在位于不同地理位置的网络中的两个主机系统之间提供连接和路径选择。Internet的发展使得从世界各站点访问信息的用户数大大增加，而网络层正是管理这种连接的层。 传输层：定义了一些传输数据的协议和端口号（WWW端口80等），如：TCP（传输控制协议，传输效率低，可靠性强，用于传输可靠性要求高，数据量大的数据），UDP（用户数据报协议，与TCP特性恰恰相反，用于传输可靠性要求不高，数据量小的数据，如QQ聊天数据就是通过这种方式传输的）。 主要是将从下层接收的数据进行分段和传输，到达目的地址后再进行重组。常常把这一层数据叫做段。 会话层：通过传输层(端口号：传输端口与接收端口)建立数据传输的通路。主要在你的系统之间发起会话或者接受会话请求（设备之间需要互相认识可以是IP也可以是MAC或者是主机名）。 表示层：可确保一个系统的应用层所发送的信息可以被另一个系统的应用层读取。例如，PC程序与另一台计算机进行通信，其中一台计算机使用扩展二一十进制交换码(EBCDIC)，而另一台则使用美国信息交换标准码（ASCII）来表示相同的字符。如有必要，表示层会通过使用一种通格式来实现多种数据格式之间的转换。 应用层：是最靠近用户的OSI层。这一层为用户的应用程序（例如电子邮件、文件传输和终端仿真）提供网络服务。 层与协议每一层都是为了完成一种功能，为了实现这些功能，就需要大家都遵守共同的规则。大家都遵守这规则，就叫做“协议”（protocol）。网络的每一层，都定义了很多协议。这些协议的总称，叫“TCP/IP协议”。TCP/IP协议是一个大家族，不仅仅只有TCP和IP协议，它还包括其它的协议，如下图： 各层功能 链路层以太网规定，连入网络的所有设备，都必须具有“网卡”接口。数据包必须是从一块网卡，传送到另一块网卡。通过网卡能够使不同的计算机之间连接，从而完成数据通信等功能。网卡的地址——MAC 地址，就是数据包的物理发送地址和物理接收地址。 网络层网络层的作用是引进一套新的地址，使得我们能够区分不同的计算机是否属于同一个子网络。这套地址就叫做“网络地址”，就是我们平时所说的IP地址。这个IP地址好比我们的手机号码，通过手机号码可以得到用户所在的归属地。网络地址帮助我们确定计算机所在的子网络，MAC 地址则将数据包送到该子网络中的目标网卡。网络层协议包含的主要信息是源IP和目的IP。于是，“网络层”出现以后，每台计算机有了两种地址，一种是 MAC 地址，另一种是网络地址。两种地址之间没有任何联系，MAC 地址是绑定在网卡上的，网络地址则是管理员分配的，它们只是随机组合在一起。网络地址帮助我们确定计算机所在的子网络，MAC 地址则将数据包送到该子网络中的目标网卡。因此，从逻辑上可以推断，必定是先处理网络地址，然后再处理 MAC 地址。 传输层当我们一边聊QQ，一边聊微信，当一个数据包从互联网上发来的时候，我们怎么知道，它是来自QQ的内容，还是来自微信的内容？也就是说，我们还需要一个参数，表示这个数据包到底供哪个程序（进程）使用。这个参数就叫做“端口”（port），它其实是每一个使用网卡的程序的编号。每个数据包都发到主机的特定端口，所以不同的程序就能取到自己所需要的数据。端口特点： 对于同一个端口，在不同系统中对应着不同的进程 对于同一个系统，一个端口只能被一个进程拥有 应用层应用程序收到“传输层”的数据，接下来就要进行解读。由于互联网是开放架构，数据来源五花八门，必须事先规定好格式，否则根本无法解读。“应用层”的作用，就是规定应用程序的数据格式。 通信过程两台计算机通过TCP/IP协议通讯的过程如下所示：]]></content>
      <categories>
        <category>网络编程协议</category>
      </categories>
      <tags>
        <tag>通信协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git知识点详解]]></title>
    <url>%2F2018%2F09%2F07%2Fgit%E7%9F%A5%E8%AF%86%E7%82%B9%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Git知识点详解 文件状态现在我们手上已经有了一个真实项目的 Git 仓库，并从这个仓库中取出了所有文件的工作拷贝。接下来，对这些文件作些修改，在完成了一个阶段的目标之后，提交本次更新到仓库。 请记住，工作目录下面的所有文件都不外乎这两种状态：已跟踪或未跟踪。已跟踪的文件是指本来就被纳入版本控制管理的文件，在上次快照中有它们的记 录，工作一段时间后，它们的状态可能是未更新，已修改或者已放入暂存区。而所有其他文件都属于未跟踪文件。它们既没有上次更新时的快照，也不在当前的暂存 区域。初次克隆某个仓库时，工作目录中的所有文件都属于已跟踪文件，且状态为未修改。 在编辑过某些文件之后，Git 将这些文件标为已修改。我们逐步把这些修改过的文件放到暂存区域，直到最后一次性提交所有这些暂存起来的文件，如此重复。如下图： 检查当前文件状态要确定哪些文件当前处于什么状态，可以用 git status 命令。如果在克隆仓库之后立即执行此命令，会看到类似这样的输出： 123$ git status# On branch masternothing to commit (working directory clean) 这说明你现在的工作目录相当干净。换句话说，当前没有任何跟踪着的文件，也没有任何文件在上次提交后更改过。此外，上面的信息还表明，当前目录下没 有出现任何处于未跟踪的新文件，否则 Git 会在这里列出来。最后，该命令还显示了当前所在的分支是 master，这是默认的分支名称，实际是可以修改的，现在先不用考虑。下一章我们就会详细讨论分支和引用。 现在让我们用 vim 编辑一个新文件 README，保存退出后运行 git status 会看到该文件出现在未跟踪文件列表中： 12345678$ vim README$ git status# On branch master# Untracked files:# (use &quot;git add ...&quot; to include in what will be committed)## READMEnothing added to commit but untracked files present (use &quot;git add&quot; to track) 就是在“Untracked files”这行下面。Git 不会自动将之纳入跟踪范围，除非你明明白白地告诉它“我需要跟踪该文件”，因而不用担心把临时文件什么的也归入版本管理。不过现在的例子中，我们确实想要跟踪管理 README 这个文件。 跟踪新文件 使用命令 git add 开始跟踪一个新文件。所以，要跟踪 README 文件，运行： 1$ git add README 此时再运行 git status 命令，会看到 README 文件已被跟踪，并处于暂存状态： 1234567$ git status# On branch master# Changes to be committed:# (use &quot;git reset HEAD ...&quot; to unstage)## new file: README# 只要在 “Changes to be committed” 这行下面的，就说明是已暂存状态。如果此时提交，那么该文件此时此刻的版本将被留存在历史记录中。你可能会想起之前我们使用git init 后就运行了 git add 命令，开始跟踪当前目录下的文件。在 git add 后面可以指明要跟踪的文件或目录路径。如果是目录的话，就说明要递归跟踪该目录下的所有文件。（译注：其实git add 的潜台词就是把目标文件快照放入暂存区域，也就是 add file into staged area，同时未曾跟踪过的文件标记为需要跟踪。这样就好理解后续 add 操作的实际意义了。） 暂存已修改文件 现在我们修改下之前已跟踪过的文件 benchmarks.rb，然后再次运行 status 命令，会看到这样的状态报告： 123456789101112$ git status# On branch master# Changes to be committed:# (use &quot;git reset HEAD ...&quot; to unstage)## new file: README## Changed but not updated:# (use &quot;git add ...&quot; to update what will be committed)## modified: benchmarks.rb# 文件 benchmarks.rb 出现在 “Changed but not updated” 这行下面，说明已跟踪文件的内容发生了变化，但还没有放到暂存区。要暂存这次更新，需要运行git add 命令（这是个多功能命令，根据目标文件的状态不同，此命令的效果也不同：可以用它开始跟踪新文件，或者把已跟踪的文件放到暂存区，还能用于合并时把有冲突的文件标记为已解决状态等）。现在让我们运行git add 将 benchmarks.rb 放到暂存区，然后再看看 git status 的输出： 123456789$ git add benchmarks.rb$ git status# On branch master# Changes to be committed:# (use &quot;git reset HEAD ...&quot; to unstage)## new file: README# modified: benchmarks.rb# 现在两个文件都已暂存，下次提交时就会一并记录到仓库。假设此时，你想要在 benchmarks.rb 里再加条注释，重新编辑存盘后，准备好提交。不过且慢，再运行git status 看看： 1234567891011121314$ vim benchmarks.rb $ git status# On branch master# Changes to be committed:# (use &quot;git reset HEAD ...&quot; to unstage)## new file: README# modified: benchmarks.rb## Changed but not updated:# (use &quot;git add ...&quot; to update what will be committed)## modified: benchmarks.rb# 怎么回事？benchmarks.rb 文件出现了两次！一次算未暂存，一次算已暂存，这怎么可能呢？好吧，实际上 Git 只不过暂存了你运行 git add 命令时的版本，如果现在提交，那么提交的是添加注释前的版本，而非当前工作目录中的版本。所以，运行了git add 之后又作了修订的文件，需要重新运行 git add 把最新版本重新暂存起来： 123456789$ git add benchmarks.rb$ git status# On branch master# Changes to be committed:# (use &quot;git reset HEAD ...&quot; to unstage)## new file: README# modified: benchmarks.rb# 忽略某些文件一般我们总会有些文件无需纳入 Git 的管理，也不希望它们总出现在未跟踪文件列表。通常都是些自动生成的文件，比如日志文件，或者编译过程中创建的临时文件等。我们可以创建一个名为 .gitignore 的文件，列出要忽略的文件模式。来看一个实际的例子： 123$ cat .gitignore*.[oa]*~ 第一行告诉 Git 忽略所有以 .o 或 .a 结尾的文件。一般这类对象文件和存档文件都是编译过程中出现的，我们用不着跟踪它们的版本。第二行告诉 Git 忽略所有以波浪符（~）结尾的文件，许多文本编辑软件（比如 Emacs）都用这样的文件名保存副本。此外，你可能还需要忽略 log，tmp 或者 pid 目录，以及自动生成的文档等等。要养成一开始就设置好 .gitignore 文件的习惯，以免将来误提交这类无用的文件。 文件 .gitignore 的格式规范如下： 所有空行或者以注释符号 ＃ 开头的行都会被 Git 忽略。 可以使用标准的 glob 模式匹配。 匹配模式最后跟反斜杠（/）说明要忽略的是目录。 要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号（!）取反。 所谓的 glob 模式是指 shell 所使用的简化了的正则表达式。星号（*）匹配零个或多个任意字符；[abc] 匹配任何一个列在方括号中的字符（这个例子要么匹配一个 a，要么匹配一个 b，要么匹配一个 c）；问号（?）只匹配一个任意字符；如果在方括号中使用短划线分隔两个字符，表示所有在这两个字符范围内的都可以匹配（比如[0-9] 表示匹配所有 0 到 9 的数字）。 我们再看一个 .gitignore 文件的例子： 123456# 此为注释 – 将被 Git 忽略*.a # 忽略所有 .a 结尾的文件!lib.a # 但 lib.a 除外/TODO # 仅仅忽略项目根目录下的 TODO 文件，不包括 subdir/TODObuild/ # 忽略 build/ 目录下的所有文件doc/*.txt # 会忽略 doc/notes.txt 但不包括 doc/server/arch.txt 查看已暂存和未暂存的更新实际上 git status 的显示比较简单，仅仅是列出了修改过的文件，如果要查看具体修改了什么地方，可以用 git diff 命令。稍后我们会详细介绍git diff，不过现在，它已经能回答我们的两个问题了：当前做的哪些更新还没有暂存？有哪些更新已经暂存起来准备好了下次提交？ git diff 会使用文件补丁的格式显示具体添加和删除的行。 假如再次修改 README 文件后暂存，然后编辑 benchmarks.rb 文件后先别暂存，运行 status命令，会看到： 123456789101112$ git status# On branch master# Changes to be committed:# (use &quot;git reset HEAD ...&quot; to unstage)## new file: README## Changed but not updated:# (use &quot;git add ...&quot; to update what will be committed)## modified: benchmarks.rb# 要查看尚未暂存的文件更新了哪些部分，不加参数直接输入 git diff： 12345678910111213141516$ git diffdiff --git a/benchmarks.rb b/benchmarks.rbindex 3cb747f..da65585 100644--- a/benchmarks.rb+++ b/benchmarks.rb@@ -36,6 +36,10 @@ def main @commit.parents[0].parents[0].parents[0] end+ run_code(x, &apos;commits 1&apos;) do+ git.commits.size+ end+ run_code(x, &apos;commits 2&apos;) do log = git.commits(&apos;master&apos;, 15) log.size 此命令比较的是工作目录中当前文件和暂存区域快照之间的差异，也就是修改之后还没有暂存起来的变化内容。 若要看已经暂存起来的文件和上次提交时的快照之间的差异，可以用 git diff --cached 命令。（Git 1.6.1 及更高版本还允许使用git diff --staged，效果是相同的，但更好记些。）来看看实际的效果： 123456789101112$ git diff --cacheddiff --git a/README b/READMEnew file mode 100644index 0000000..03902a1--- /dev/null+++ b/README2@@ -0,0 +1,5 @@+grit+ by Tom Preston-Werner, Chris Wanstrath+ http://github.com/mojombo/grit++Grit is a Ruby library for extracting information from a Git repository 请注意，单单 git diff 不过是显示还没有暂存起来的改动，而不是这次工作和上次提交之间的差异。所以有时候你一下子暂存了所有更新过的文件后，运行git diff 后却什么也没有，就是这个原因。 像之前说的，暂存 benchmarks.rb 后再编辑，运行 git status 会看到暂存前后的两个版本： 12345678910111213$ git add benchmarks.rb$ echo &apos;# test line&apos; &gt;&gt; benchmarks.rb$ git status# On branch master## Changes to be committed:## modified: benchmarks.rb## Changed but not updated:## modified: benchmarks.rb# 现在运行 git diff 看暂存前后的变化： 12345678910$ git diffdiff --git a/benchmarks.rb b/benchmarks.rbindex e445e28..86b2f7c 100644--- a/benchmarks.rb+++ b/benchmarks.rb@@ -127,3 +127,4 @@ end main() ##pp Grit::GitRuby.cache_client.stats+# test line 然后用 git diff --cached 查看已经暂存起来的变化： 12345678910111213141516$ git diff --cacheddiff --git a/benchmarks.rb b/benchmarks.rbindex 3cb747f..e445e28 100644--- a/benchmarks.rb+++ b/benchmarks.rb@@ -36,6 +36,10 @@ def main @commit.parents[0].parents[0].parents[0] end+ run_code(x, &apos;commits 1&apos;) do+ git.commits.size+ end+ run_code(x, &apos;commits 2&apos;) do log = git.commits(&apos;master&apos;, 15) log.size 提交更新现在的暂存区域已经准备妥当可以提交了。在此之前，请一定要确认还有什么修改过的或新建的文件还没有 git add过，否则提交的时候不会记录这些还没暂存起来的变化。所以，每次准备提交前，先用git status 看下，是不是都已暂存起来了，然后再运行提交命令 git commit： 1$ git commit 这种方式会启动文本编辑器以便输入本次提交的说明。（默认会启用 shell 的环境变量 $EDITOR 所指定的软件，一般都是 vim 或 emacs。当然也可以按照第一章介绍的方式，使用git config --global core.editor 命令设定你喜欢的编辑软件。） 编辑器会显示类似下面的文本信息（本例选用 Vim 的屏显方式展示）： 123456789101112# Please enter the commit message for your changes. Lines starting# with &apos;#&apos; will be ignored, and an empty message aborts the commit.# On branch master# Changes to be committed:# (use &quot;git reset HEAD ...&quot; to unstage)## new file: README# modified: benchmarks.rb~~~&quot;.git/COMMIT_EDITMSG&quot; 10L, 283C 可以看到，默认的提交消息包含最后一次运行 git status 的输出，放在注释行里，另外开头还有一空行，供你输入提交说明。你完全可以去掉这些注释行，不过留着也没关系，多少能帮你回想起这次更新的内容有哪些。（如果觉得这还不够，可以用-v 选项将修改差异的每一行都包含到注释中来。）退出编辑器时，Git 会丢掉注释行，将说明内容和本次更新提交到仓库。 另外也可以用 -m 参数后跟提交说明的方式，在一行命令中提交更新： 1234$ git commit -m &quot;Story 182: Fix benchmarks for speed&quot;[master]: created 463dc4f: &quot;Fix benchmarks for speed&quot; 2 files changed, 3 insertions(+), 0 deletions(-) create mode 100644 README 好，现在你已经创建了第一个提交！可以看到，提交后它会告诉你，当前是在哪个分支（master）提交的，本次提交的完整 SHA-1 校验和是什么（463dc4f），以及在本次提交中，有多少文件修订过，多少行添改和删改过。 记住，提交时记录的是放在暂存区域的快照，任何还未暂存的仍然保持已修改状态，可以在下次提交时纳入版本管理。每一次运行提交操作，都是对你项目作一次快照，以后可以回到这个状态，或者进行比较。 跳过使用暂存区域尽管使用暂存区域的方式可以精心准备要提交的细节，但有时候这么做略显繁琐。Git 提供了一个跳过使用暂存区域的方式，只要在提交的时候，给 git commit 加上-a 选项，Git 就会自动把所有已经跟踪过的文件暂存起来一并提交，从而跳过 git add 步骤： 看到了吗？提交之前不再需要 git add 文件 benchmarks.rb 了。 移除文件要从 Git 中移除某个文件，就必须要从已跟踪文件清单中移除（确切地说，是从暂存区域移除），然后提交。可以用 git rm 命令完成此项工作，并连带从工作目录中删除指定的文件，这样以后就不会出现在未跟踪文件清单中了。 如果只是简单地从工作目录中手工删除文件，运行 git status 12345678910$ git status# On branch master## Changed but not updated:## modified: benchmarks.rb#$ git commit -a -m &apos;added new benchmarks&apos;[master 83e38c7] added new benchmarks 1 files changed, 5 insertions(+), 0 deletions(-) 时就会在 “Changed but not updated” 部分（也就是_未暂存_清单）看到： 123456789$ rm grit.gemspec$ git status# On branch master## Changed but not updated:# (use &quot;git add/rm ...&quot; to update what will be committed)## deleted: grit.gemspec# 然后再运行 git rm 记录此次移除文件的操作： 12345678910$ git rm grit.gemspecrm &apos;grit.gemspec&apos;$ git status# On branch master## Changes to be committed:# (use &quot;git reset HEAD ...&quot; to unstage)## deleted: grit.gemspec# 最后提交的时候，该文件就不再纳入版本管理了。如果删除之前修改过并且已经放到暂存区域的话，则必须要用强制删除选项 -f（译注：即 force 的首字母），以防误删除文件后丢失修改的内容。 另外一种情况是，我们想把文件从 Git 仓库中删除（亦即从暂存区域移除），但仍然希望保留在当前工作目录中。换句话说，仅是从跟踪清单中删除。比如一些大型日志文件或者一堆.a 编译文件，不小心纳入仓库后，要移除跟踪但不删除文件，以便稍后在 .gitignore 文件中补上，用 --cached 选项即可： 1$ git rm --cached readme.txt 后面可以列出文件或者目录的名字，也可以使用 glob 模式。比方说： 1$ git rm log/\*.log 注意到星号 * 之前的反斜杠 \，因为 Git 有它自己的文件模式扩展匹配方式，所以我们不用 shell 来帮忙展开（译注：实际上不加反斜杠也可以运行，只不过按照 shell 扩展的话，仅仅删除指定目录下的文件而不会递归匹配。上面的例子本来就指定了目录，所以效果等同，但下面的例子就会用递归方式匹配，所以必须加反斜 杠。）。此命令删除所有log/ 目录下扩展名为 .log 的文件。类似的比如： 1$ git rm \*~ 会递归删除当前目录及其子目录中所有 ~ 结尾的文件。 移动文件不像其他的 VCS 系统，Git 并不跟踪文件移动操作。如果在 Git 中重命名了某个文件，仓库中存储的元数据并不会体现出这是一次改名操作。不过 Git 非常聪明，它会推断出究竟发生了什么，至于具体是如何做到的，我们稍后再谈。 既然如此，当你看到 Git 的 mv 命令时一定会困惑不已。要在 Git 中对文件改名，可以这么做： 1$ git mv file_from file_to 它会恰如预期般正常工作。实际上，即便此时查看状态信息，也会明白无误地看到关于重命名操作的说明： 12345678910$ git mv README.txt README$ git status# On branch master# Your branch is ahead of &apos;origin/master&apos; by 1 commit.## Changes to be committed:# (use &quot;git reset HEAD..&quot; to unstage)## renamed: README.txt -&gt; README# 其实，运行 git mv 就相当于运行了下面三条命令： 123$ mv README.txt README$ git rm README.txt$ git add README 如此分开操作，Git 也会意识到这是一次改名，所以不管何种方式都一样。当然，直接用 git mv轻便得多，不过有时候用其他工具批处理改名的话，要记得在提交前删除老的文件名，再添加新的文件名。 查看提交历史 在提交了若干更新之后，又或者克隆了某个项目，想回顾下提交历史，可以使用 git log 命令查看。 接下来的例子会用我专门用于演示的 simplegit 项目，运行下面的命令获取该项目源代码： 1git clone git://github.com/schacon/simplegit-progit.git 然后在此项目中运行 git log，应该会看到下面的输出： 123456789101112131415161718192021222324252627$ git logcommit ca82a6dff817ec66f44342007202690a93763949Author: Scott Chacon &lt;schacon@gee-mail.com&gt; Date: Mon Mar 17 21:52:11 2008 -0700 changed the version numbercommit 085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7Author: Scott Chacon &lt;schacon@gee-mail.com&gt; Date: Sat Mar 15 16:40:33 2008 -0700 removed unnecessary test codecommit a11bef06a3f659402fe7563abf99ad00de2209e6Author: Scott Chacon &lt;schacon@gee-mail.com&gt; Date: Sat Mar 15 10:31:28 2008 -0700 first commit 默认不用任何参数的话，git log 会按提交时间列出所有的更新，最近的更新排在最上面。看到了吗，每次更新都有一个 SHA-1 校验和、作者的名字和电子邮件地址、提交时间，最后缩进一个段落显示提交说明。 git log 有许多选项可以帮助你搜寻感兴趣的提交，接下来我们介绍些最常用的。 我们常用 -p 选项展开显示每次提交的内容差异，用 -2 则仅显示最近的两次更新： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687$ git log -p -2commit ca82a6dff817ec66f44342007202690a93763949Author: Scott Chacon &lt;schacon@gee-mail.com&gt; Date: Mon Mar 17 21:52:11 2008 -0700 changed the version numberdiff --git a/Rakefile b/Rakefileindex a874b73..8f94139 100644--- a/Rakefile+++ b/Rakefile@@ -5,7 +5,7 @@ require &apos;rake/gempackagetask&apos; spec = Gem::Specification.new do |s|- s.version = &quot;0.1.0&quot;+ s.version = &quot;0.1.1&quot; s.author = &quot;Scott Chacon&quot;commit 085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7Author: Scott Chacon &lt;schacon@gee-mail.com&gt; Date: Sat Mar 15 16:40:33 2008 -0700 removed unnecessary test codediff --git a/lib/simplegit.rb b/lib/simplegit.rbindex a0a60ae..47c6340 100644--- a/lib/simplegit.rb+++ b/lib/simplegit.rb@@ -18,8 +18,3 @@ class SimpleGit end end--if $0 == __FILE__- git = SimpleGit.new- puts git.show-end\ No newline at end of file在做代码审查，或者要快速浏览其他协作者提交的更新都作了哪些改动时，就可以用这个选项。此外，还有许多摘要选项可以用，比如 --stat，仅显示简要的增改行数统计：$ git log --stat commit ca82a6dff817ec66f44342007202690a93763949Author: Scott Chacon &lt;schacon@gee-mail.com&gt;Date: Mon Mar 17 21:52:11 2008 -0700 changed the version number Rakefile | 2 +- 1 files changed, 1 insertions(+), 1 deletions(-)commit 085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7Author: Scott Chacon &lt;schacon@gee-mail.com&gt;Date: Sat Mar 15 16:40:33 2008 -0700 removed unnecessary test code lib/simplegit.rb | 5 ----- 1 files changed, 0 insertions(+), 5 deletions(-)commit a11bef06a3f659402fe7563abf99ad00de2209e6Author: Scott Chacon &lt;schacon@gee-mail.com&gt;Date: Sat Mar 15 10:31:28 2008 -0700 first commit README | 6 ++++++ Rakefile | 23 +++++++++++++++++++++++ lib/simplegit.rb | 25 +++++++++++++++++++++++++ 3 files changed, 54 insertions(+), 0 deletions(-) 每个提交都列出了修改过的文件，以及其中添加和移除的行数，并在最后列出所有增减行数小计。还有个常用的 --pretty 选项，可以指定使用完全不同于默认格式的方式展示提交历史。比如用oneline 将每个提交放在一行显示，这在提交数很大时非常有用。另外还有 short，full 和fuller 可以用，展示的信息或多或少有些不同，请自己动手实践一下看看效果如何。 但最有意思的是 format，可以定制要显示的记录格式，这样的输出便于后期编程提取分析，像这样： 12345678$ git log --pretty=format:&quot;%h - %an, %ar : %s&quot;$ git log --pretty=onelineca82a6dff817ec66f44342007202690a93763949 changed the version number085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7 removed unnecessary test codea11bef06a3f659402fe7563abf99ad00de2209e6 first commitca82a6d - Scott Chacon, 11 months ago : changed the version number085bb3b - Scott Chacon, 11 months ago : removed unnecessary test codea11bef0 - Scott Chacon, 11 months ago : first commit 表 列出了常用的格式占位符写法及其代表的意义。 1 你一定奇怪_作者（author）_和_提交者（committer）_之间究竟有何差别，其实作者指的是实际作出修改的人，提交者指的是最后将此 工作成果提交到仓库的人。所以，当你为某个项目发布补丁，然后某个核心成员将你的补丁并入项目时，你就是作者，而那个核心成员就是提交者。我们会在第五章 再详细介绍两者之间的细微差别。 用 oneline 或 format 时结合 --graph 选项，可以看到开头多出一些 ASCII 字符串表示的简单图形，形象地展示了每个提交所在的分支及其分化衍合情况。在我们之前提到的 Grit 项目仓库中可以看到： 12345678910111213141516171819202122232425262728$ git log --pretty=format:&quot;%h %s&quot; --graph* 2d3acf9 ignore errors from SIGCHLD on trap* 5e3ee11 Merge branch &apos;master&apos; of git://github.com/dustin/grit|\| * 420eac9 Added a method for getting the current branch.* | 30e367c timeout code and tests* | 5a09431 add timeout protection to grit* | e1193f8 support for heads with slashes in them|/* d6016bc require time for xmlschema* 11d191e Merge b选项 说明%H 提交对象（commit）的完整哈希字串%h 提交对象的简短哈希字串%T 树对象（tree）的完整哈希字串%t 树对象的简短哈希字串%P 父对象（parent）的完整哈希字串%p 父对象的简短哈希字串%an 作者（author）的名字%ae 作者的电子邮件地址%ad 作者修订日期（可以用 -date= 选项定制格式）%ar 作者修订日期，按多久以前的方式显示%cn 提交者(committer)的名字%ce 提交者的电子邮件地址%cd 提交日期%cr 提交日期，按多久以前的方式显示%s 提交说明ranch &apos;defunkt&apos; into local 以上只是简单介绍了一些 git log 命令支持的选项。表 2-2 还列出了一些其他常用的选项及其释义。 123456789-p 按补丁格式显示每个更新之间的差异。--stat 显示每次更新的文件修改统计信息。--shortstat 只显示 --stat 中最后的行数修改添加移除统计。--name-only 仅在提交信息后显示已修改的文件清单。--name-status 显示新增、修改、删除的文件清单。--abbrev-commit 仅显示 SHA-1 的前几个字符，而非所有的 40 个字符。--relative-date 使用较短的相对时间显示（比如，“2 weeks ago”）。--graph 显示 ASCII 图形表示的分支合并历史。--pretty 使用其他格式显示历史提交信息。可用的选项包括 oneline，short，full，fuller 和 format（后跟指定格式） 限制输出长度除了定制输出格式的选项之外，git log 还有许多非常实用的限制输出长度的选项，也就是只输出部分提交信息。之前我们已经看到过 -2 了，它只显示最近的两条提交，实际上，这是 -选项的写法，其中的 n 可以是任何自然数，表示仅显示最近的若干条提交。不过实践中我们是不太用这个选项的，Git 在输出所有提交时会自动调用分页程序（less），要看更早的更新只需翻到下页即可。 另外还有按照时间作限制的选项，比如 --since 和 --until。下面的命令列出所有最近两周内的提交： 1$ git log --since=2.weeks 你可以给出各种时间格式，比如说具体的某一天（“2008-01-15”），或者是多久以前（“2 years 1 day 3 minutes ago”）。 还可以给出若干搜索条件，列出符合的提交。用 --author 选项显示指定作者的提交，用 --grep选项搜索提交说明中的关键字。（请注意，如果要得到同时满足这两个选项搜索条件的提交，就必须用--all-match 选项。） 如果只关心某些文件或者目录的历史提交，可以在 git log 选项的最后指定它们的路径。因为是放在最后位置上的选项，所以用两个短划线（--）隔开之前的选项和后面限定的路径名。 表 2-3 还列出了其他常用的类似选项。 123456选项 说明-(n) 仅显示最近的 n 条提交--since, --after 仅显示指定时间之后的提交。--until, --before 仅显示指定时间之前的提交。--author 仅显示指定作者相关的提交。--committer 仅显示指定提交者相关的提交。 来看一个实际的例子，如果要查看 Git 仓库中，2008 年 10 月期间，Junio Hamano 提交的但未合并的测试脚本（位于项目的 t/ 目录下的文件），可以用下面的查询命令： 12345678$ git log --pretty=&quot;%h - %s&quot; --author=gitster --since=&quot;2008-10-01&quot; \ --before=&quot;2008-11-01&quot; --no-merges -- t/5610e3b - Fix testcase failure when extended attributeacd3b9e - Enhance hold_lock_file_for_&#123;update,append&#125;()f563754 - demonstrate breakage of detached checkout wid1a43f2 - reset --hard/read-tree --reset -u: remove un51a94af - Fix &quot;checkout --track -b newbranch&quot; on detacb0ad11e - pull: allow &quot;git pull origin $something:$cur Git 项目有 20,000 多条提交，但我们给出搜索选项后，仅列出了其中满足条件的 6 条。 使用图形化工具查阅提交历史有时候图形化工具更容易展示历史提交的变化，随 Git 一同发布的 gitk 就是这样一种工具。它是用 Tcl/Tk 写成的，基本上相当于 git log 命令的可视化版本，凡是git log 可以用的选项也都能用在 gitk 上。在项目工作目录中输入 gitk 命令后，就会启动 上半个窗口显示的是历次提交的分支祖先图谱，下半个窗口显示当前点选的提交对应的具体差异。 撤销操作任何时候，你都有可能需要撤消刚才所做的某些操作。接下来，我们会介绍一些基本的撤消操作相关的命令。请注意，有些操作并不总是可以撤消的，所以请务必谨慎小心，一旦失误，就有可能丢失部分工作成果。 修改最后一次提交有时候我们提交完了才发现漏掉了几个文件没有加，或者提交信息写错了。想要撤消刚才的提交操作，可以使用 --amend 选项重新提交： 1$ git commit --amend 此命令将使用当前的暂存区域快照提交。如果刚才提交完没有作任何改动，直接运行此命令的话，相当于有机会重新编辑提交说明，但将要提交的文件快照和之前的一样。 启动文本编辑器后，会看到上次提交时的说明，编辑它确认没问题后保存退出，就会使用新的提交说明覆盖刚才失误的提交。 如果刚才提交时忘了暂存某些修改，可以先补上暂存操作，然后再运行 --amend 提交： 123$ git commit -m &apos;initial commit&apos;$ git add forgotten_file$ git commit --amend 上面的三条命令最终只是产生一个提交，第二个提交命令修正了第一个的提交内容。 取消已经暂存的文件接下来的两个小节将演示如何取消暂存区域中的文件，以及如何取消工作目录中已修改的文件。不用担心，查看文件状态的时候就提示了该如何撤消，所以不需要死记硬背。来看下面的例子，有两个修改过的文件，我们想要分开提交，但不小心用git add . 全加到了暂存区域。该如何撤消暂存其中的一个文件呢？其实，git status 的命令输出已经告诉了我们该怎么做： 123456789$ git add .$ git status# On branch master# Changes to be committed:# (use &quot;git reset HEAD ...&quot; to unstage)## modified: README.txt# modified: benchmarks.rb# 就在 “Changes to be committed” 下面，括号中有提示，可以使用 git reset HEAD ...的方式取消暂存。好吧，我们来试试取消暂存 benchmarks.rb 文件： 123456789101112131415$ git reset HEAD benchmarks.rbbenchmarks.rb: locally modified$ git status# On branch master# Changes to be committed:# (use &quot;git reset HEAD ...&quot; to unstage)## modified: README.txt## Changed but not updated:# (use &quot;git add ...&quot; to update what will be committed)# (use &quot;git checkout -- ...&quot; to discard changes in working directory)## modified: benchmarks.rb# 这条命令看起来有些古怪，先别管，能用就行。现在 benchmarks.rb 文件又回到了之前已修改未暂存的状态。 取消对文件的修改如果觉得刚才对 benchmarks.rb 的修改完全没有必要，该如何取消修改，回到之前的状态（也就是修改之前的版本）呢？git status 同样提示了具体的撤消方法，接着上面的例子，现在未暂存区域看起来像这样： 123456# Changed but not updated:# (use &quot;git add ...&quot; to update what will be committed)# (use &quot;git checkout -- ...&quot; to discard changes in working directory)## modified: benchmarks.rb# 在第二个括号中，我们看到了抛弃文件修改的命令（至少在 Git 1.6.1 以及更高版本中会这样提示，如果你还在用老版本，我们强烈建议你升级，以获取最佳的用户体验），让我们试试看： 12345678$ git checkout -- benchmarks.rb$ git status# On branch master# Changes to be committed:# (use &quot;git reset HEAD ...&quot; to unstage)## modified: README.txt# 可以看到，该文件已经恢复到修改前的版本。你可能已经意识到了，这条命令有些危险，所有对文件的修改都没有了，因为我们刚刚把之前版本的文件复制过 来重写了此文件。所以在用这条命令前，请务必确定真的不再需要保留刚才的修改。如果只是想回退版本，同时保留刚才的修改以便将来继续工作，可以用下章介绍 的 stashing 和分支来处理，应该会更好些。 记住，任何已经提交到 Git 的都可以被恢复。即便在已经删除的分支中的提交，或者用 --amend重新改写的提交，都可以被恢复（关于数据恢复的内容见第九章）。所以，你可能失去的数据，仅限于没有提交过的，对 Git 来说它们就像从未存在过一样。 远程仓库的使用 要参与任何一个 Git 项目的协作，必须要了解该如何管理远程仓库。远程仓库是指托管在网络上的项目仓库，可能会有好多个，其中有些你只能读，另外有些可以写。同他人协作开发某 个项目时，需要管理这些远程仓库，以便推送或拉取数据，分享各自的工作进展。管理远程仓库的工作，包括添加远程库，移除废弃的远程库，管理各式远程库分 支，定义是否跟踪这些分支，等等。本节我们将详细讨论远程库的管理和使用。 查看当前的远程库 要查看当前配置有哪些远程仓库，可以用 git remote 命令，它会列出每个远程库的简短名字。在克隆完某个项目后，至少可以看到一个名为 origin 的远程库，Git 默认使用这个名字来标识你所克隆的原始仓库： 12345678910$ git clone git://github.com/schacon/ticgit.gitInitialized empty Git repository in /private/tmp/ticgit/.git/remote: Counting objects: 595, done.remote: Compressing objects: 100% (269/269), done.remote: Total 595 (delta 255), reused 589 (delta 253)Receiving objects: 100% (595/595), 73.31 KiB | 1 KiB/s, done.Resolving deltas: 100% (255/255), done.$ cd ticgit$ git remoteorigin 也可以加上 -v 选项（译注：此为 --verbose 的简写，取首字母），显示对应的克隆地址： 12$ git remote -vorigin git://github.com/schacon/ticgit.git 如果有多个远程仓库，此命令将全部列出。比如在我的 Grit 项目中，可以看到： 1234567$ cd grit$ git remote -vbakkdoor git://github.com/bakkdoor/grit.gitcho45 git://github.com/cho45/grit.gitdefunkt git://github.com/defunkt/grit.gitkoke git://github.com/koke/grit.gitorigin git@github.com:mojombo/grit.git 这样一来，我就可以非常轻松地从这些用户的仓库中，拉取他们的提交到本地。请注意，上面列出的地址只有 origin 用的是 SSH URL 链接，所以也只有这个仓库我能推送数据上去（我们会在第四章解释原因）。 添加远程仓库 要添加一个新的远程仓库，可以指定一个简单的名字，以便将来引用，运行 git remote add [shortname] [url]： 123456$ git remoteorigin$ git remote add pb git://github.com/paulboone/ticgit.git$ git remote -vorigin git://github.com/schacon/ticgit.gitpb git://github.com/paulboone/ticgit.git 现在可以用字串 pb 指代对应的仓库地址了。比如说，要抓取所有 Paul 有的，但本地仓库没有的信息，可以运行 git fetch pb： 12345678$ git fetch pbremote: Counting objects: 58, done.remote: Compressing objects: 100% (41/41), done.remote: Total 44 (delta 24), reused 1 (delta 0)Unpacking objects: 100% (44/44), done.From git://github.com/paulboone/ticgit * [new branch] master -&gt; pb/master * [new branch] ticgit -&gt; pb/ticgit 现在，Paul 的主干分支（master）已经完全可以在本地访问了，对应的名字是 pb/master，你可以将它合并到自己的某个分支，或者切换到这个分支，看看有些什么有趣的更新。 从远程仓库抓取数据正如之前所看到的，可以用下面的命令从远程仓库抓取数据到本地： 1$ git fetch [remote-name] 此命令会到远程仓库中拉取所有你本地仓库中还没有的数据。运行完成后，你就可以在本地访问该远程仓库中的所有分支，将其中某个分支合并到本地，或者只是取出某个分支，一探究竟。（我们会在第三章详细讨论关于分支的概念和操作。） 如果是克隆了一个仓库，此命令会自动将远程仓库归于 origin 名下。所以，git fetch origin 会抓取从你上次克隆以来别人上传到此远程仓库中的所有更新（或是上次 fetch 以来别人提交的更新）。有一点很重要，需要记住，fetch 命令只是将远端的数据拉到本地仓库，并不自动合并到当前工作分支，只有当你确实准备好了，才能手工合并。 如果设置了某个分支用于跟踪某个远端仓库的分支（参见下节及第三章的内容），可以使用 git pull 命令自动抓取数据下来，然后将远端分支自动合并到本地仓库中当前分支。在日常工作中我们经常这么用，既快且好。实际上，默认情况下git clone 命令本质上就是自动创建了本地的 master 分支用于跟踪远程仓库中的 master 分支（假设远程仓库确实有 master 分支）。所以一般我们运行git pull，目的都是要从原始克隆的远端仓库中抓取数据后，合并到工作目录中的当前分支。 推送数据到远程仓库项目进行到一个阶段，要同别人分享目前的成果，可以将本地仓库中的数据推送到远程仓库。实现这个任务的命令很简单： git push [remote-name] [branch-name]。如果要把本地的 master 分支推送到origin 服务器上（再次说明下，克隆操作会自动使用默认的 master 和 origin 名字），可以运行下面的命令： 1$ git push origin master 只有在所克隆的服务器上有写权限，或者同一时刻没有其他人在推数据，这条命令才会如期完成任务。如果在你推数据前，已经有其他人推送了若干更新，那 你的推送操作就会被驳回。你必须先把他们的更新抓取到本地，合并到自己的项目中，然后才可以再次推送。有关推送数据到远程仓库的详细内容见第三章。 查看远程仓库信息我们可以通过命令 git remote show [remote-name] 查看某个远程仓库的详细信息，比如要看所克隆的 origin 仓库，可以运行： 12345678$ git remote show origin* remote origin URL: git://github.com/schacon/ticgit.git Remote branch merged with &apos;git pull&apos; while on branch master master Tracked remote branches master ticgit 除了对应的克隆地址外，它还给出了许多额外的信息。它友善地告诉你如果是在 master 分支，就可以用 git pull 命令抓取数据合并到本地。另外还列出了所有处于跟踪状态中的远端分支。 上面的例子非常简单，而随着使用 Git 的深入，git remote show 给出的信息可能会像这样： 123456789101112131415161718192021$ git remote show origin* remote origin URL: git@github.com:defunkt/github.git Remote branch merged with &apos;git pull&apos; while on branch issues issues Remote branch merged with &apos;git pull&apos; while on branch master master New remote branches (next fetch will store in remotes/origin) caching Stale tracking branches (use &apos;git remote prune&apos;) libwalker walker2 Tracked remote branches acl apiv2 dashboard2 issues master postgres Local branch pushed with &apos;git push&apos; master:master 它告诉我们，运行 git push 时缺省推送的分支是什么（译注：最后两行）。它还显示了有哪些远端分支还没有同步到本地（译注：第六行的caching 分支），哪些已同步到本地的远端分支在远端服务器上已被删除（译注：Stale tracking branches 下面的两个分支），以及运行git pull 时将自动合并哪些分支（译注：前四行中列出的 issues 和 master 分支）。 远程仓库的删除和重命名在新版 Git 中可以用 git remote rename 命令修改某个远程仓库在本地的简短名称，比如想把 pb改成paul，可以这么运行： 1234$ git remote rename pb paul$ git remoteoriginpaul 注意，对远程仓库的重命名，也会使对应的分支名称发生变化，原来的 pb/master 分支现在成了 paul/master。 碰到远端仓库服务器迁移，或者原来的克隆镜像不再使用，又或者某个参与者不再贡献代码，那么需要移除对应的远端仓库，可以运行 git remote rm 命令： 123$ git remote rm paul$ git remoteorigin 打标签同大多数 VCS 一样，Git 也可以对某一时间点上的版本打上标签。人们在发布某个软件版本（比如 v1.0 等等）的时候，经常这么做。本节我们一起来学习如何列出所有可用的标签，如何新建标签，以及各种不同类型标签之间的差别。 列出已有的标签 列出现有标签的命令非常简单，直接运行 git tag 即可： 123$ git tagv0.1v1.3 显示的标签按字母顺序排列，所以标签的先后并不表示重要程度的轻重。 我们可以用特定的搜索模式列出符合条件的标签。在 Git 自身项目仓库中，有着超过 240 个标签，如果你只对 1.4.2 系列的版本感兴趣，可以运行下面的命令： 12345$ git tag -l &apos;v1.4.2.*&apos;v1.4.2.1v1.4.2.2v1.4.2.3v1.4.2.4 新建标签 Git 使用的标签有两种类型：轻量级的（lightweight）和含附注的（annotated）。轻量级标签就像是个不会变化的分支，实际上它就是个指向特 定提交对象的引用。而含附注标签，实际上是存储在仓库中的一个独立对象，它有自身的校验和信息，包含着标签的名字，电子邮件地址和日期，以及标签说明，标 签本身也允许使用 GNU Privacy Guard (GPG) 来签署或验证。一般我们都建议使用含附注型的标签，以便保留相关信息；当然，如果只是临时性加注标签，或者不需要旁注额外信息，用轻量级标签也没问题。 含附注的标签创建一个含附注类型的标签非常简单，用 -a （译注：取 annotated 的首字母）指定标签名字即可： 12345$ git tag -a v1.4 -m &apos;my version 1.4&apos;$ git tagv0.1v1.3v1.4 而 -m 选项则指定了对应的标签说明，Git 会将此说明一同保存在标签对象中。如果没有给出该选项，Git 会启动文本编辑软件供你输入标签说明。 可以使用 git show 命令查看相应标签的版本信息，并连同显示打标签时的提交对象。 12345678910111213$ git show v1.4tag v1.4Tagger: Scott Chacon &lt;schacon@gee-mail.com&gt; Date: Mon Feb 9 14:45:11 2009 -0800my version 1.4commit 15027957951b64cf874c3557a0f3547bd83b3ff6Merge: 4a447f7... a6b4c97...Author: Scott Chacon &lt;schacon@gee-mail.com&gt;Date: Sun Feb 8 19:02:46 2009 -0800 Merge branch &apos;experiment&apos; 我们可以看到在提交对象信息上面，列出了此标签的提交者和提交时间，以及相应的标签说明。 轻量级标签轻量级标签实际上就是一个保存着对应提交对象的校验和信息的文件。要创建这样的标签，一个 -a，-s 或 -m 选项都不用，直接给出标签名字即可： 1234567$ git tag v1.4-lw$ git tagv0.1v1.3v1.4v1.4-lwv1.5 现在运行 git show 查看此标签信息，就只有相应的提交对象摘要： 12345678910$ git show v1.4-lwcommit 15027957951b64cf874c3557a0f3547bd83b3ff6Merge: 4a447f7... a6b4c97...Author: Scott Chacon &lt;schacon@gee-mail.com&gt; Date: Sun Feb 8 19:02:46 2009 -0800 Merge branch &apos;experiment&apos; 验证标签可以使用 git tag -v [tag-name] （译注：取 verify 的首字母）的方式验证已经签署的标签。此命令会调用 GPG 来验证签名，所以你需要有签署者的公钥，存放在 keyring 中，才能验证： 123456789$ git tag -v v1.4.2.1object 883653babd8ee7ea23e6a5c392bb739348b1eb61type committag v1.4.2.1tagger Junio C Hamano &lt;junkio@cox.net&gt; 1158138501 -0700 GIT 1.4.2.1Minor fixes since 1.4.2, including git-mv and git-http with alternates.gpg: Signature made Wed Sep 13 02:08:25 2006 PDT using DSA key ID F3119B9Agpg: Good signature from &quot;Junio C Hamano &lt;junkio@cox.net&gt;&quot;gpg: aka &quot;[jpeg image of size 1513]&quot;Primary key fingerprint: 3565 2A26 2040 E066 C9A7 4A7D C0C6 D9A4 F311 9B9A 分享标签默认情况下，git push 并不会把标签传送到远端服务器上，只有通过显式命令才能分享标签到远端仓库。其命令格式如同推送分支，运行git push origin [tagname] 即可： 1234567$ git push origin v1.5Counting objects: 50, done.Compressing objects: 100% (38/38), done.Writing objects: 100% (44/44), 4.56 KiB, done.Total 44 (delta 18), reused 8 (delta 1)To git@github.com:schacon/simplegit.git* [new tag] v1.5 -&gt; v1.5 如果要一次推送所有本地新增的标签上去，可以使用 --tags 选项： 1234567891011$ git push origin --tagsCounting objects: 50, done.Compressing objects: 100% (38/38), done.Writing objects: 100% (44/44), 4.56 KiB, done.Total 44 (delta 18), reused 8 (delta 1)To git@github.com:schacon/simplegit.git * [new tag] v0.1 -&gt; v0.1 * [new tag] v1.2 -&gt; v1.2 * [new tag] v1.4 -&gt; v1.4 * [new tag] v1.4-lw -&gt; v1.4-lw * [new tag] v1.5 -&gt; v1.5 现在，其他人克隆共享仓库或拉取数据同步后，也会看到这些标签。 技巧和窍门在结束本章之前，我还想和大家分享一些 Git 使用的技巧和窍门。很多使用 Git 的开发者可能根本就没用过这些技巧，我们也不是说在读过本书后非得用这些技巧不可，但至少应该有所了解吧。说实话，有了这些小窍门，我们的工作可以变得更简单，更轻松，更高效。 自动补全如果你用的是 Bash shell，可以试试看 Git 提供的自动完成脚本。下载 Git 的源代码，进入 contrib/completion 目录，会看到一个git-completion.bash 文件。将此文件复制到你自己的用户主目录中（译注：按照下面的示例，还应改名加上点：cp git-completion.bash ~/.git-completion.bash），并把下面一行内容添加到你的.bashrc文件中： 1source ~/.git-completion.bash 也可以为系统上所有用户都设置默认使用此脚本。Mac 上将此脚本复制到 /opt/local/etc/bash_completion.d 目录中，Linux 上则复制到/etc/bash_completion.d/ 目录中。这两处目录中的脚本，都会在 Bash 启动时自动加载。 如果在 Windows 上安装了 msysGit，默认使用的 Git Bash 就已经配好了这个自动完成脚本，可以直接使用。 在输入 Git 命令的时候可以敲两次跳格键（Tab），就会看到列出所有匹配的可用命令建议： 12$ git co commit config 此例中，键入 git co 然后连按两次 Tab 键，会看到两个相关的建议（命令） commit 和 config。继而输入 m会自动完成git commit 命令的输入。 命令的选项也可以用这种方式自动完成，其实这种情况更实用些。比如运行 git log 的时候忘了相关选项的名字，可以输入开头的几个字母，然后敲 Tab 键看看有哪些匹配的： 123$ git log --s --shortstat --since= --src-prefix= --stat --summary 这个技巧不错吧，可以节省很多输入和查阅文档的时间。 Git命令别名Git 并不会推断你输入的几个字符将会是哪条命令，不过如果想偷懒，少敲几个命令的字符，可以用 git config 为命令设置别名。来看看下面的例子： 1234$ git config --global alias.co checkout$ git config --global alias.br branch$ git config --global alias.ci commit$ git config --global alias.st status 现在，如果要输入 git commit 只需键入 git ci 即可。而随着 Git 使用的深入，会有很多经常要用到的命令，遇到这种情况，不妨建个别名提高效率。 使用这种技术还可以创造出新的命令，比方说取消暂存文件时的输入比较繁琐，可以自己设置一下： 1$ git config --global alias.unstage &apos;reset HEAD --&apos; 这样一来，下面的两条命令完全等同： 12$ git unstage fileA$ git reset HEAD fileA 显然，使用别名的方式看起来更清楚。另外，我们还经常设置 last 命令： 1$ git config --global alias.last &apos;log -1 HEAD&apos; 然后要看最后一次的提交信息，就变得简单多了： 1234$ git lastcommit 66938dae3329c7aebe598c2246a8e6af90d04646Author: Josh Goebel &lt;dreamer3@example.com&gt;Date: Tue Aug 26 19:48:51 2008 +0800 可以看出，实际上 Git 只是简单地在命令中替换了你设置的别名。不过有时候我们希望运行某个外部命令，而非 Git 的附属工具，这个好办，只需要在命令前加上 ! 就行。如果你自己写了些处理 Git 仓库信息的脚本的话，就可以用这种技术包装起来。作为演示，我们可以设置用 git visual启动gitk： 1$ git config --global alias.visual &quot;!gitk&quot; 到目前为止，你已经学会了最基本的 Git 操作：创建和克隆仓库，做出更新，暂存并提交这些更新，以及查看所有历史更新记录。接下来，我们将学习 Git 的必杀技特性：分支模型。 转载链接Git细节拾遗]]></content>
      <categories>
        <category>git使用及讲解</category>
      </categories>
      <tags>
        <tag>git工具使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git的基础使用]]></title>
    <url>%2F2018%2F09%2F07%2Fgit%E5%92%8Csvn%E7%9A%84%E8%AF%A6%E7%BB%86%E5%AF%B9%E6%AF%94%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[Git是一个分布式的版本控制工具，本篇文章从介绍Git开始，重点在于介绍Git的基本命令和使用技巧，让你尝试使用Git的同时，体验到原来一个版 本控制工具可以对开发产生如此之多的影响，文章分为两部分，第一部分介绍Git的一些常用命令，其中穿插介绍Git的基本概念和原理，第二篇重点介绍 Git的使用技巧，最后会在Git Hub上创建一个开源项目开启你的Git实战之旅。 Git是什么Git在Wikipedia上的定义：它是一个免费的、分布式的版本控制工具，或是一个强调了速度快的源代码管理工具。Git最初被Linus Torvalds开发出来用于管理Linux内核的开发。每一个Git的工作目录都是一个完全独立的代码库，并拥有完整的历史记录和版本追踪能力，不依赖 于网络和中心服务器。 Git的出现减轻了许多开发者和开源项目对于管理分支代码的压力，由于对分支的良好控制，更鼓励开发者对自己感兴趣的项目做出贡献。其实许多开源项目 包括Linux kernel, Samba, X.org Server, Ruby on Rails，都已经过渡到使用Git作为自己的版本控制工具。对于我们这些喜欢写代码的开发者嘛，有两点最大的好处，我们可以在任何地点(在上班的地铁 上)提交自己的代码和查看代码版本;我们可以开许许多多个分支来实践我们的想法，而合并这些分支的开销几乎可以忽略不计。 Git 初始化 现在进入本篇文章真正的主题，介绍一下Git的基本命令和操作，会从Git的版本库的初始化，基本操作和独有的常用命令三部分着手，让大家能够开始使用Git。 Git通常有两种方式来进行初始化: git clone: 这是较为简单的一种初始化方式，当你已经有一个远程的Git版本库，只需要在本地克隆一份，例如’git clone git://github.com/someone/some_project.git some_project’命令就是将’git://github.com/someone/some_project.git’这个URL地址的远程版 本库完全克隆到本地some_project目录下面 git init和git remote：这种方式稍微复杂一些，当你本地创建了一个工作目录，你可以进入这个目录，使用 git init 命令进行初始化，Git以后就会对该目录下的文件进行版本控制，这时候如果你需要将它放到远程服务器上，可以在远程服务器上创建一个目录，并把 可访问的URL记录下来，此时你就可以利用 git remote add 命令来增加一个远程服务器端，例如’git remote add origin git://github.com/someone/another_project.git’这条命令就会增加URL地址为’git: //github.com/someone/another_project.git’，名称为origin的远程服务器，以后提交代码的时候只需要使用 origin别名即可 Git 基本命令 现在我们有了本地和远程的版本库，让我们来试着用用Git的基本命令吧： git pull：从版本库(既可以是远程的也可以是本地的)将代码更新到本地，例如：’git pull origin master’就是将origin这个版本库的代码更新到本地的master主枝，该功能类似于SVN的update git add：将所有改动的文件（新增和有变动的）放在暂存区，由git进行管理 git rm：从当前的工作空间中和索引中删除文件，例如’git rm app/model/user.rb’，移除暂存区 git commit：提交当前工作空间的修改内容，类似于SVN的commit命令，例如’git commit -m “story #3, add user model”‘，提交的时候必须用-m来输入一条提交信息 git push：将本地commit的代码更新到远程版本库中，例如’git push origin branchname’就会将本地的代码更新到名为orgin的远程版本库中 git log：查看历史日志 git revert：还原一个版本的修改，必须提供一个具体的Git版本号，例如’git revert bbaf6fb5060b4875b18ff9ff637ce118256d6f20’，Git的版本号都是生成的一个哈希值、 上面的命令几乎都是每个版本控制工具所公有的，下面就开始尝试一下Git独有的一些命令： Git 独有命令 git branch：对分支的增、删、查等操作，例如 git branch new_branch 会从当前的工作版本创建一个叫做new_branch的新分支，git branch -D new_branch 就会强制删除叫做new_branch的分支，git branch 就会列出本地所有的分支 git checkout：Git的checkout有两个作用，其一是在 不同的branch之间进行切换，例如 ‘git checkout new_branch’就会切换到new_branch的分支上去;另一个功能是 还原代码的作用，例如git checkout app/model/user.rb 就会将user.rb文件从上一个已提交的版本中更新回来，未提交的内容全部会回滚 git rebase：用下面两幅图解释会比较清楚一些，rebase命令执行后，实际上是将分支点从C移到了G，这样分支也就具有了从C到G的功能 （使历史更加简洁明了） git reset：回滚到指定的版本号，我们有A-G提交的版本，其中C 的版本号是 bbaf6fb，我们执行了’git reset bbaf6fb’那么结果就只剩下了A-C三个提交的版本 git stash：将当前未提交的工作存入Git工作栈中，时机成熟的时候再应用回来，这里暂时提一下这个命令的用法，后面在技巧篇会重点讲解 git config：新增、更改Git的各种设置，例如：git config branch.master.remote origin 就将master的远程版本库设置为别名叫做origin版本库 git tag：将某个版本打上一个标签，例如：git tag revert_version bbaf6fb50 来标记这个被你还原的版本，那么以后你想查看该版本时，就可以使用 revert_version标签名，而不是哈希值了 Git其他命令add #添加文件内容至索引 branch #列出、创建或删除分支 checkout #检出一个分支或路径到工作区 clone #克隆一个版本库到一个新目录 commit #最近一次的提交，–amend修改最近一次提交说明 diff #显示提交之间、提交和工作区之间等的差异 fetch #从另外一个版本库下载对象和引用 init #创建一个空的 Git 版本库或重新初始化一个已存在的版本库 log #显示提交日志 –stat 具体文件的改动 reflog #记录丢失的历史 merge #合并两个或更多开发历史，–squash 把分支所有提交合并成一个提交 mv #移动或重命名一个文件、目录或符号链接 pull #获取并合并另外的版本库或一个本地分支（相当于git fetch和git merge） push #更新远程引用和相关的对象 rebase #本地提交转移至更新后的上游分支中 reset #重置当前HEAD到指定状态 rm #从工作区和索引中删除文件 show #显示各种类型的对象 status #显示工作区状态 tag #创建、列出、删除或校验一个GPG签名的 tag 对象 cherry-pick #从其他分支复制指定的提交，然后导入到现在的分支 git分支命令创建分支： git branch linux #创建分支 git checkout linux #切换分支 git branch #查看当前分支情况,当前分支前有*号 git add readme.txt #提交到暂存区 git commit -m “new branch” #提交到git版本仓库 git checkout master #我们在提交文件后再切回master分支 分支合并：（合并前必须保证在master主干上） git branch #查看在哪个位置 git merge Linux #合并创建的Linux分支（–no–ff默认情况下，Git执行”快进式合并”（fast-farward merge），会直接将Master分支指向Develop分支。使用–no–ff参数后，会执行正常合并，在Master分支上生成一个新节点。） git branch -d linux #确认合并后删除分支 如果有冲突： git merge linux #合并Linux分支(冲突) Auto-merging readme.txt CONFLICT (content): Merge conflict in readme.txt Automatic merge failed; fix conflicts and then commit the result. 那么此时，我们在master与linux分支上都分别对中readme文件进行了修改并提交了，那这种情况下Git就没法再为我们自动的快速合并了，它只能告诉我们readme文件的内容有冲突，需要手工处理冲突的内容后才能继续合并 自己修改完readme.txt文件后再次提交 git全局配置1234567891011yum install git #安装Gitgit config –global user.name “xubusi” #配置git使用用户git config –global user.email “xubusi@mail.com” #配置git使用邮箱git config –global color.ui true #加颜色 git config –list #所有配置的信息（上面的结果）user.name=xubusiuser.email=xubusi@mail.comcolor.ui=true .git目录结构Git之所以能够提供方便的本地分支等特性，是与它的文件存储机制有关的。Git存储版本控制信息时使用它自己定义的一套文件系统存储机制，在代码根目录下有一个.git文件夹，会有如下这样的目录结构： 123456789HEADbranches/configdescriptionhooks/indexinfo/objects/refs/ 有几个比较重要的文件和目录需要解释一下： HEAD：文件存放根节点的信息，其实目录结构就表示一个树型结构，Git采用这种树形结构来存储版本信息， 那么HEAD就表示根; refs：目录存储了你在当前版本控制目录下的各种不同引用(引用指的是你本地和远程所用到的各个树分支的信息)，它有heads、 remotes、stash、tags四个子目录，分别存储对不同的根、远程版本库、Git栈和标签的四种引用，你可以通过命令’git show-ref’更清晰地查看引用信息; logs：目录根据不同的引用存储了日志信息。因此，Git只需要代码根目录下的这一个.git目录就可以记录完 整的版本控制信息，而不是像SVN那样根目录和子目录下都有.svn目录。那么下面就来看一下Git与SVN的区别吧 .gitigmore: 放一些不需要git管理的文件（例：IDE的工作目录 .idea，） git与svn的不同VN(Subversion)是当前使用最多的版本控制工具。与它相比较，Git最大的优势在于两点：易于本地增加分支和分布式的特性。 下面两幅图可以形象的展示Git与SVN的不同之处 GIT对于易于本地增加分支，图中Git本地和服务器端结构都很灵活，所有版本都存储在一个目录中，你只需要进行分支的切换即可达到在某个分支工作的效果。 SVN则完全不同，如果你需要在本地试验一些自己的代码，只能本地维护多个不同的拷贝，每个拷贝对应一个SVN服务器地址。 分布式对于Git而言，你可以本地提交代码，所以在上面的图中，Git有利于将一个大任务分解，进行本地的多次提交，而SVN只能在本地进行大量的一 次性更改，导致将来合并到主干上造成巨大的风险。Git的代码日志是在本地的，可以随时查看。SVN的日志在服务器上的，每次查看日志需要先从服务器上下 载下来。我工作的小组，代码服务器在美国，每次查看小组几年前所做的工作时，日志下载就需要十分钟，这不能不说是一个痛苦。后来我们迁移到Git上，利用 Git日志在本地的特性，我用Ruby编写了一个Rake脚本，可以查看某个具体任务的所有代码历史，每次只需要几秒钟，大大方便我的工作。当然分布式并 不是说用了Git就不需要一个代码中心服务器，如果你工作在一个团队里，还是需要一个服务器来保存所有的代码的。 实际的例子： 以前我所 在的小组使用SVN作为版本控制工具，当我正在试图增强一个模块，工作做到一半，由于会改变原模块的行为导致代码服务器上许多测试的失败，所以并没有提交 代码。这时候上级对我说，现在有一个很紧急的Bug需要处理， 必须在两个小时内完成。我只好将本地的所有修改diff，并输出成为一个patch文件，然后回滚有关当前任务的所有代码，再开始修改Bug的任务，等到 修改好后，在将patch应用回来。前前后后要完成多个繁琐的步骤，这还不计中间代码发生冲突所要进行的工作量。 可是如果使用Git， 我们只需要开一个分支或者转回到主分支上，就可以随时开始Bug修改的任务，完成之后，只要切换到原来的分支就可以优雅的继续以前的任务。只要你愿意，每 一个新的任务都可以开一个分支，完成后，再将它合并到主分支上，轻松而优雅。 gitlab介绍安装服务相关命令安装有可能的依赖： yum install openssh-server yum install postfix yum install cronie 安装gitlab： curl -sS https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh #下载数据源 yum install gitlab-ce 安装完成后： gitlab-ctl reconfigure #使配置文件生效 但是会初始化除了gitlab.rb之外的所有文件 gitlab-ctl status #查看状态 gitlab-ctl stop #停服务 gitlab-ctl start #起服务 gitlab-ctl tail #查看日志的命令（Gitlab 默认的日志文件存放在/var/log/gitlab 目录下） 如下表示启动成功：（全是run，有down表示有的服务没启动成功） 然后打开浏览器输入ip或者域名 相关目录.git/config #版本库特定的配置设置，可用–file修改 ~/.gitconfig #用户特定的配置设置，可用–global修改 /var/opt/gitlab/git-data/repositories/root #库默认存储目录 /opt/gitlab #是gitlab的应用代码和相应的依赖程序 /var/opt/gitlab #此目录下是运行gitlab-ctl reconfigure命令编译后的应用数据和配置文件，不需要人为修改配置/etc/gitlab #此目录下存放了以omnibus-gitlab包安装方式时的配置文件，这里的配置文件才需要管理员手动编译配置/var/log/gitlab #此目录下存放了gitlab各个组件产生的日志 /var/opt/gitlab/backups/ #备份文件生成的目录 相关文件/opt/gitlab/embedded/service/gitlab-rails/config #配置文件（修改clone的ip地址） /etc/gitlab/gitlab.rb #设置相关选项进行配置（gitlab地址就在这） /var/opt/gitlab/git-data #Git存储库数据（默认) 转载链接Git使用基础篇]]></content>
      <categories>
        <category>git使用及讲解</category>
      </categories>
      <tags>
        <tag>git工具使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git和svn的详细对比表]]></title>
    <url>%2F2018%2F09%2F07%2Fgit%E7%9A%84%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[刚开始工作那会，工作做版本控制的选型，几个tl最后选的git，当时不是很懂，只知道git性能多一些，现在回头看了一下这个表格，更加明白他们之间的差异了，git完胜！ 版本工具差异 svn git 系统特点 1.集中式版本控制系统（文档管理很方便）2.企业内部并行集中开发3.windows系统上开发推荐使用4.克隆一个拥有将近一万个提交(commit),五个分支,每个分支有大约1500个文件，用时将近一个小时 1.分布式系统（代码管理很方便）2.开源项目开发3.mac,Linux系统上开发推荐使用4.克隆一个拥有将近一万个提交(commit),五个分支,每个分支有大约1500个文件，用时1分钟 灵活性 1.搭载svn的服务器出现故障，无法与之交互2.所有的svn操作都需要中央仓库交互（例：拉分支，看日志等） 1.可以单机操作，git服务器故障也可以在本地git仓库工作2.除了push和pull（或fetch）操作，其他都可以在本地操作3.根据自己开发任务任意在本地创建分支4.日志都是在本地查看，效率较高 安全性 较差，定期备份，并且是整个svn都得备份 较高，每个开发者的本地就是一套完整版本库，记录着版本库的所有信息（gitlab集成了备份功能） 分支方面 1.拉分支更像是copy一个路径2.可针对任何子目录进行branch3.拉分支的时间较慢，因为拉分支相当于copy4.创建完分支后，影响全部成员，每个人都会拥有这个分支5.多分支并行开发较重（工作较多而且繁琐） 1.我可以在Git的任意一个提交点（commit point）开启分支！（git checkout -b newbranch HashId）2.拉分支时间较快，因为拉分支只是创建文件的指针和HEAD3.自己本地创建的分支不会影响其他人4.比较适合多分支并行开发5.git checkout hash值(切回之前的版本，无需版本回退)6.强大的cherry-pick 版本控制 1.保存前后变化的差异数据，作为版本控制2.版本号进行控制，每次操作都会产生一个高版本号（svn的全局版本号，这是svn一个较大的特点，git是hash值） 1.git只关心文件数据的整体发生变化，更像是把文件做快照，文件没有改变时，分支只想这个文件的指针不会改变，文件发生改变，指针指向新版本2. 40 位长的哈希值作为版本号，没有先后之分3.git rebase操作可以更好的保持提交记录的整洁 工作流程 1.每次更改文件之前都得update操作，有的时候修改过程中这个文件有更新，commit不会成功2.有冲突，会打断提交动作（冲突解决是一个提交速度的竞赛：手快者，先提交，平安无事；手慢者，后提交，可能遇到麻烦的冲突解决。） 1.开始工作前进行fetch操作，完成开发工作后push操作，有冲突解决冲突2.git的提交过程不会被打断，有冲突会标记冲突文件3.gitflow流程（经典） 内容管理 svn对中文支持好，操作简单，适用于大众 对程序的源代码管理方便，代码库占用的空间少，易于分支化管理 学习成本 使用起来更方便，svn对中文支持好，操作简单，适用于大众 更在乎效率而不是易用性，成本较高（有很多独有的命令，rebase，远程仓库交互的命令，等等） 权限管理 svn的权限管理相当严格，可以按组、个人针对某个子目录的权限控制（每个目录下都会有个.svn的隐藏文件） git没有严格的权限管理控制，只有账号角色划分（在项目的home文件下有且只有一个.svn目录） 管理平台 有吧（这个“吧”字，肯定有，但本人没有接触过） gitlab（建议使用，集成的功能较多，API开发），gerrit，github等 转载链接： git和svn的详细对比]]></content>
      <categories>
        <category>git使用及讲解</category>
      </categories>
      <tags>
        <tag>git工具使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github向导]]></title>
    <url>%2F2018%2F09%2F07%2Fgithub%E5%90%91%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[Hello World项目是计算机编程的悠久传统。这是一个简单的练习，让你开始学习新的东西。让我们开始使用GitHub！ 你将学到如下内容： 1: 创建和使用一个仓库。 2: 开始和管理一个分支。 3: 更改一个文件，然后推送到仓库，并且附带一些注释。 4: 打开和合并一个推送请求。 创建一个仓库仓库用来组织一个单一的项目，可以包含目录和文件，图片，视频，表格和数据集等所有项目所需要的内容。建议增加一个README文件用来描述项目相关信息。github可以直接生成一个空的README文件。 1: 进入github，在右上角找到+号，然后选择新建项目。 2: 输入项目名称，比如wuman-Small-projects。 3: 写一些简短的描述。 4: 选择可见等级； 5: 单击创建项目，即可完成创建。 创建完成后，如果是空的项目，会显示一个命令列表，以帮助用户通过git进行操作： 命令行指令 Git 全局设置 12git config --global user.name &quot;wumansgy&quot;git config --global user.email &quot;wumansgy@wumansgy.com&quot; 创建新版本库 123456git clone https://github.com/wumansgy/wuman-Small-projects.gitcd wuman-Small-projectstouch README.mdgit add README.mdgit commit -m &quot;add README&quot;git push -u origin master 已存在的文件夹 123456cd existing_foldergit initgit remote add origin https://github.com/wumansgy/wuman-Small-projects.gitgit add .git commitgit push -u origin master 已存在的 Git 版本库 1234cd existing_repogit remote add origin https://github.com/wumansgy/wuman-Small-projects.gitgit push -u origin --allgit push -u origin --tags 创建一个分支github上默认分支为master。并且还提供了将分支合并的功能。 1: 打开wuman-Small-projects仓库首页。 2: 在项目名称之后单击+号，弹出菜单，并选择新分支，赚到分支创建页。 3：输入分支名称，单击绿色创建分支按钮，即可创建成功。 4: 创建成功后，回到wuman-Small-projects项目首页，可以看到新创建的分支。 更改和提交更新1: 在wuman-Small-projects项目首页，在对应项目名称的后面单击+号，弹出菜单，并选择新文件（也可以选择上传文件以上传一个新的本地文件，或者单击新目录以创建一个新目录）。 或者如果有文件存在，打开对应的文件，然后单击编辑按钮，以开始编辑一个存在的文件。 2: 我们以新文件为例，如下图，输入文件名称，文件内容，并且在下方输入注释，然后单击提交修改即可完成新文件或者修改文件的功能： 开启一个推送请求如果将某个分支的更改情况推送到另外一个分支，或者master，需要提交一个推送请求。 1: 打开wuman-Small-projects项目首页，单击最上头的合并请求。 2: 单击绿色的新建合并请求。 3: 选择来源分支（即当前分支newbranch）与目标分支（比如master），单击比较分支后继续。 4: 填写标题和描述，确定来源分支和目标分支，以及确定最下方的提交和变更内容，最后单击绿色的提交新的合并请求。 合并一个推送请求经过步骤3之后，项目的所有者或者在上述步骤中指定了指派人，会收到一个合并请求的通知。 当确认后，会进行具体的合并过程。 此过程，也可以通过命令行来完成，具体过程如下 检出，在本地审查和合并 Step 1. 获取并检出此合并请求的分支 12git fetch origingit checkout -b newbranch origin/newbranch Step 2. 本地审查变更 Step 3. 合并分支并修复出现的任何冲突 Step 4. 推送合并的结果到 GitLab 1git push origin master 常用的命令行功能1: 更新 12$ git fetch origin 更新主分支的更新$ git fetch 更新所有内容 2: 克隆 1$ git clone https://github.com/wumansgy/wuman-Small-projects.git 3: 在某个分支上克隆 1$ git clone -b newbranch https://github.com/wumansgy/wuman-Small-projects.git 4: 合并 1$ git merge origin/master 5: 更新，然后合并 1$ git pull 6: 添加文件 1$ git add [file.name](http://file.name) 7: 删除文件 1$ git rm [file.name](http://file.name) 8: 添加注释 1$ git commit -m ‘add a new file’ 9: 推送更改 1$ git push -u origin/master]]></content>
      <categories>
        <category>git使用及讲解</category>
      </categories>
      <tags>
        <tag>git工具使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP通信三次握手四次挥手]]></title>
    <url>%2F2018%2F09%2F06%2FTcp%E9%80%9A%E4%BF%A1%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%2F</url>
    <content type="text"><![CDATA[TCP通信过程下图是一次TCP通讯的时序图。TCP连接建立断开。包含大家熟知的三次握手和四次握手 在这个例子中，首先客户端主动发起连接、发送请求，然后服务器端响应请求，然后客户端主动关闭连接。两条竖线表示通讯的两端，从上到下表示时间的先后顺序。注意，数据从一端传到网络的另一端也需要时间，所以图中的箭头都是斜的。 三次握手：所谓三次握手（Three-Way Handshake）即建立TCP连接，就是指建立一个TCP连接时，需要客户端和服务端总共发送3个包以确认连接的建立。好比两个人在打电话：Client:“喂，你听得到吗？”Server:“我听得到，你听得到我吗？”Client:“我能听到你，今天balabala…” 建立连接（三次握手）的过程：1.客户端发送一个带SYN标志的TCP报文到服务器。这是上图中三次握手过程中的段1。客户端发出SYN位表示连接请求。序号是1000，这个序号在网络通讯中用作临时的地址，每发一个数据字节，这个序号要加1，这样在接收端可以根据序号排出数据包的正确顺序，也可以发现丢包的情况。另外，规定SYN位和FIN位也要占一个序号，这次虽然没发数据，但是由于发了SYN位，因此下次再发送应该用序号1001。mss表示最大段尺寸，如果一个段太大，封装成帧后超过了链路层的最大长度，就必须在IP层分片，为了避免这种情况，客户端声明自己的最大段尺寸，建议服务器端发来的段不要超过这个长度。2.服务器端回应客户端，是三次握手中的第2个报文段，同时带ACK标志和SYN标志。表示对刚才客户端SYN的回应；同时又发送SYN给客户端，询问客户端是否准备好进行数据通讯。服务器发出段2，也带有SYN位，同时置ACK位表示确认，确认序号是1001，表示“我接收到序号1000及其以前所有的段，请你下次发送序号为1001的段”，也就是应答了客户端的连接请求，同时也给客户端发出一个连接请求，同时声明最大尺寸为1024。3.客户必须再次回应服务器端一个ACK报文，这是报文段3。客户端发出段3，对服务器的连接请求进行应答，确认序号是8001。在这个过程中，客户端和服务器分别给对方发了连接请求，也应答了对方的连接请求，其中服务器的请求和应答在一个段中发出。因此一共有三个段用于建立连接，称为“三方握手”。在建立连接的同时，双方协商了一些信息，例如，双方发送序号的初始值、最大段尺寸等。数据传输的过程：1.客户端发出段4，包含从序号1001开始的20个字节数据。2.服务器发出段5，确认序号为1021，对序号为1001-1020的数据表示确认收到，同时请求发送序号1021开始的数据，服务器在应答的同时也向客户端发送从序号8001开始的10个字节数据。3.客户端发出段6，对服务器发来的序号为8001-8010的数据表示确认收到，请求发送序号8011开始的数据。在数据传输过程中，ACK和确认序号是非常重要的，应用程序交给TCP协议发送的数据会暂存在TCP层的发送缓冲区中，发出数据包给对方之后，只有收到对方应答的ACK段才知道该数据包确实发到了对方，可以从发送缓冲区中释放掉了，如果因为网络故障丢失了数据包或者丢失了对方发回的ACK段，经过等待超时后TCP协议自动将发送缓冲区中的数据包重发。 四次挥手：所谓四次挥手（Four-Way-Wavehand）即终止TCP连接，就是指断开一个TCP连接时，需要客户端和服务端总共发送4个包以确认连接的断开。在socket编程中，这一过程由客户端或服务器任一方执行close来触发。好比两个人打完电话要挂断：Client:“我要说的事情都说完了，我没事了。挂啦？”Server:“等下，我还有一个事儿。Balabala…”Server:“好了，我没事儿了。挂了啊。”Client:“ok！拜拜”关闭连接（四次握手）的过程：由于TCP连接是全双工的，因此每个方向都必须单独进行关闭。这原则是当一方完成它的数据发送任务后就能发送一个FIN来终止这个方向的连接。收到一个 FIN只意味着这一方向上没有数据流动，一个TCP连接在收到一个FIN后仍能发送数据。首先进行关闭的一方将执行主动关闭，而另一方执行被动关闭。1.客户端发出段7，FIN位表示关闭连接的请求。2.服务器发出段8，应答客户端的关闭连接请求。3.服务器发出段9，其中也包含FIN位，向客户端发送关闭连接请求。4.客户端发出段10，应答服务器的关闭连接请求。建立连接的过程是三次握手，而关闭连接通常需要4个段，服务器的应答和关闭连接请求通常不合并在一个段中，因为有连接半关闭的情况，这种情况下客户端关闭连接之后就不能再发送数据给服务器了，但是服务器还可以发送数据给客户端，直到服务器也关闭连接为止。 这就是简单的3次握手和四次挥手的讲解，如果可以理解记住如下状态那就更好 总结过程：TCP状态转换： 1. 主动端：CLOSE –&gt; SYN –&gt; SYN_SEND状态 –&gt; ESTABLISHED状态（数据通信期间处于的状态） —&gt; FIN –&gt; FIN_WAIT_1状态。—&gt; 接收 ACK —&gt; FIN_WAIT_2状态 (半关闭—— 只出现在主动端) —&gt; 接收FIN、回ACK ——&gt; TIME_WAIT (等2MSL)—&gt; 确保最后一个ACK能被对端收到。(只出现在主动端)2. 被动端：CLOSE –&gt; LISTEN —&gt; ESTABLISHED状态（数据通信期间处于的状态） —&gt; 接收 FIN、回复ACK –&gt; CLOSE_WAIT(对应 对端处于 半关闭) –&gt; 发送FIN –&gt; LAST_ACK —&gt; 接收ACK —&gt; CLOSE]]></content>
      <categories>
        <category>网络编程协议</category>
      </categories>
      <tags>
        <tag>TCP/IP</tag>
        <tag>通信协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通信Socket编程]]></title>
    <url>%2F2018%2F09%2F06%2F%E9%80%9A%E4%BF%A1Socket%E7%BC%96%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Socket编程什么是Socket：Socket，英文含义是【插座、插孔】，一般称之为套接字，用于描述IP地址和端口。可以实现不同程序间的数据通信。Socket起源于Unix，而Unix基本哲学之一就是“一切皆文件”，都可以用“打开open –&gt; 读写write/read –&gt; 关闭close”模式来操作。Socket就是该模式的一个实现，网络的Socket数据传输是一种特殊的I/O，Socket也是一种文件描述符。Socket也具有一个类似于打开文件的函数调用：Socket()，该函数返回一个整型的Socket描述符，随后的连接建立、数据传输等操作都是通过该Socket实现的。套接字的内核实现较为复杂，不宜在学习初期深入学习，了解到如下结构足矣。 套接字通讯原理示意在TCP/IP协议中，“IP地址+TCP或UDP端口号”唯一标识网络通讯中的一个进程。“IP地址+端口号”就对应一个socket。欲建立连接的两个进程各自有一个socket来标识，那么这两个socket组成的socket pair就唯一标识一个连接。因此可以用Socket来描述网络连接的一对一关系。常用的Socket类型有两种：流式Socket（SOCK_STREAM）和数据报式Socket（SOCK_DGRAM）。流式是一种面向连接的Socket，针对于面向连接的TCP服务应用；数据报式Socket是一种无连接的Socket，对应于无连接的UDP服务应用。 网络应用程序设计模式C/S模式传统的网络应用设计模式，客户机(client)/服务器(server)模式。需要在通讯两端各自部署客户机和服务器来完成数据通信。B/S模式浏览器(Browser)/服务器(Server)模式。只需在一端部署服务器，而另外一端使用每台PC都默认配置的浏览器即可完成数据的传输。优缺点对于C/S模式来说，其优点明显。客户端位于目标主机上可以保证性能，将数据缓存至客户端本地，从而提高数据传输效率。且，一般来说客户端和服务器程序由一个开发团队创作，所以他们之间所采用的协议相对灵活。可以在标准协议的基础上根据需求裁剪及定制。例如，腾讯所采用的通信协议，即为ftp协议的修改剪裁版。因此，传统的网络应用程序及较大型的网络应用程序都首选C/S模式进行开发。如，知名的网络游戏魔兽世界。3D画面，数据量庞大，使用C/S模式可以提前在本地进行大量数据的缓存处理，从而提高观感。C/S模式的缺点也较突出。由于客户端和服务器都需要有一个开发团队来完成开发。工作量将成倍提升，开发周期较长。另外，从用户角度出发，需要将客户端安插至用户主机上，对用户主机的安全性构成威胁。这也是很多用户不愿使用C/S模式应用程序的重要原因。B/S模式相比C/S模式而言，由于它没有独立的客户端，使用标准浏览器作为客户端，其工作开发量较小。只需开发服务器端即可。另外由于其采用浏览器显示数据，因此移植性非常好，不受平台限制。如早期的偷菜游戏，在各个平台上都可以完美运行。B/S模式的缺点也较明显。由于使用第三方浏览器，因此网络应用支持受限。另外，没有客户端放到对方主机上，缓存数据不尽如人意，从而传输数据量受到限制。应用的观感大打折扣。第三，必须与浏览器一样，采用标准http协议进行通信，协议选择不灵活。因此在开发过程中，模式的选择由上述各自的特点决定。根据实际需求选择应用程序设计模式。TCP的C/S架构 简单的C/S模型通信 Server端： 12345678910Listen函数： func Listen(network, address string) (Listener, error) network：选用的协议：TCP、UDP， 如：“tcp”或 “udp” address：IP地址+端口号, 如：“127.0.0.1:8000”或 “:8000”Listener 接口：type Listener interface &#123; Accept() (Conn, error) Close() error Addr() Addr&#125; Conn 接口： 12345678910type Conn interface &#123; Read(b []byte) (n int, err error) Write(b []byte) (n int, err error) Close() error LocalAddr() Addr RemoteAddr() Addr SetDeadline(t time.Time) error SetReadDeadline(t time.Time) error SetWriteDeadline(t time.Time) error&#125; 参看 https://studygolang.com/pkgdoc 中文帮助文档中的demo： 示例代码： TCP服务器.go 1234567891011121314151617181920212223242526272829303132333435package mainimport ( &quot;net&quot; &quot;fmt&quot;)func main() &#123; // 创建监听 listener, err:= net.Listen(&quot;tcp&quot;, &quot;:8000&quot;) if err != nil &#123; fmt.Println(&quot;listen err:&quot;, err) return &#125; defer listener.Close() // 主协程结束时，关闭listener fmt.Println(&quot;服务器等待客户端建立连接...&quot;) // 等待客户端连接请求 conn, err := listener.Accept() if err != nil &#123; fmt.Println(&quot;accept err:&quot;, err) return &#125; defer conn.Close() // 使用结束，断开与客户端链接 fmt.Println(&quot;客户端与服务器连接建立成功...&quot;) // 接收客户端数据 buf := make([]byte, 1024) // 创建1024大小的缓冲区，用于read n, err := conn.Read(buf) if err != nil &#123; fmt.Println(&quot;read err:&quot;, err) return &#125; fmt.Println(&quot;服务器读到:&quot;, string(buf[:n])) // 读多少，打印多少。&#125; 如图，在整个通信过程中，服务器端有两个socket参与进来，但用于通信的只有 conn 这个socket。它是由 listener创建的。隶属于服务器端。 Client 端： 123func Dial(network, address string) (Conn, error) network：选用的协议：TCP、UDP，如：“tcp”或 “udp” address：服务器IP地址+端口号, 如：“121.36.108.11:8000”或 “www.itcast.cn:8000” Conn 接口： 12345678910type Conn interface &#123; Read(b []byte) (n int, err error) Write(b []byte) (n int, err error) Close() error LocalAddr() Addr RemoteAddr() Addr SetDeadline(t time.Time) error SetReadDeadline(t time.Time) error SetWriteDeadline(t time.Time) error&#125; 客户端实现 1234567891011121314151617181920212223package mainimport ( "net" "fmt")func main() &#123; // 主动发起连接请求 conn, err := net.Dial("tcp", "127.0.0.1:8000") if err != nil &#123; fmt.Println("Dial err:", err) return &#125; defer conn.Close() // 结束时，关闭连接 // 发送数据 _, err = conn.Write([]byte("Are u ready?")) if err != nil &#123; fmt.Println("Write err:", err) return &#125;&#125; 下一章节将讲解一下并发模型的服务器，以及TCP通信过程，还有UDP服务器的讲解；]]></content>
      <categories>
        <category>网络编程协议</category>
      </categories>
      <tags>
        <tag>通信协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构--散列表（哈希表）2]]></title>
    <url>%2F2018%2F09%2F06%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%95%A3%E5%88%97%E8%A1%A8%E5%93%88%E5%B8%8C%E8%A1%A82%2F</url>
    <content type="text"><![CDATA[本文主要采用： 构造方法：除留余数法： f(key)=key%p (P&lt;=m m:散列表的长度) 处理散列冲突方法：链地址法（单链表） 代码实例：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125package main import &quot;fmt&quot; /* 除留余数发定址 线性探测发解决冲突*/type keyType int //key值的类型type valueType int //value值的类型const maxSize = 12 //hastable的最大长度//除留余数发： f(key)=key%p (P&lt;=m m:散列表的长度)var p = 11var nullKey = keyType(-65535)var nullValue = valueType(-65535) //无效的数据 type hashData struct &#123; key keyType //key值 value valueType //value值 next *hashData&#125; var hashTable [maxSize]hashData //定义哈希表，大小为maxSize，类型为：hashData//初始化哈希表func initHashTable() &#123; for i := 0; i &lt; len(hashTable); i++ &#123; // 头结点 p := new(hashData) p.key = nullKey //空值 p.value = nullValue //空值 p.next = nil dataNode := hashData&#123;keyType(nullKey), valueType(nullValue), p&#125; hashTable[i] = dataNode &#125;&#125; //向哈希表添加数据元素func insertHashTable(ht *[maxSize]hashData, key keyType, value valueType) bool &#123; //先查找，如果key值已经存在，则替换成key新对应的value data:=searchHashTable(ht,key) if data!=nil&#123; //已经存在 data.value=value return true &#125; addr := int(key) % p //开辟空间 新建节点 q := new(hashData) q.key = key //赋 key值 q.value = value //赋value值 r := ht[addr].next //找到最末尾元素 for r.next != nil &#123; r = r.next &#125; q.next = r.next r.next = q return true&#125; //查找数据func searchHashTable(ht *[maxSize]hashData, key keyType) (data *hashData) &#123; //按照添加的位置 找对应的数据（链表），而不是对数组遍历 addr := int(key) % p //fmt.Println(&quot;------------&quot;,addr) data = nil q := ht[addr].next for q != nil &#123; if q.key == key &#123; return q //如果找个，返回对应的节点（指针指向此节点） &#125; q = q.next //移动到下一个位置继续 &#125; return&#125; //删除数据func deleteHashTable(ht *[maxSize]hashData, key keyType) bool &#123; addr := int(key) % p q := ht[addr].next r := q.next for r != nil &#123; //删除节点 if r.key == key &#123; q.next = r.next return true &#125; q = r r = q.next &#125; return false&#125;func main() &#123; //初始化哈希表 initHashTable() //添加数据 // 元素0对应的数据 insertHashTable(&amp;hashTable, 0, 48) m := searchHashTable(&amp;hashTable, 0) if m != nil &#123; fmt.Printf(&quot;【%d】：%d\n&quot;, m.key, m.value) &#125; else &#123; fmt.Println(&quot;没有查找到数据！&quot;) &#125; //添加key已经存在的数据 insertHashTable(&amp;hashTable,0,666) m = searchHashTable(&amp;hashTable, 0) if m != nil &#123; fmt.Printf(&quot;【%d】：%d\n&quot;, m.key, m.value) &#125; else &#123; fmt.Println(&quot;没有查找到数据！&quot;) &#125; //删除数据元素 deleteHashTable(&amp;hashTable, 0) //删除之后 再次查找 m = searchHashTable(&amp;hashTable, 0) if m != nil &#123; fmt.Printf(&quot;【%d】：%d\n&quot;, m.key, m.value) &#125; else &#123; fmt.Println(&quot;没有查找到数据！&quot;) &#125; &#125;]]></content>
      <categories>
        <category>数据结构和算法</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构--散列表（哈希表）1]]></title>
    <url>%2F2018%2F09%2F06%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E6%95%A3%E5%88%97%E8%A1%A8%E5%93%88%E5%B8%8C%E8%A1%A81%2F</url>
    <content type="text"><![CDATA[本文主要采用： 构造方法：除留余数法： f(key)=key%p (P&lt;=m m:散列表的长度) 处理散列冲突方法：线性探测法 代码实例：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112package main import &quot;fmt&quot; /* 除留余数发定址 线性探测发解决冲突*/type keyType int //key值的类型type valueType int //value值的类型 const maxSize = 12 //hastable的最大长度//除留余数发： f(key)=key%p (P&lt;=m m:散列表的长度)var p = 11var nullValue = keyType(-65535) //无效的数据 type hashData struct &#123; key keyType //key值 value valueType //value值 //count int //探查次数&#125; var hashTable [maxSize]hashData //定义哈希表，大小为maxSize，类型为：hashData//初始化哈希表func initHashTable() &#123; for i:=0;i&lt;len(hashTable);i++&#123; dataNode:=hashData&#123;nullValue,0,&#125; hashTable[i]=dataNode &#125;&#125;//向哈希表添加数据元素func insertHashTable(ht *[maxSize]hashData, key keyType, value valueType) bool&#123; count := 0 //添加时 查找次数 addr := int(key) % p //线性探测发： f(key)=( f(key)+d)%m (d=1,d=2,d=3...) d := 0 for count &lt; maxSize &#123; addr := (addr + d) % p if ht[addr].key ==nullValue &#123; dataNode := hashData&#123;key, value, &#125; ht[addr] = dataNode return true &#125; d++ count++ &#125; return false&#125;//查找数据func searchHashTable( ht *[maxSize]hashData,key keyType) (positon int) &#123; //addr:=int(key)%p positon=-1 //没有找到，返回值为：-1 for index,data:=range ht&#123; if data.key==key &#123; positon=index return //返回对应于数组的下标 &#125; &#125; return&#125;//删除数据func deleteHashTable(ht *[maxSize]hashData,key keyType) bool &#123; for index,data:=range ht&#123; if data.key==key &#123; /* 特别注意;这种方法不能更改数据 data.key=nullValue //重置key data.value=0 //清空数据 */ dataNode:=hashData&#123;nullValue,0&#125; ht[index]=dataNode return true &#125; &#125; return false &#125;func main() &#123; //初始化哈希表 initHashTable() //向哈希表中添加10个数据 for i:=0;i&lt;10;i++&#123; insertHashTable(&amp;hashTable,keyType(i),valueType(i*100)) &#125; fmt.Println(hashTable) //打印，是否添加成功 //查找 index:=searchHashTable(&amp;hashTable,keyType(3)) if index==-1 &#123; fmt.Println(&quot;没有查询到对应的数据&quot;) &#125;else &#123; value:=hashTable[index].value fmt.Printf(&quot;key：%d对应的value：%d\n&quot;,index,value) &#125; //删除数据元素 deleteState:=deleteHashTable(&amp;hashTable,keyType(3)) if deleteState &#123; fmt.Println(&quot;找到对应的元素，删除成功！&quot;) &#125;else &#123; fmt.Println(&quot;没有找到元素，删除失败！&quot;) &#125; //再次查找，看是否真正删除 index1:=searchHashTable(&amp;hashTable,keyType(3)) if index1==-1 &#123; fmt.Println(&quot;没有查询到对应的数据&quot;) &#125;else &#123; value:=hashTable[index1].value fmt.Printf(&quot;key：%d对应的value：%d&quot;,index1,value) &#125; //fmt.Println(hashTable)&#125;]]></content>
      <categories>
        <category>数据结构和算法</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字符二进制转换]]></title>
    <url>%2F2018%2F09%2F05%2F%E5%AD%97%E7%AC%A6%E4%BA%8C%E8%BF%9B%E5%88%B6%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[字符二进制转换运用位操作左移和右移来实现字符二进制转换的一个源码（自己也可以去实现） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112package Btosimport ( "errors" "regexp")const ( zero = byte('0') one = byte('1') lsb = byte('[') // left square brackets rsb = byte(']') // right square brackets space = byte(' '))var uint8arr [8]uint8 //// ErrBadStringFormat represents a error of input string's format is illegal .var ErrBadStringFormat = errors.New("bad string format")// ErrEmptyString represents a error of empty input string.var ErrEmptyString = errors.New("empty string")func init() &#123; uint8arr[0] = 128 uint8arr[1] = 64 uint8arr[2] = 32 uint8arr[3] = 16 uint8arr[4] = 8 uint8arr[5] = 4 uint8arr[6] = 2 uint8arr[7] = 1&#125;// append bytes of string in binary format.func appendBinaryString(bs []byte, b byte) []byte &#123; var a byte for i := 0; i &lt; 8; i++ &#123; a = b b &lt;&lt;= 1 b &gt;&gt;= 1 switch a &#123; case b: bs = append(bs, zero) default: bs = append(bs, one) &#125; b &lt;&lt;= 1 &#125; return bs&#125;// ByteToBinaryString get the string in binary format of a byte or uint8.func ByteToBinaryString(b byte) string &#123; buf := make([]byte, 0, 8) buf = appendBinaryString(buf, b) return string(buf)&#125;// BytesToBinaryString get the string in binary format of a []byte or []int8.func BytesToBinaryString(bs []byte) string &#123; l := len(bs) bl := l*8 + l + 1 buf := make([]byte, 0, bl) buf = append(buf, lsb) for _, b := range bs &#123; buf = appendBinaryString(buf, b) buf = append(buf, space) &#125; buf[bl-1] = rsb return string(buf)&#125;// regex for delete useless string which is going to be in binary format.var rbDel = regexp.MustCompile(`[^01]`)// BinaryStringToBytes get the binary bytes according to the// input string which is in binary format.func BinaryStringToBytes(s string) (bs []byte) &#123; if len(s) == 0 &#123; panic(ErrEmptyString) &#125; s = rbDel.ReplaceAllString(s, "") l := len(s) if l == 0 &#123; panic(ErrBadStringFormat) &#125; mo := l % 8 l /= 8 if mo != 0 &#123; l++ &#125; bs = make([]byte, 0, l) mo = 8 - mo var n uint8 for i, b := range []byte(s) &#123; m := (i + mo) % 8 switch b &#123; case one: n += uint8arr[m] &#125; if m == 7 &#123; bs = append(bs, n) n = 0 &#125; &#125; return&#125;]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>二进制转换</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go语言贪食蛇和c语言区别]]></title>
    <url>%2F2018%2F09%2F05%2Fgo%E8%AF%AD%E8%A8%80%E8%B4%AA%E9%A3%9F%E8%9B%87%E5%92%8Cc%E8%AF%AD%E8%A8%80%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[CSDN博客链接 GO贪食蛇小Demo利用go语言写贪食蛇游戏那么就会利用面向对象的思想来写一下，创造蛇身体对象，然后写出来，go语言写的时候我们需要调用一个c语言写的一个包，go语言可以直接调用调用c语言的函数，很方便简洁，我们先来看一下我自己写的C语言的一个包 1234567891011121314151617181920212223242526272829303132333435363738394041424344package Clib/*#include &lt;windows.h&gt;#include &lt;conio.h&gt;// 使用了WinAPI来移动控制台的光标void gotoxy(int x,int y)&#123; COORD c; c.X=x,c.Y=y; SetConsoleCursorPosition(GetStdHandle(STD_OUTPUT_HANDLE),c);&#125;// 从键盘获取一次按键，但不显示到控制台int direct()&#123; return _getch();&#125;//去掉控制台光标void hideCursor()&#123; CONSOLE_CURSOR_INFO cci; cci.bVisible = FALSE; cci.dwSize = sizeof(cci); SetConsoleCursorInfo(GetStdHandle(STD_OUTPUT_HANDLE), &amp;cci);&#125;*/import "C" // go中可以嵌入C语言的函数//设置控制台光标位置func GotoPostion(X int, Y int) &#123; //调用C语言函数 C.gotoxy(C.int(X), C.int(Y))&#125;//无显获取键盘输入的字符func Direction() (key int) &#123; key = int(C.direct()) return&#125;//设置控制台光标隐藏func HideCursor() &#123; C.hideCursor()&#125; 这个包把一些需要用到c语言的函数写进来，调用c语言的函数需要用到c语言的环境，别忘记自己电脑上要装c语言的环境奥，我们来看一下这个目录结构 首先我们的代码是放在GoCode里面下面的src目录下面，Clib里面是自己写的C语言的一个包，贪食蛇和Clib是同级别目录，在这里我们用的是goland编译器，编译器这里可以自己选择，我们编译的时候就不能单个文件编译了，因为需要调用自己的包，所以要选择多文件编译如图 我自己用的goland编译的时候需要改一下改成Directory 然后目录选到所在目录的src目录，然后设置好后就可以直接编译运行啦，当然也可以直接命令行编译运行 如图，我们可以在所在目录下面直接go build ./这样就是生成可执行文件.exe的，也可以直接使用go run命令直接编译运行， 感兴趣的小伙伴可以自己去试试啦 下面来看一下go语言写的代码，（可以自己去完善一下奥，比如加入等级，加入障碍物，蛇的速度都是自己可以调节的奥） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284package mainimport ( "Clib" "fmt" "os" "math/rand" "time")const wide int = 20const high int = 20var key int64 = 1 //关卡var food1 food //定义一个全局食物结构体//var size int = 2 //定义一个全局蛇的长度var score int = 0 //定义一个全局分数var dx int = 0var dy int = 0 //蛇的偏移量var barr1 barrier //障碍物结构体var c cake //定义一个蛋糕var FLAG bool=truetype postion struct &#123; x int y int //父类坐标&#125;type cake struct&#123; ca [5]postion&#125; //定义一个蛋糕type snake struct &#123; p [wide * high]postion size int dir byte&#125;type barrier struct &#123; barr [6]postion&#125; //障碍物结构体func (c *cake)setcake()&#123; x:=rand.Intn(wide-6)+3 y:=rand.Intn(high-6)+3 c.ca[0].x,c.ca[0].y=x,y c.ca[1].x,c.ca[1].y=x-1,y c.ca[2].x,c.ca[2].y= x-2,y c.ca[3].x,c.ca[3].y=x-1,y-1 c.ca[4].x,c.ca[4].y=x-1,y+1&#125;func (b *barrier)setbarrier()&#123; //定义一些随机障碍物 b.barr[0].x,b.barr[0].y=rand.Intn(wide-1)+1,rand.Intn(high-3)+1 b.barr[1].x,b.barr[1].y=rand.Intn(wide-1)+1,rand.Intn(high-3)+1 b.barr[2].x,b.barr[2].y=rand.Intn(wide-1)+1,rand.Intn(high-3)+1 //b.barr[3].x,b.barr[3].y=rand.Intn(wide-1)+1,rand.Intn(high-3)+1 //b.barr[4].x,b.barr[4].y=rand.Intn(wide-1)+1,rand.Intn(high-3)+1 //b.barr[5].x,b.barr[5].y=rand.Intn(wide-1)+1,rand.Intn(high-3)+1&#125;type food struct &#123; postion&#125; //食物func drawui(p postion, ch byte) &#123; Clib.GotoPostion(p.x*2+4, p.y+2+2) fmt.Fprintf(os.Stderr, "%c", ch)&#125;func (s *snake) initsnake() &#123; //蛇初始化 s.p[0].x = wide / 2 s.p[0].y = high / 2 s.p[1].x = wide/2 - 1 s.p[1].y = high / 2 //蛇头和第一个蛇结点初始化 s.dir = 'R' s.size=2 fmt.Fprintln(os.Stderr, ` #-----------------------------------------# | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | #-----------------------------------------#`) food1 = food&#123;postion&#123;rand.Intn(wide), rand.Intn(high) - 2&#125;&#125; //食物初始化 drawui(food1.postion, 'o') //画出食物 //go func()&#123; Clib.GotoPostion(46,19) fmt.Printf("正在进行第%d关，小心障碍物",key) //&#125;() go func()&#123; for&#123; time.Sleep(time.Second) num:=rand.Intn(10) if num==6&#123; c.setcake() break &#125; &#125; for i:=0;i&lt;len(c.ca);i++&#123; drawui(c.ca[i],'#') &#125; &#125;() //吃蛋糕的作用 go func()&#123; for i:=0;i&lt;len(barr1.barr);i++&#123; Clib.GotoPostion(barr1.barr[i].x,barr1.barr[i].y) drawui(barr1.barr[i],'!') &#125; &#125;() //打印出障碍物 go func() &#123; for &#123; switch Clib.Direction() &#123; case 72, 87, 119: if s.dir == 'D' &#123; break &#125; s.dir = 'U' case 65, 97, 75: if s.dir == 'R' &#123; break &#125; s.dir = 'L' case 100, 68, 77: if s.dir == 'L' &#123; break &#125; s.dir = 'R' case 83, 115, 80: if s.dir == 'U' &#123; break &#125; s.dir = 'D' case 32: s.dir = 'P' &#125; &#125; &#125;() //获取蛇跑的方向&#125;func (s *snake) playgame() &#123; //barr:=barrier&#123;postion&#123;rand.Intn(wide-5)+5,rand.Intn(high-5)+3&#125; //drawui(barr.postion,'p') for &#123; switch key &#123; case 1: time.Sleep(time.Second / 3) case 2:time.Sleep(time.Second / 5) case 3:time.Sleep(time.Second / 6) case 4:time.Sleep(time.Second / 7) case 5:time.Sleep(time.Second / 8) case 6:time.Sleep(time.Second / 9) //用来每增加一关蛇的速度加快 &#125; if s.dir == 'P' &#123; continue &#125; if s.p[0].x &lt; 0 || s.p[0].x &gt;= wide || s.p[0].y+2 &lt; 0 || s.p[0].y &gt;= high-2 &#123; Clib.GotoPostion(wide*3, high-3) FLAG=false return //如果蛇头碰墙就死亡 &#125; //if s.p[0].x==barr.postion.x&amp;&amp;s.p[0].y==barr.postion.y&#123; // Clib.GotoPostion(wide*3, high-3) // return //如果蛇头碰障碍物就死亡 //&#125; for i := 1; i &lt;s.size; i++ &#123; if s.p[0].x == s.p[i].x &amp;&amp; s.p[0].y == s.p[i].y &#123; Clib.GotoPostion(wide*3, high-3) FLAG=false return &#125; &#125; for j:=0;j&lt;len(barr1.barr);j++&#123; if s.p[0].x==barr1.barr[j].x&amp;&amp;s.p[0].y==barr1.barr[j].y&#123; Clib.GotoPostion(wide*3, high-3) FLAG=false return &#125; //碰到障碍物死亡 &#125; for m:=0;m&lt;len(c.ca);m++&#123; if s.p[0].x==c.ca[m].x&amp;&amp;s.p[0].y==c.ca[m].y&#123; s.size++ score++ &#125; if score &gt;= int(6+key*2) &#123; key++ return &#125; &#125; if s.p[0].x == food1.x &amp;&amp; s.p[0].y == food1.y &#123; s.size++ score++ if score &gt;= int(6+key*2) &#123; key++ return &#125; //画蛇 //food1 = food&#123;postion&#123;rand.Intn(wide), rand.Intn(high) - 2&#125;&#125; for &#123; flag := true temp := food&#123;postion&#123;rand.Intn(wide), rand.Intn(high) - 2&#125;&#125; for i := 1; i &lt; s.size; i++ &#123; if (temp.postion.x == s.p[i].x &amp;&amp; temp.postion.y == s.p[i].y) &#123; flag = false break &#125; &#125; for i:=0;i&lt;len(barr1.barr);i++&#123; if temp.postion.x==barr1.barr[i].x&amp;&amp;temp.postion.y==barr1.barr[i].y&#123; flag=false break &#125; &#125; if flag == true &#123; food1 = temp break &#125; &#125; drawui(food1.postion, 'o') &#125; switch s.dir &#123; case 'U': dx = 0 dy = -1 case 'D': dx = 0 dy = 1 case 'L': dx = -1 dy = 0 case 'R': dx = 1 dy = 0 &#125; lp := s.p[s.size-1] //蛇尾位置 for i := s.size - 1; i &gt; 0; i-- &#123; s.p[i] = s.p[i-1] drawui(s.p[i], '*') &#125; drawui(lp, ' ') //蛇尾画空格 s.p[0].x += dx s.p[0].y += dy //更新蛇头 drawui(s.p[0], 'O') //画蛇头 &#125;&#125;func main() &#123; rand.Seed(time.Now().UnixNano()) var s snake for k:=1;k&lt;=6;k++&#123; //用来循环6次代表6个关卡，这里可以自己设置多少关卡 s.initsnake() //初始化 barr1.setbarrier() //障碍物 s.playgame() //玩游戏开始 if FLAG==false&#123; //这个代表蛇死亡返回的，所以这样就退出了 Clib.GotoPostion(46,21) fmt.Printf("你已死亡，第%d关总分：%d分",k, score) break &#125; Clib.GotoPostion(46,21) fmt.Printf("第%d关总分：%d分,稍等进入下一关",k, score) //key++ time.Sleep(time.Second * 5) //延时5秒 Clib.Cls() //每一关清屏一下 //size=2 score=0 //每一关分数置为0 &#125; time.Sleep(time.Second * 5) //延时5秒&#125;]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>小Demo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C语言贪食蛇实现]]></title>
    <url>%2F2018%2F09%2F05%2FC%E8%AF%AD%E8%A8%80%E8%B4%AA%E9%A3%9F%E8%9B%87%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[CSDN博客 贪食蛇小Demo我们先来看一下C语言的贪食蛇代码，相对于面向对象的的语言，C语言是一门面向过程的语言，C语言写出来的代码都是顺着平常的思路来一步一步实现的，我们先来看C语言的代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;windows.h&gt;#include&lt;time.h&gt;//函数声明区void Pos(int x, int y);//光标位置设定void muban();//打印模板void initSnake();//蛇身的初始化void creatFood();//创建食物char reDirection();//识别方向int snakeMove();//蛇移动int crossWall();//不能穿墙int eatSelf();//不能吃自己typedef struct Snake//相当于蛇一个节点&#123; int x;//横坐标 int y;//纵坐标 struct Snake *next;&#125;snake;snake *head;//头指针snake *p;//用来遍历snake *food1;//用来标记的char status='L';//初始方向的状态，解决开始会动的问题int score=0;//分数int add=10;//一个食物的分int leap=0;//用来标志是否结束，0没有，1代表蛇死了代表结束了int endleap=0;//结束标志 1就是结束int sleepTime=500;void initSnake()//蛇身初始化，给定一个长度，用结构体表示是蛇的骨架，真正要显示出来是打印▇&#123; int i; snake *tail;//尾指针 tail=(snake*)malloc(sizeof(snake));//第一个节点/头结点 tail-&gt;x=30;//2的倍数，因为方块的长是两个单位 tail-&gt;y=10;//1个单位 tail-&gt;next=NULL; for(i=1;i&lt;=4;i++)//尾插法 &#123; head=(snake*)malloc(sizeof(snake));//申请一个节点 head-&gt;next=tail;//连接成链 head-&gt;x=30-2*i;//下一个节点的位置 head-&gt;y=10; tail=head; &#125; //遍历打印出来 while(tail!=NULL) &#123; Pos(tail-&gt;x,tail-&gt;y); printf("▇"); tail=tail-&gt;next; &#125;&#125;char reDirection()//识别用户按下的键值 保留方向值&#123; if(GetAsyncKeyState(VK_F7))//热键 &#123; if(sleepTime&gt;300)//最多减到300 &#123; sleepTime-=50;//每次减50 add++;//每次食物加1分 &#125; &#125; if(GetAsyncKeyState(VK_F8)) &#123; if(sleepTime&lt;800)//最多加到800 &#123; sleepTime+=50;//每次加50 add--;//每次食物减1分 &#125; &#125; if(GetAsyncKeyState(VK_UP)&amp;&amp;status!='D') status='U'; if(GetAsyncKeyState(VK_DOWN)&amp;&amp;status!='U') status='D'; if(GetAsyncKeyState(VK_LEFT)&amp;&amp;status!='R') status='L'; if(GetAsyncKeyState(VK_RIGHT)&amp;&amp;status!='L') status='R'; return status;&#125;void Pos(int x, int y)//设置光标位置，从哪里开始输出&#123; COORD pos;//表示一个字符在控制台屏幕上的坐标，左上角(0,0) HANDLE hOutput; pos.X = x; pos.Y = y; hOutput = GetStdHandle(STD_OUTPUT_HANDLE);//返回标准的输入、输出或错误的设备的句柄，也就是获得输入、输出/错误的屏幕缓冲区的句柄 SetConsoleCursorPosition(hOutput, pos);&#125;void creatFood()//创建食物&#123; snake *food;//创造一个食物 food=(snake*)malloc(sizeof(snake)); srand((unsigned int)time(NULL));//随着时间变化，产生不一样种子，就会得到没规律的食物 while(food-&gt;x%2!=0) &#123; food-&gt;x=rand()%56+2; &#125; food-&gt;y=rand()%23+1; //上面虽然解决了食物不会出现在城墙里，没有考虑食物出现在蛇本身里面 p=head;//用p来遍历 while(p!=NULL)//解决食物出现在蛇本身 &#123; if(food-&gt;x==p-&gt;x&amp;&amp;food-&gt;y==p-&gt;y) &#123; free(food); creatFood(); &#125; p=p-&gt;next; &#125; Pos(food-&gt;x,food-&gt;y); food1=food;//food1用来标记的作用 printf("▇"); Pos(70,20);//解决有光标闪烁的办法 printf("您的分数是:%d",score);&#125;void muban()&#123; int i; for(i=0;i&lt;=60;i+=2)//方块水平方向占两个单位 &#123; Pos(i,0); printf("▇");//上行 Pos(i,26); printf("▇");//下行 &#125; for(i=0;i&lt;=25;i+=1)//方块垂直方向占1个单位 &#123; Pos(0,i);//左列 printf("▇"); Pos(60,i);//右列 printf("▇"); &#125;&#125;int snakeMove()&#123; snake *nexthead; nexthead=(snake*)malloc(sizeof(snake)); if(status=='R')//向右走 &#123; nexthead-&gt;x=head-&gt;x+2; nexthead-&gt;y=head-&gt;y; if(nexthead-&gt;x==food1-&gt;x&amp;&amp;nexthead-&gt;y==food1-&gt;y)//吃掉了食物 &#123; nexthead-&gt;next=head; head=nexthead; p=head;//p用来从头遍历，打印方块 while(p!=NULL) &#123; Pos(p-&gt;x,p-&gt;y); printf("▇"); p=p-&gt;next; &#125;//吃掉了食物得创造 score=score+add; creatFood(); &#125; else//没有食物 &#123; nexthead-&gt;next=head; head=nexthead; p=head;//p用来从头遍历，打印方块 while(p-&gt;next-&gt;next!=NULL) &#123; Pos(p-&gt;x,p-&gt;y); printf("▇"); p=p-&gt;next; &#125; Pos(p-&gt;next-&gt;x,p-&gt;next-&gt;y); printf(" ");//会带来一个光标闪烁 Pos(70,20);//解决办法 printf("您的分数是:%d",score); free(p-&gt;next); p-&gt;next=NULL; &#125; &#125; if(status=='L')//向左走 &#123; nexthead-&gt;x=head-&gt;x-2; nexthead-&gt;y=head-&gt;y; if(nexthead-&gt;x==food1-&gt;x&amp;&amp;nexthead-&gt;y==food1-&gt;y)//吃掉了食物 &#123; nexthead-&gt;next=head; head=nexthead; p=head;//p用来从头遍历，打印方块 while(p!=NULL) &#123; Pos(p-&gt;x,p-&gt;y); printf("▇"); p=p-&gt;next; &#125;//吃掉了食物得创造 score=score+add; creatFood(); &#125; else//没有食物 &#123; nexthead-&gt;next=head; head=nexthead; p=head;//p用来从头遍历，打印方块 while(p-&gt;next-&gt;next!=NULL) &#123; Pos(p-&gt;x,p-&gt;y); printf("▇"); p=p-&gt;next; &#125; Pos(p-&gt;next-&gt;x,p-&gt;next-&gt;y); printf(" "); Pos(70,20);//解决办法 printf("您的分数是:%d",score); free(p-&gt;next); p-&gt;next=NULL; &#125; &#125; if(status=='U')//向上走 &#123; nexthead-&gt;x=head-&gt;x; nexthead-&gt;y=head-&gt;y-1; if(nexthead-&gt;x==food1-&gt;x&amp;&amp;nexthead-&gt;y==food1-&gt;y)//吃掉了食物 &#123; nexthead-&gt;next=head; head=nexthead; p=head;//p用来从头遍历，打印方块 while(p!=NULL) &#123; Pos(p-&gt;x,p-&gt;y); printf("▇"); p=p-&gt;next; &#125;//吃掉了食物得创造 score=score+add; creatFood(); &#125; else//没有食物 &#123; nexthead-&gt;next=head; head=nexthead; p=head;//p用来从头遍历，打印方块 while(p-&gt;next-&gt;next!=NULL) &#123; Pos(p-&gt;x,p-&gt;y); printf("▇"); p=p-&gt;next; &#125; Pos(p-&gt;next-&gt;x,p-&gt;next-&gt;y); printf(" "); Pos(70,20);//解决办法 printf("您的分数是:%d",score); free(p-&gt;next); p-&gt;next=NULL; &#125; &#125; if(status=='D')//向下走 &#123; nexthead-&gt;x=head-&gt;x; nexthead-&gt;y=head-&gt;y+1; if(nexthead-&gt;x==food1-&gt;x&amp;&amp;nexthead-&gt;y==food1-&gt;y)//吃掉了食物 &#123; nexthead-&gt;next=head; head=nexthead; p=head;//p用来从头遍历，打印方块 while(p!=NULL) &#123; Pos(p-&gt;x,p-&gt;y); printf("▇"); p=p-&gt;next; &#125;//吃掉了食物得创造 score=score+add; creatFood(); &#125; else//没有食物 &#123; nexthead-&gt;next=head; head=nexthead; p=head;//p用来从头遍历，打印方块 while(p-&gt;next-&gt;next!=NULL) &#123; Pos(p-&gt;x,p-&gt;y); printf("▇"); p=p-&gt;next; &#125; Pos(p-&gt;next-&gt;x,p-&gt;next-&gt;y); printf(" "); Pos(70,20);//解决办法 printf("您的分数是:%d",score); free(p-&gt;next); p-&gt;next=NULL; &#125; &#125; Sleep(sleepTime);//蛇移动的速度，里面是毫秒，越大速度越慢 status=reDirection();//判别下方向先 if(crossWall()==1||eatSelf()==1) //exit(0);//直接把程序关闭了 endleap=1; return endleap;&#125;int crossWall()//判断蛇有没穿透墙&#123; if(head-&gt;x==0||head-&gt;y==0||head-&gt;x==60||head-&gt;y==25) leap=1; return leap;&#125;int eatSelf()//判断是否咬到了自己&#123; snake *q;//遍历的 q=head-&gt;next; while(q!=NULL) &#123; if(q-&gt;x==head-&gt;x&amp;&amp;head-&gt;y==q-&gt;y) leap=1; q=q-&gt;next; &#125; return leap;&#125;//打印食物的时候会出现光标，解决办法就是引开它int main()&#123; muban();//打印模板 initSnake();//初始化蛇 creatFood();//创建食物 while(1)//死循环，让蛇一直动起来，直到蛇死了 &#123; if(snakeMove()==1)//判断是否结束 &#123; Pos(70,23); printf("蛇死了"); system("pause");//用来暂停 Pos(70,24);//解决press any key to continue 在该地点打印 大家试下 break; &#125; &#125; printf("是否继续游戏，y or n：");//y 继续 if(getch()=='y')//重新游戏 &#123; //蛇一开始就死了，因为全局变量没有恢复原值，仍然保留上一局的值 status='L';//初始方向的状态，解决开始会动的问题 score=0;//分数 add=10;//一个食物的分 leap=0;//用来标志是否结束，0没有，1代表蛇死了代表结束了 endleap=0;//结束标志 1就是结束 sleepTime=500; system("cls");//清理屏幕 main();//自己调用自己 看不一样的编译器，vc6.0允许调用自己 &#125; if(getch()=='n') &#123; Pos(70,25);//定一个位置，再打印press exit(0);//退出程序 &#125; return 0;&#125;//蛇的速度变化，每个食物的分数增加//是否继续游戏//按键的作用 在C语言中，我们利用定义一个一个函数模块来实现蛇的基础实现，然后定义蛇的一个结构体，利用链表的知识来串联蛇身体，来让蛇身连接起来并走动起来。 /*go语言实现的贪食蛇请见博下一章/]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>C/C++</tag>
        <tag>小Demo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C和Go相互调用]]></title>
    <url>%2F2018%2F09%2F05%2FC%E5%92%8CGo%E7%9B%B8%E4%BA%92%E8%B0%83%E7%94%A8%2F</url>
    <content type="text"><![CDATA[转载处：https://colobu.com/2018/08/28/c-and-go-calling-interaction/ C和Go相互调用C可以调用Go，并且Go可以调用C， 如果更进一步呢， C--&gt;Go--&gt;C 或者 Go--&gt;C--&gt;Go的调用如何实现？ 本文通过两个简单的例子帮助你了解这两种复杂的调用关系。本文不涉及两者之间的复杂的数据转换，官方文章C? Go? Cgo!、wiki/cgo和cmd/cgo有一些介绍。 Go—&gt;C—&gt;GoGo程序调用C实现的函数，然后C实现的函数又调用Go实现的函数。 1、首先，我们新建一个hello.go的文件： hello.go 1`package mainimport "C"import "fmt"//export HelloFromGofunc HelloFromGo() &#123; fmt.Printf("Hello from Go!\n")&#125;` 它定义了一个HelloFromGo函数，注意这个函数是一个纯的Go函数，我们定义它的输出符号为HelloFromGo。 2、接着我们新建一个hello.c的文件： 12345678#include &lt;stdio.h&gt;#include &quot;_cgo_export.h&quot;int helloFromC() &#123; printf(&quot;Hi from C\n&quot;); //call Go function HelloFromGo(); return 0;&#125; 这个c文件定义了一个C函数helloFromC,内部它会调用我们刚才定义的HelloFromGo函数。 这样，我们实现了C调用Go: C--&gt;Go,下面我们再实现Go调用C。 3、最后新建一个main.go文件： 123456789package main/*extern int helloFromC();*/import &quot;C&quot;func main() &#123; //call c function C.helloFromC()&#125; 它调用第二步实现的C函数helloFromC。 运行测试一下： 123$ go run .Hi from CHello from Go! 可以看到，期望的函数调用正常的运行。第一行是C函数的输出，第二行是Go函数的输出。 C—&gt;Go—&gt;C第二个例子演示了C程序调用Go实现的函数，然后Go实现的函数又调用C实现的函数。 1、首先新建一个hello.c文件： 12345#include &lt;stdio.h&gt;int helloFromC() &#123; printf(&quot;Hi from C\n&quot;); return 0;&#125; 它定义了一个纯C实现的函数。 2、接着新建一个hello.go文件： 1234567891011121314// go build -o hello.so -buildmode=c-shared .package main/*extern int helloFromC();*/import &quot;C&quot;import &quot;fmt&quot;//export HelloFromGofunc HelloFromGo() &#123; fmt.Printf(&quot;Hello from Go!\n&quot;) C.helloFromC()&#125;func main() &#123;&#125; 它实现了一个Go函数HelloFromGo,内部实现调用了C实现的函数helloFromC,这样我们就实现了Go--&gt;C。 注意包名设置为package main，并且增加一个空的main函数。 运行go build -o hello.so -buildmode=c-shared .生成一个C可以调用的库，这调命令执行完后会生成hello.so文件和hello.h文件。 3、最后新建一个文件夹，随便起个名字，比如main 将刚才生成的hello.so文件和hello.h文件复制到main文件夹，并在main文件夹中新建一个文件main.c: 123456789#include &lt;stdio.h&gt;#include &quot;hello.h&quot;int main() &#123; printf(&quot;use hello lib from C:\n&quot;); HelloFromGo(); return 0;&#125; 运行gcc -o main main.c hello.so生成可执行文件main, 运行main: 1234$ ./mainuse hello lib from C:Hello from Go!Hi from C 第一行输出来自main.c,第二行来自Go函数，第三行来自hello.c中的C函数，这样我们就实现了C--&gt;Go--C的复杂调用。 C--&gt;Go--&gt;C的状态变量我们来分析第二步中的一个特殊的场景， 为了下面我们好区分，我们给程序标记一下， 记为C1--&gt;Go--&gt;C2, C2的程序修改一下，加入一个状态变量a,并且函数helloFromC中会打印a的地址和值，也会将a加一。 123456#include &lt;stdio.h&gt;int a = 1;int helloFromC() &#123; printf(&quot;Hi from C: %p, %d\n&quot;, &amp;a, a++); return 0;&#125; 然后修改main.c程序,让它既通过Go嗲用C1.helloFromC,又直接调用C1.helloFromC,看看多次调用的时候a的指针是否一致，并且a的值是否有变化。 1234567891011121314#include &lt;stdio.h&gt;#include &quot;hello.h&quot;int main() &#123; printf(&quot;use hello lib from C:\n&quot;); // 1. 直接调用C函数 helloFromC(); // 2. 调用Go函数 HelloFromGo(); // 3. 直接调用C函数 helloFromC(); return 0;&#125; 激动人心的时候到了。我们不同的编译方式会产生不同的结果。 1、gcc -o main main.c hello.so 和第二步相同的编译方式，编译出main并执行， 因为hello.so中包含C1.helloFromC实现，所以可以正常执行。 123456./mainuse hello lib from C:Hi from C: 0x10092a370, 1Hello from Go!Hi from C: 0x10092a370, 2Hi from C: 0x10092a370, 3 可以看到a的指针是同一个值，无论通过Go函数改变还是通过C函数改变都是更改的同一个变量。 nm可以查看生成的main的符号： 1234567nm main U _HelloFromGo0000000100000000 T __mh_execute_header U _helloFromC0000000100000f10 T _main U _printf U dyld_stub_binder U代表这个符号是未定义的符号，通过动态库链接进来。 2、 gcc -o main main.c hello.so ../hello.c 我们编译的时候直接链接hello.c的实现，然后运行main: 123456./mainuse hello lib from C:Hi from C: 0x104888020, 1Hello from Go!Hi from C: 0x1049f7370, 1Hi from C: 0x104888020, 2 可以看到a是不同的两个变量。 nm可以查看生成的main的符号： 12345678nm main U _HelloFromGo0000000100000000 T __mh_execute_header0000000100001020 D _a0000000100000f10 T _helloFromC0000000100000ec0 T _main U _printf U dyld_stub_binder 可以看到_a是初始化的环境变量，_helloFromC的类型是T而不是U,代表它是一个全局的Text符号,这和上一步是不一样的]]></content>
      <categories>
        <category>C/C++</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>C/C++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链以太坊相关资料]]></title>
    <url>%2F2018%2F09%2F04%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E4%BB%A5%E5%A4%AA%E5%9D%8A%E7%9B%B8%E5%85%B3%E8%B5%84%E6%96%99%2F</url>
    <content type="text"><![CDATA[收集整理了一些免费区块链、以太坊技术开发相关的文件，有需要的可以下载，文件链接： web3.js API官方文档中文版：https://pan.baidu.com/s/1hOV9hEzi7hFxJCL4LTvC6g 以太坊官方文档中文版 ：https://pan.baidu.com/s/1ktODJKLMBmkOsi8MPrpIJA 以太坊白皮书中文版 ：https://pan.baidu.com/s/1bzAFnzJ35hlQxJ2J4Oj-Ow Solidity的官方文档中文版 ：https://pan.baidu.com/s/18yp9XjEqAHpiFm2ZSCygHw Truffle的官方文档中文版 ：https://pan.baidu.com/s/1y6SVd7lSLUHK21YF5FzIUQ C#区块链编程指南 ：https://pan.baidu.com/s/1sJPLqp1eQqkG7jmxqwn3EA 区块链技术指南： ：https://pan.baidu.com/s/13cJxAa80I6iMCczA04CZhg 精通比特币中文版： ：https://pan.baidu.com/s/1lz6te3wcQuNJm28rFvBfxg Node.js区块链开发 ：https://pan.baidu.com/s/1Ldpn0DvJ5LgLqwix6eWgyg geth使用指南文档中文版 ：https://pan.baidu.com/s/1M0WxhmumF_fRqzt_cegnag 以太坊DApp开发环境搭建-Ubuntu : https://pan.baidu.com/s/10qL4q-uKooMehv9X2R1qSA 以太坊DApp开发环境搭建-windows ：https://pan.baidu.com/s/1cyYkhIJIFuI2oyxM9Ut0eA 以太坊DApp开发私链搭建-Ubuntu : https://pan.baidu.com/s/1aBOFZT2bCjD2o0EILBWs-g 以太坊DApp开发私链搭建-windows ：https://pan.baidu.com/s/10Y6F1cqUltZNN99aJv9kAA 以太坊ganache CLI命令行参数详解：https://pan.baidu.com/s/1lnknFkwenacaeM4asOcBdg 使用truflle和infura部署以太坊合约：https://pan.baidu.com/s/1PTxSVff2vHSVUihYczRRqw IPFS安装部署与开发环境搭建-windows：https://pan.baidu.com/s/1bnhDvqCoOgAqEBZXMtVbRg EOS.IO教程： EOS智能合约与DApp开发入门：http://t.cn/RealN1W 以太坊教程： 以太坊DApp开发实战入门：http://t.cn/RmeEwxJ 以太坊node.js电商实战：http://t.cn/RnmDmaD C#开发以太坊区块链的教程：http://t.cn/ReYjplC java开发以太坊区块链的教程，web3j开发详解：http://t.cn/RrpULLJ PHP开发以太坊区块链的教程：http://t.cn/RrRAlAO python用web3.py开发以太坊区块链应用的教程：http://t.cn/RdXcpVD]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>以太坊</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链中的双花问题]]></title>
    <url>%2F2018%2F09%2F04%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E4%B8%AD%E7%9A%84%E5%8F%8C%E8%8A%B1%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[区块链中的“双花”问题我们举个简单的例子，比如你在商场刷卡买东西。这个行为面临三种危险： 首先，刷卡这个行为，验证的是你的信用卡信息，也就是说只要给刷卡机提供同样的信息，就能从你的账户里把钱刷走。没错，很多朋友都听说过，有犯罪组织专门从事复制卡信息的勾当，然后“盗刷”你的卡。在一些不发达国家的小店里刷卡就特别容易中招。 其次，负责记账和结算的卡组织和银行的服务器可能被黑客攻破，造成数据泄露和伪造交易。回想这些年一波又一波某某大公司数据库被黑客攻入的新闻，这危险并非危言耸听。（好吧，认真的geek会说这里用词应该是cracker骇客而非hacker黑客，不过这年头认真的人越来越少了） 最后，还有一种可能，就是用卡人自己可能利用系统网络延迟，在进行第一笔交易、用完所有额度后，趁系统还没记账把额度扣完，立刻进行第二笔交易，形成诈骗。当然目前的结算系统延迟极小，这情况不太可能，不过像在优惠券或者抢购资格这种另外搭建的相对脆弱的系统上还是有可能的。 网上支付也一样，犯罪分子可以用特殊手段（例如木马，伪造WIFI等）截获你跟服务器之间的传递数据，如果商家加密技术太弱的话信息就可能被破解——嗯，某国很多时候数据干脆是不加密的。所以大家才一直被警告不要乱装程序、不要连可疑的WiFi。 区块链是怎么处理这些问题的呢？我们以比特币交易为例，逐条分析。 首先，比特币拥有者想要完成某项交易，比如买手机吧，他会向全网广播：我小A向小B支付1个比特币（嗯，这金额现在大致可以买个5个iPhone 8）。 与这条信息一起的，还有一条加密信息，这条信息是用Hash函数对上一条信息加密生成一个摘要后，再用A的私钥进行加密的（称为私钥“签名”）。 接收到这条信息的B和其他用户先用同样的Hash函数对明文信息生成摘要，再用A的公钥对加密信息进行解密，如果解密得到的摘要与明文生成的摘要相同，便认为信息确实是A发出的，且没有经过篡改。 A的公钥和Hash是公开的，私钥则无法算出，只有A知道，这样就既保证了交易的达成，又保证了A的信息无法被窃取。 其次，由在POW（运算力证明）中胜出的矿工负责这段时间的记账，事先完全无法知道究竟哪个矿工来记账，黑客也就无从黑起，除非碰运气。 最后，在传统系统中因为结算速度极快而不太可能的情况，在比特币网络中反而可能性比较大。因为没有中心化的管理者，交易确认的时间要长很多，使得这种诈骗有可能实现，这就是比特币的double spending双重花费问题，简称“双花”。 对于双花问题，比特币网络，或者说区块链网络，是这么应对的： -每笔交易都需要先确认对应比特币之前的状态，如果它之前已经被标记为花掉，那么新的交易会被拒绝。 -如果先发起一笔交易，在它被确认前，也就是这个时间段的交易还未被记账成区块block时，进行矛盾的第二笔交易，那么在记账时，这些交易会被拒绝。 -上面只是小伎俩，现在tricky的部分开始了。如果诈骗者刻意把第一笔交易向一半网络进行广播，把第二笔交易向另一半网络广播——这个诈骗者智商还挺高——然后两边正好有两个矿工几乎同时取得记账权，把各自记的block发布给大家的话（这个概率很低），网络是不是会混乱呢，区块链的规则是这样的：先选择任意一个账本都可以，这时候原来统一的账本出现了分叉： 但是在两个账本中各只有一笔交易，诈骗者不会有好处。接下来，下一个矿工选择在A基础上继续记账的话，A分支就会比B分支更长，根据区块链的规则，最长的分支会被认可，短的分支会被放弃，账本还是会回归为一个，交易也只有一笔有效： -那么如果这个诈骗犯真的智商非常高，他会这么做：如果是A分支被认可（B也一样），相应交易确认，拿到商品之后，立刻自己变身矿工，争取到连续两次记账权，然后在B分支上连加两个block，就像这样： 于是B分支成为认可的分支，A被舍弃，A分支中的交易不再成立，但他已经拿到商品，诈骗成功。 在B分支落后的情况下要强行让它超过A分支，其实是挺难的，假设诈骗者掌握了全网1%的计算能力，那么他争取到记账权的概率就是1%，两次就是10的负4次方。但这个概率还没有太低。 应对办法呢？建议大家在一笔交易确认后，也就是一个block被记下来之后，再等5个block，也就是等6个block被确认后再把交易对应的商品交付。这样，诈骗者还能追上的概率就几乎为0了。除非…… 如果诈骗者掌握了全网50%以上的计算力，那么，即使落后很多，他追上也只是时间问题，这就是比特币的“51%攻击”。 这就是区块链需要警惕的问题。虽然在比特币网络中，用户已经极多，全网算力总和非常大，如果真掌握50%以上，也不用靠这个诈骗了，挖矿的收益都更高。但是在小的区块链网络中呢？况且，没有50%以上的算力，还是有机会成功的，只是概率低而已。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>双花问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是区块链]]></title>
    <url>%2F2018%2F09%2F04%2F%E4%BB%80%E4%B9%88%E6%98%AF%E5%8C%BA%E5%9D%97%E9%93%BE%2F</url>
    <content type="text"><![CDATA[【定义】区块链（Blockchain）是指通过去中心化和去信任的方式集体维护一个可靠数据库的技术方案。该技术方案让参与系统中的任意多个节点，把一段时间系统内全部信息交流的数据，通过密码学算法计算和记录到一个数据块（block），并且生成该数据块的指纹用于链接（chain）下个数据块和校验，系统所有参与节点来共同认定记录是否为真。 区块链是一种类似于NoSQL（非关系型数据库）这样的技术解决方案统称，并不是某种特定技术，能够通过很多编程语言和架构来实现区块链技术。并且实现区块链的方式种类也有很多，目前常见的包括POW（Proof of Work，工作量证明），POS（Proof of Stake，权益证明），DPOS（Delegate Proof of Stake，股份授权证明机制）等。 区块链的概念首次在论文《比特币：一种点对点的电子现金系统（Bitcoin: A Peer-to-Peer Electronic Cash System）》中提出，作者为自称中本聪（Satoshi Nakamoto）的个人（或团体）。因此可以把比特币看成区块链的首个在金融支付领域中的应用。 【通俗解释】无论多大的系统或者多小的网站，一般在它背后都有数据库。那么这个数据库由谁来维护？在一般情况下，谁负责运营这个网络或者系统，那么就由谁来进行维护。如果是微信数据库肯定是腾讯团队维护，淘宝的数据库就是阿里的团队在维护。大家一定认为这种方式是天经地义的，但是区块链技术却不是这样。 如果我们把数据库想象成是一个账本：比如支付宝就是很典型的账本，任何数据的改变就是记账型的。数据库的维护我们可以认为是很简单的记账方式。在区块链的世界也是这样，区块链系统中的每一个人都有机会参与记账。系统会在一段时间内，可能选择十秒钟内，也可能十分钟，选出这段时间记账最快最好的人，由这个人来记账，他会把这段时间数据库的变化和账本的变化记在一个区块（block）中，我们可以把这个区块想象成一页纸上，系统在确认记录正确后，会把过去账本的数据指纹链接（chain）这张纸上，然后把这张纸发给整个系统里面其他的所有人。然后周而复始，系统会寻找下一个记账又快又好的人，而系统中的其他所有人都会获得整个账本的副本。这也就意味着这个系统每一个人都有一模一样的账本，这种技术，我们就称之为区块链技术（Blockchain），也称为分布式账本技术。 由于每个人（计算机）都有一模一样的账本，并且每个人（计算机）都有着完全相等的权利，因此不会由于单个人（计算机）失去联系或宕机，而导致整个系统崩溃。既然有一模一样的账本，就意味着所有的数据都是公开透明的，每一个人可以看到每一个账户上到底有什么数字变化。它非常有趣的特性就是，其中的数据无法篡改。因为系统会自动比较，会认为相同数量最多的账本是真的账本，少部分和别人数量不一样的账本是虚假的账本。在这种情况下，任何人篡改自己的账本是没有任何意义的，因为除非你能够篡改整个系统里面大部分节点。如果整个系统节点只有五个、十个节点也许还容易做到，但是如果有上万个甚至上十万个，并且还分布在互联网上的任何角落，除非某个人能控制世界上大多数的电脑，否则不太可能篡改这样大型的区块链。 【要素】结合区块链的定义，我们认为必须具有如下四点要素才能被称为公开区块链技术，如果只具有前3点要素，我们将认为其为私有区块链技术（私有链）。 1、点对点的对等网络（权力对等、物理点对点连接） 2、可验证的数据结构（可验证的PKC体系，不可篡改数据库） 3、分布式的共识机制（解决拜占庭将军问题，解决双重支付） 4、纳什均衡的博弈设计（合作是演化稳定的策略） 【特性】结合定义区块链的定义，区块链会现实出四个主要的特性：去中心化（Decentralized）、去信任（Trustless）、集体维护（Collectively maintain）、可靠数据库（Reliable Database）。并且由四个特征会引申出另外2个特征：开源（Open Source）、隐私保护（Anonymity）。如果一个系统不具备这些特征，将不能视其为基于区块链技术的应用。 去中心化（Decentralized）：整个网络没有中心化的硬件或者管理机构，任意节点之间的权利和义务都是均等的，且任一节点的损坏或者失去都会不影响整个系统的运作。因此也可以认为区块链系统具有极好的健壮性。 去信任（Trustless）：参与整个系统中的每个节点之间进行数据交换是无需互相信任的，整个系统的运作规则是公开透明的，所有的数据内容也是公开的，因此在系统指定的规则范围和时间范围内，节点之间是不能也无法欺骗其它节点。 集体维护（Collectively maintain）：系统中的数据块由整个系统中所有具有维护功能的节点来共同维护的，而这些具有维护功能的节点是任何人都可以参与的。 可靠数据库（Reliable Database）：整个系统将通过分数据库的形式，让每个参与节点都能获得一份完整数据库的拷贝。除非能够同时控制整个系统中超过51%的节点，否则单个节点上对数据库的修改是无效的，也无法影响其他节点上的数据内容。因此参与系统中的节点越多和计算能力越强，该系统中的数据安全性越高。 开源（Open Source）：由于整个系统的运作规则必须是公开透明的，所以对于程序而言，整个系统必定会是开源的。 隐私保护（Anonymity）：由于节点和节点之间是无需互相信任的，因此节点和节点之间无需公开身份，在系统中的每个参与的节点的隐私都是受到保护。 【区块链意义之一 ：解决拜占庭将军问题】区块链解决的核心问题不是“数字货币”，而是在信息不对称、不确定的环境下，如何建立满足经济活动赖以发生、发展的“信任”生态体系。而这个问题称之为“拜占庭将军问题”，也可称为“拜占庭容错”或者“两军问题”，这是一个分布式系统中进行信息机交互时面临的难题，即在整个网络中的任意节点都无法信任与之通信的对方时，如何能创建出共识基础来进行安全的信息交互而无需担心数据被篡改。区块链使用算法证明机制来保证整个网络的安全，借助它，整个系统中的所有节点能够在去信任的环境下自动安全的交换数据。更多介绍请参见《比特币与拜占庭将军问题》。 【区块链意义之二：实现跨国价值转移】互联网诞生最初，最早核心解决的问题是信息制造和传输，我们可以通过互联网将信息快速生成并且复制到全世界每一个有着网络的角落，但是它尚始终不能解决价值转移和信用转移。这里所谓的价值转移是指，在网络中每个人都能够认可和确认的方式，将某一部分价值精确的从某一个地址转移到另一个地址，而且必须确保当价值转移后，原来的地址减少了被转移的部分，而新的地址增加了所转移的价值。这里说的价值可以是货币资产，也可以是某种实体资产或者虚拟资产（包括有价证券、金融衍生品等）。而这操作的结果必须获得所有参与方的认可，且其结果不能受到任何某一方的操纵。 在目前的互联网中也有各种各样的金融体系，也有许多政府银行提供或者第三方提供的支付系统，但是它还是依靠中心化的方案来解决。所谓中心化的方案，就是通过某个公司或者政府信用作为背书，将所有的价值转移计算放在一个中心服务器（集群）中，尽管所有的计算也是由程序自动完成，但是却必须信任这个中心化的人或者机构。事实上通过中心化的信用背书来解决，也只能将信用局限在一定的机构、地区或者国家的范围之内。由此可以看出，必须要解决的这个根本问题，那就是信用。所以价值转移的核心问题是跨国信用共识。 在如此纷繁复杂的全球体系中，要凭空建立一个全球性的信用共识体系是很难的，由于每个国家的政治、经济和文化情况不同，对于两个国家的企业和政府完全互信是几乎做不到的，这也就意味着无论是以个人抑或企业政府的信用进行背书，对于跨国之间的价值交换即使可以完成，也有着巨大的时间和经济成本。但是在漫长的人类历史中，无论每个国家的宗教、政治和文化是如何的不同，唯一能取得共识的是数学（基础科学）。因此，可以毫不夸张的说，数学（算法）是全球文明的最大公约数，也是全球人类获得最多共识的基础。如果我们以数学算法（程序）作为背书，所有的规则都建立一个公开透明的数学算法（程序）之上，能够让所有不同政治文化背景的人群获得共识。 【未来的发展】互联网将使得全球之间的互动越来越紧密，伴随而来的就是巨大的信任鸿沟。目前现有的主流数据库技术架构都是私密且中心化的，在这个架构上是永远无法解决价值转移和互信问题。所以区块链技术有可能将成为下一代数据库架构。通过去中心化技术，将能够在大数据的基础上完成数学（算法）背书、全球互信这个巨大的进步。 区块链技术作为一种特定分布式存取数据技术，它通过网络中多个参与计算的节点开共同参与数据的计算和记录，并且互相验证其信息的有效性（防伪）。从这一点来，区块链技术也是一种特定的数据库技术。互联网刚刚进入大数据时代，但是从目前来看，大数据还处于非常基础的阶段。但是当进入到区块链数据库阶段，将进入到真正的强信任背书的大数据时代。这里面的所有数据都获得坚不可摧的质量，任何人都没有能力也没有必要去质疑。 也许我们现在正处在一个重大的转折点之上——和工业革命所带来的深刻变革几乎相同的重大转折的早期阶段。不仅仅是新技术指数级、数字化和组合式的进步与变革，更多的惊喜也许还会在我们前面。在未来的24个月里，这个星球所增长的计算机算力和记录的数据将会超过所有历史阶段的总和。在过去的24个月里，这个增值可能已经超过了1000倍。这些数字化的数据信息还在以比摩尔定律更快的速度增长。区块链技术将不仅仅应用在金融支付领域，而是将会扩展到目前所有应用范围，诸如去中心化的微博、微信、搜索、租房，甚至是打车软件都有可能会出现。因为区块链将可以让人类无地域限制的、去信任的方式来进行大规模协作。 我们这一代人将很可能会幸运地经历人类历史上两个最让人吃惊的事件，地球上的所有人和所有机器通过区块链技术以前所未有的互信展开了空前的大规模协作，其次就是基于此真正的人工智能将被创造出来。这两个时间将会深深地改变这个世界的经济发展模式。创业者、企业家、科学家以及各种各样的极客将利用这个充裕的世界去创造能让我们震惊和快乐。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拜占庭将军问题]]></title>
    <url>%2F2018%2F09%2F04%2F%E6%8B%9C%E5%8D%A0%E5%BA%AD%E5%B0%86%E5%86%9B%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[拜占庭将军问题（Byzantine failures）是由莱斯利·兰伯特提出的点对点通信中的基本问题。含义是在存在消息丢失的不可靠信道上试图通过消息传递的方式达到一致性是不可能的。因此对一致性的研究一般假设信道是可靠的，或不存在本问题。这个难题也被称为“拜占庭容错”、“拜占庭将军问题”、或者“两军问题”。 拜占庭将军问题是一个协议问题，拜占庭帝国军队的将军们必须全体一致的决定是否攻击某一支敌军。问题是这些将军在地理上是分隔开来的，并且将军中存在叛徒。叛徒可以任意行动以达到以下目标：欺骗某些将军采取进攻行动；促成一个不是所有将军都同意的决定，如当将军们不希望进攻时促成进攻行动；或者迷惑某些将军，使他们无法做出决定。如果叛徒达到了这些目的之一，则任何攻击行动的结果都是注定要失败的，只有完全达成一致的努力才能获得胜利。 拜占庭假设是对现实世界的模型化，由于硬件错误、网络拥塞或断开以及遭到恶意攻击，计算机和网络可能出现不可预料的行为。拜占庭容错协议必须处理这些失效，并且这些协议还要满足所要解决的问题要求的规范。 首先，不要把比特币当成一种货币，而是一个总账。它是个电子账本，网络上的每一个参与者的电脑都会有一份账本的备份，并且所有的备份都是在实时的持续的更新、对账、以及同步着。每一个参与者都能在这本账本里记上一笔，这一笔记录着一定数量的币从一个参与者那里被发送到另一个参与者那里，并且每一条这样的记录都接着就实时的广播到网络了，所以在每一台电脑上的每一分份拷贝都是几乎同时更新的，并且所有的账本拷贝都保持着同步。这本公开的分布式的账本就可以称为“区块链（blockchain）”，并且它使用了BT技术以保证所有的拷贝都是同步的。 可以把比特币当作一个对于在分布式系统领域的一个复杂的算法难题的通用解决方法。 这一问题的趣味非正式表述如下：想象一下，在拜占庭时代有一个墙高壁厚的城邦，拜占庭，高墙之内是它的邻居想象不到之多的财富。它被其他10个城邦所环绕，这10个城邦也很富饶，但和拜占庭相比就微不足道了。它的十个邻居都觊觎拜占庭的财富，并希望侵略并占领它。 但是，拜占庭的防御是如此的强大，没有一个相邻的城邦能够成功入侵。任何单个城邦的入侵行动都会失败，而入侵者的军队也会被歼灭，使得其自身容易遭到其他九个城邦的入侵和劫掠。这十个城邦之间也互相觊觎对方的财富并持续互相对抗着。而且，拜占庭的防御如此之强，十个邻居的一半以上同时进攻才能攻破它。 也就是说，如果六个或者更多的相邻敌军一起进攻，他们就会成功并获得拜占庭的财富。然而，如果其中有一个或者更多背叛了其他人，答应一起入侵但在其他人进攻的时候又不干了，也就导致只有五支或者更少的军队在同时进攻，那么所有的进攻军队都会被歼灭，并随后被其他的（包括背叛他们的那（几）个）邻居所劫掠。这是一个由不互相信任的各方构成的网络，但他们又必须一起努力以完成共同的使命。 而且，是个邻居之间通讯和协调统计时间的唯一途径是通过骑马在他们之间传递信息。他们不能聚在一个地方开个会（所有的王都不互相信任他们的安全在自己的城堡或者军队范围之外能够得到保障）。然而，他们可以在任意时间以任意频率派出任意数量的信使到任意的对方。每条信息都包含类似如下的内容：“我将在第四天的6点钟进攻，你愿意加入吗？”。 如果收信人同意了，他们就会在原信上附上一份签名了的/认证了的/盖了图章的/验证了的回应，然后把新合并了的信息的拷贝再次发送给九个邻居，要求他们也如此这样做。最后的目标是，通过在原始信息链上盖上他们所有十个人的图章，让他们在时间上达成共识。最后的结果是，会有一个盖有十个同意同一时间的图章信息链，可能还会有一些被抛弃了的包含部分但不是全部图章的信息链。 但是，问题在于如果每个城邦向其他九个城邦派出一名信使，那么就是十个城邦每个派出了九名信使，也就是在任何一个时间又总计90次的传输，并且每个城市分别收到九个信息，可能每一封都写着不同的进攻时间。除此以外，部分城邦会答应超过一个的攻击时间，故意背叛发起人，所以他们将重新广播超过一条的信息链。这个系统迅速变质成不可信的信息和攻击时间相互矛盾的纠结体。 比特币通过对这个系统做出一个简单的（事后看是简单的）修改解决了这个问题，它为发送信息加入了成本，这降低了信息传递的速率，并加入了一个随机元素以保证在一个时间只有一个城邦可以进行广播。它加入的成本是“工作量证明”，并且它是基于计算一个随机哈希算法的。哈希是一种算法，它唯一做的事情就是获得一些输入然后进行计算，并得到一串64位的随机数字和字母的字符串，就像这个： 1d70298566aa2f1a66d892dc31fedce6147b5bf509e28d29627078d9a01a8f86b 在比特币的世界中，输入数据包括了到当前时间点的整个总账（区块链）。并且尽管单个哈希值用现在的计算机可以几乎即时的计算出来，但只有一个前13个字符是0的哈希值结果可以被比特币系统接受成为“工作量证明”。这样一个13个0的哈希值是极其不可能与罕见的，并且在当前需要花费整个比特币网络大约10分钟的时间来找到一个。在一台网络中的机器随机的找到一个有效哈希值之前，上十亿个的无效值会被计算出来，这就是减慢信息传递速率并使得整个系统可用的“工作量证明”。下面是一个例子： 123f51d0199c4a6d9f6da230b579d850698dff6f695b47d868cc1165c0ce74df5e1d70298566aa2f1a66d892dc31fedce6147b5bf509e28d29627078d9a01a8f86b119c506ceaa18a973a5dbcfbf23253bc970114edd1063bd1288fbba468dcb7f8 在找到一个有效值之前，成百万上亿个更多的类似上面这样的字符串被计算出来…… 1000000000000084b6550604bf21ad8a955b945a0f78c3408c5002af3cdcc14f5 那台发现下一个有效哈希值的机器（或者说在我们类比中的城邦），把所有的之前的信息放到一起，附上它自己的，以及它的签名/印章/诸如此类，并向网络中的其他机器广播出去。只要其他网络中的机器接收到并验证通过了这个13个0的哈希值和附着在上面的信息，他们就会停止他们当下的计算，使用新的信息更新他们的总账拷贝，然后把新更新的总账/区块链作为哈希算法的输入，再次开始计算哈希值。哈希计算竞赛从一个新的开始点重新开始。如此这般，网络持续同步着，所有网络上的电脑都使用着同一版本的总账。 与此同时，每一次成功找到有效哈希值以及区块链更新的间隔大概是10分钟（这是故意的，算法难度每两周调整一次以保证网络一直需要花费10分钟来找到一个有效的哈希值）。在那10分钟以内，网络上的参与者发送信息并完成交易，并且因为网络上的每一条机器都是使用同一个总账，所有的这些交易和信息都会进入遍布全网的每一份总账拷贝。当区块链更新并在全网同步之后，所有的在之前的10分钟内进入区块链的交易也被更新并同步了。因此分散的交易记录是在所有的参与者之间进行对账和同步的。 最后，在个人向网络输入一笔交易的时候，他们使用内嵌在比特币客户端的标准公钥加密工具来同时他们的私钥以及接收者的公钥来为这笔交易签名。这对应于拜占庭将军问题中他们用来签名和验证消息时使用的“印章”。因此，哈希计算速率的限制，加上公钥加密，使得一个不可信网络变成了一个可信的网络，使得所有参与者可以在某些事情上达成一致（比如说攻击时间、或者一系列的交易、域名记录、政治投票系统、或者任何其他的需要分布式协议的地方）。 这里是比特币为何如此特别的关键：它代表了一个对于一个困难的算法上的难题的解决方案，这一解决方案在一系列的历史事件发生之前是不可能的，这些事件有： 互联网的创造 公钥加密算法的发明 点对点Bitorrent(BT)协议的发明。BT协议最开始是开发来用于在网络上的相对小的用户子集之间共享许多文件的，但比特币用它来在所有用户之间共享单个文件。 人们意识到，在系统中添加一个简单的时间延迟，同时使用公钥加密算法以验证每笔交易，可以解决这个问题。 如果说一些最棒的想法在事后看来是很简单的，那么上述的第四点就完全符合条件，尽管整个项目是站在了巨人的肩膀上的。 最后，这一对于拜占庭将军问题的解决方案，可以推广到任何核心问题是在分布式网络上缺乏信任的领域。如我们已经提到乐的，人们正在为互联网建设一个分布式的域名系统，以及为政治选举建设分布式的投票系统（还没有网站）。如果人们认为单纯的文件分享搅乱了这个世界，那么比特币解决方案和协议才刚刚打开洪水的闸门。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>区块链</tag>
        <tag>拜占庭将军问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[队列的链式存储结构]]></title>
    <url>%2F2018%2F09%2F04%2F%E9%98%9F%E5%88%97%E7%9A%84%E9%93%BE%E5%BC%8F%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[队列的链式存储结构，其实就是线性表的单链表，只不过它只能尾进头出而已，我们把它简称为链队列。 为了操作上的方便，我们将队头指针指向链队列的头结点，而队尾指针指向终端结点。链队列示意图： 当队列为空时，front和rear都指向头结点 实例代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package mainimport ( "errors" "fmt")type qelemType int//数据节点(链表形式)type QNode struct &#123; data qelemType next *QNode&#125;type queuePtr *QNode//定义队列type LinkQueue struct &#123; front, rear queuePtr&#125;//创建头结点func createHeadNode(q *LinkQueue) &#123; s := queuePtr(new(QNode)) s.next = nil q.front = s q.rear = s&#125;//如队列操作func enQueue(q *LinkQueue, e qelemType) &#123; s := queuePtr(new(QNode)) s.data = e s.next = nil //队尾为空 q.rear.next = s q.rear = s //rear指向新添加的数据（保证指向最后一个元素）&#125;//出队列func deQueue(q *LinkQueue) (err error, res qelemType) &#123; if q.front == q.rear &#123; err = errors.New("队列为空，没有数据出队列") return &#125; s := q.front.next res = s.data q.front.next = s.next if q.rear == s &#123; q.rear = q.front &#125; return&#125;func main() &#123; var p LinkQueue /* 注意 需要创建头结点，不然头结点为空，操作它的.next 会发生异常,异常信息如下： panic: runtime error: invalid memory address or nil pointer dereference [signal 0xc0000005 code=0x0 addr=0x0 pc=0x48b5c9] */ createHeadNode(&amp;p) enQueue(&amp;p, qelemType(123)) enQueue(&amp;p, qelemType(345)) enQueue(&amp;p, qelemType(567)) _, res := deQueue(&amp;p) fmt.Println(res) _, res = deQueue(&amp;p) fmt.Println(res) _, res = deQueue(&amp;p) fmt.Println(res) err, res := deQueue(&amp;p) fmt.Println(err, res)&#125; 运行效果：]]></content>
      <categories>
        <category>数据结构和算法</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>单链表</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go语言之陷阱for range]]></title>
    <url>%2F2018%2F09%2F04%2Fgo%E8%AF%AD%E8%A8%80%E4%B9%8B%E9%99%B7%E9%98%B1for-range%2F</url>
    <content type="text"><![CDATA[在go语言中，遍历有两种方法，一种就是for的普通方法，还有一种就是for range的遍历，但是在使用for range时，如果使用不当，就会出现一些问题比如我们下面先来看一个例题 123456789101112131415161718192021package mainimport "fmt"type Student struct&#123; Name string Age int&#125; //一个学生结构体func main()&#123; m:=make(map[string]*Student) //声明一个映射 stus:=[]Student&#123; &#123;"宋",22&#125;, &#123;"高",23&#125;, &#123;"徐",24&#125;, &#123;"李",25&#125;, &#125; //一个学生类切片 for _,stu:=range stus&#123; m[stu.Name]=&amp;stu //遍历赋值给映射 &#125; for _,value:=range m&#123; fmt.Println(*value) //遍历打印出来 &#125;&#125; 我们的代码是把stus这个结构体切片里面的内容用for range赋值给m映射，看起来代码好像没什么问题，一次循环赋值一次循环打印，那我们来看一下打印结果是什么 打印结果竟然是这样，为什么都是一样的呢，而且是结构体切片最后的一个元素，看下面这张图 这是因为我们第一次使用for range遍历的时候 我们是使用零时变量stu的地址来传给m的，而且零时变量stu每次的地址都是不会变的，所以一直到遍历最后一次就会把最后一个值的地址传给m，这就导致了m里面的值都是一样，我们可以试着来打印一下地址来看看 我们先来打印一下m看看 看到没，这四个地址竟然都是一样的，这就是因为用stu零时变量去地址去传的话地址都是一样的，那样传值就达不到预期的效果，所以一定要小心这个陷阱，那我们上面应该怎样改就可以完整的传值呢 看到没，我们可以在for range里面弄一个stu1来接受零时变量stu的值，然后取stu1的地址传值，这样就不会出错啦，我们来看看打印结果 这样我们每次的地址也不一样了，打印出来的结果也就正确达到预期的结果了，因为map是无序的所以打印出来也是无序的，切忌用for range的时候小心陷阱]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go语言错误捕获和panic异常]]></title>
    <url>%2F2018%2F09%2F04%2Fgo%E8%AF%AD%E8%A8%80%E9%94%99%E8%AF%AF%E6%8D%95%E8%8E%B7%E5%92%8Cpanic%E5%BC%82%E5%B8%B8%2F</url>
    <content type="text"><![CDATA[在Go语言中我们首先来看一下err错误信息，我们先来看一段代码 123456789101112131415161718192021222324252627package mainimport ( "fmt" "errors")func calc(a int, b int) (v int, err error) &#123; //捕获错误信息 if b == 0 &#123; //如果代码中出现错误 可以使用errors.New()创建错误信息 err = errors.New("除数不能为0") return &#125; v = a / b return&#125;func main() &#123; a := 10 b := 0 v, err := calc(a, b) //根据错误信息进行处理 if err != nil &#123; fmt.Println(err) &#125; else &#123; fmt.Println(v) &#125; //fmt.Println(v)&#125; 在这里我们可以接收到错误信息并打印出来，我们先看一下会报错吗？结果显示部会报错的，因为我们接收到了错误并打印出来了 我们看这里并没有报错，而是打印出了错误 信息， 在Go中我们还可以直接调用panic函数来终止程序， 我们来看这张图，这张图里面我们给数组定义为10个长度，然后下面直接调用下标为10的数组，这样会出什么错误呢，这样就是数组下标超出范围，因为数组下标是从0开始到长度减一，我们来看一下编译器运行的结果报什么错， 编译提示panic异常，然后提示数组下标越界，这是因为当我们写程序时，比如遇到一些错误比如：数组下标越界，空指针异常，野指针这些错误的时候，系统就会调用自己本身的panic函数，那么我们自己在写程序的时候也是可以调用panic函数的，下面来看这段代码 12345678910111213package mainimport "fmt"func main() &#123; fmt.Println("hello world1") fmt.Println("hello world2") fmt.Println("hello world3") //程序可以运行 但是遇到panic停止 //当程序遇到panic时 会自动崩溃 panic("终止程序") fmt.Println("hello world4") fmt.Println("hello world5") fmt.Println("hello world6")&#125; 我们来看一下运行结果 在这里我们看到，只打印了上面的三句话，当遇到panic函数的时候就会程序崩溃，然后下面的程序停止执行，我们不仅仅可以使用panic来终止程序，我们还可以捕获错误后继续执行程序，我们来看下一段代码 12345678910111213141516171819202122package mainimport "fmt"func test(i int) &#123; var arr [10]int //优先使用错误拦截 在错误出现之前进行拦截 在错误出现后进行错误捕获 //错误拦截必须配合defer使用 通过匿名函数使用 defer func() &#123; //恢复程序的控制权 err := recover() if err != nil &#123; fmt.Println(err) &#125; &#125;() arr[i] = 123 //err panic fmt.Println(arr)&#125;func main() &#123; i := 10 test(i) fmt.Println("hello world")&#125; 看这段代码，然后我们来看一下运行结果 第一句话直接打印出了错误：运行时错误，数组下标越界，但是程序并没有终止而是继续运行下去了这是为什么了， 如图所示，这里我们延迟调用了一下，因为recover必须和defer配合使用，并且调用一定要在错误出现之前调用才有效果，这样捕获到了错误并且恢复了程序的控制权。]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>错误捕获</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go语言切片深入讲解]]></title>
    <url>%2F2018%2F09%2F03%2Fgo%E8%AF%AD%E8%A8%80%E5%88%87%E7%89%87%E6%B7%B1%E5%85%A5%E8%AE%B2%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[我们在上一篇的切片讲解中，我们讲解到在go语言中 map和切片都是传引用（地址），也就是在调用函数的时候都是可以直接修改变量的值，关于切片，在某种程度上表面上来说也是可以这样说的，我们先来看一下一个小小的例题 1234567891011package mainimport &quot;fmt&quot;func Change(s []int)&#123; s[0]=11 s[1]=22&#125;func main()&#123; slice:=[]int&#123;1,2,3,4,5&#125; Change(slice) fmt.Println(slice)&#125; 我们先来看一下结果 我们可以看到切片当作函数参数的时候调用之后值确实改变了，这也间接的可以认为切片是地址传递，但是我们想要了解的更深入的话可以继续了解下去 我们继续来看一个小例子 123456789101112package mainimport "fmt"func Add(s []int)&#123; s=append(s,6,7,8)&#125;func main()&#123; slice:=[]int&#123;1,2,3,4,5&#125; Add(slice) fmt.Println(slice)&#125; 在这里函数调用了append这个函数来增加切片的个数， 我们可以清晰的看到打印的结果并没有变，我们在之前讲到过这里是因为append扩容使得地址发生了变化，所以不是指向原来的切片也就导致了并不是在原来的切片上面增加了，这就说到了切片的本质，在这里详细说一下，切片的本质不是指向数组的指针，而是一种新定义的一种数据结构，这个数据结构里面包含一个指针，len，还有cap， 12345type slice struct &#123; *Pointer len cap &#125; 看到没，切片的本质是这样一个数据结构，而且在函数调用的时候切片做的其实是一个值的传递！！！只不过这个值是一个包含指针，长度，容量的一个结构体的值，这样我们一想就可以一目了然的知道了为什么前面我们所说的切片是地址传递了吧，那是因为他传的那个值里面包含一个指针，所以函数调用的时候就可以用这个值里面的指针来操作原来的切片，我们看如下的一张图片， 我们看上面的图就可以更加的明白了，在函数调用的时候首先，在栈区里面main函数会得到一块内存（栈帧），然后调用testFunc函数的时候testFunc也会得到一块内存（栈帧），然后调用的时候把切片的值传递给形参，注意这里的值是包含一个指针，长度，容量的结构体值，我们在使用一般操作的时候不会改变那个地址，所以会正常操作main函数里面的切片，当我们使用append函数的时候就会导致保存的指针值发生变化，那样就会保存一个新的地址，操作也会在新的地方操作，这样的话原来的切片就不会发生变化，当testFunc函数调用完毕后，我们的testFunc函数就会释放，而原来的切片也没有得到改变，这就是我们所看到的，这才是切片的本质 所以最后得到的总结就是：切片当作参数传递的时候是值传递，但是这个值不是一个普通的值，而是一个包含指针，长度，容量的值，如果有不懂的也可以尝试着去看一看源码。 附图：（内存的微讲解）]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go语言切片微讲解]]></title>
    <url>%2F2018%2F09%2F03%2Fgo%E8%AF%AD%E8%A8%80%E5%88%87%E7%89%87%E5%BE%AE%E8%AE%B2%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[传递参数时分为值传递和地址传递，go语言中切片和map是地址传递，但是切片传递要有一个注意事项 例如： 123456789package mainfunc test(a []int)&#123; a=append(a,1,2,3)&#125;func main()&#123; var s []int=[]int&#123;89,4,5,6&#125; test(s) fmt.Println(s)&#125; 在这里里面为什么调用函数后切片没有变化呢，切片不是地址传递吗？这是因为在test函数里面用了append()函数,在调用函数时，在栈区里面把1 2 3 添加到a里面然后重新分配了地址，而原来的s切片还是指向原来地址，根本没有变，所以在main函数里面打印出s还是原来的，不会改变，那么如何做到用了append后改变原来切片的值呢 如下 1234567891011package mainimport "fmt"func test(a []int)(b []int)&#123; b=append(a,1,2,3,7) return&#125;func main()&#123; var s []int=[]int&#123;9,10&#125; s=test(s) fmt.Println(s)&#125; 我们可以用return 把改变后的地址传回去这样就可以了 切片用append函数的时候一定要注意，因为如果容量不足的时候会自动扩充，如果原来的地址后面没有足够的空间那么就会重新找一个足够大的空间来储存，所以切片利用append的时候地址是有可能变化的。]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[插入排序]]></title>
    <url>%2F2018%2F09%2F02%2F%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[插入排序在上一章中我们讲了算法排序中的最简单的冒泡排序，今天我们来讲解一下插入排序，后续将讲解快速排序，归并排序，希尔排序，二叉排序，这些等等，后续的排序都是在时间复杂度和空间复杂度上面优于这两种的，所以我们今天先来讲解一下插入排序 我们先来看以下的一张图 为了方便排序，我们一般将数据的第一个元素作为有序组，其他视为待插入组，图中以升序为例子进行讲解。 我们将第一个元素作为有序的数组，然后将后面的元素视为无序的，将后面的无序组第一个元素和有序组最后一个元素比较，如果符合要求就插入进去然后有序组就多一个，无序组就少一个 第二次排序的时候有序组就为两个元素，有序组的最后一个元素拿出来继续和无序组的第一个相比，然后再插入一个，这样有序组就又多了一个，无序组少一个 这样一直循环到某个条件，这样无序组就没有了，剩下的都是有序组，这样排序就完成了。 我们来看一下代码怎样实现，在这里我们就用GO语言来实现，在某些方面个人觉得go写的代码比C/C++要少很多，更加方便一点 12345678910111213141516package mainimport "fmt"func main() &#123; var arr [10]int = [10]int&#123;9, 1, 3, 4, 7, 5, 2, 10, 11, 8&#125; //插入排序 var temp ,j int //临时变量temp for i := 1; i &lt; len(arr); i++ &#123; //遍历无序数组,下标1开始 if arr[i] &lt; arr[i-1] &#123; //无序组第一个小于有序组最后一个才进入否则直接下一个元素 temp=arr[i] //用变量temp取出arr[i]的元素值 for j=i-1;j&gt;=0&amp;&amp;arr[j]&gt;temp;j--&#123; //这里面temp不能写成arr[i]是因为下面 arr[j+1]=arr[j] // 有一个arr[j+1]=arr[j]那样会导致arr[i]会变 &#125; arr[j+1]=temp //因为上面经过了j--所以这里需要arr[j+1]，for循环后就找到位置填充temp，也就是之前取出来的arr[i] &#125; &#125; fmt.Println(arr)&#125; 下面你可以自己去实现一下了，后续将讲解更难的排序方法。]]></content>
      <categories>
        <category>数据结构和算法</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>排序问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode|无重复字符的最长子串]]></title>
    <url>%2F2018%2F09%2F02%2F%E6%AF%8F%E6%97%A5%E4%B8%80%E9%81%93%E7%BC%96%E7%A8%8B%E7%AE%97%E6%B3%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[今天的题目为： 给定一个字符串，找出不含有重复字符的最长子串的长度。 示例 1: 123输入: &quot;abcabcbb&quot;输出: 3 解释: 无重复字符的最长子串是 &quot;abc&quot;，其长度为 3。 示例 2: 123输入: &quot;bbbbb&quot;输出: 1解释: 无重复字符的最长子串是 &quot;b&quot;，其长度为 1。 示例 3: 1234输入: "pwwkew"输出: 3解释: 无重复字符的最长子串是 "wke"，其长度为 3。 请注意，答案必须是一个子串，"pwke" 是一个子序列 而不是子串。 首先来思路分析：我们可以先第一次把所有不重复的字符串分割一下保存下来，这样就可以找到这一次的不重复的子字符串最长的 然后在一次一次往后面移动，这样就可以找出所有不重复的子字符串，最后再求出最大值。 比如： abcd ahjiklo字符串 我们第一次从头开始寻找，找到不重复的子字符串，那就有两个，一个为abcd另外一个为ahjiklo,如果单单重上面看那么最长的不重复子字符串就是ahjiklo，但是我们需要的不是这个，那么我们就需要再循环一次，每次把第一个字符去掉然后再寻找，比如这一次把a去掉那么找出的最长子字符串就是bcdahjiklo，这个是最长的，然后再把本次的第一个字符去掉一直循环，这样到最后找出最长的子字符串， 复杂度分析：当然，这种方法可以算是一种暴力解决的方法，没有什么技巧性，时间复杂度也是最复杂的，O（n3）的复杂度，当然如果想要优化的话可以自己去研究一下奥。 代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344package mainimport "fmt"func lengthOfLongestSubstring(str string) int &#123; //每次求出的最大值返回 s:=[]byte(str) //先把字符串转为byte类型的切片 slice:=make([]string,0)//ased//aerfch//risdud //定义一个字符串切片，可以把无重复的字符串字段保存进去flag: for i:=0;i&lt;len(s);i++&#123; for j:=i-1;j&gt;=0;j--&#123; if s[i]==s[j]&#123; //这里遍历用来找出无重复字符串段 slice=append(slice,string(s[:i])) //把无重复字符段放切片里面去 s=s[i:] //切片往后移 goto flag; //在新切片里面再次循环直到找出无重复字符段放进字符串切片里面 &#125; &#125; &#125; slice=append(slice,string(s)) //把最后一个放进切片，如果整个字符串都没有重复那么就这一个 //fmt.Println(slice) max:=len(slice[0]) for k:=0;k&lt;len(slice);k++&#123; //循环找出这一次的最大值 if len(slice[k])&gt;max&#123; max=len(slice[k]) &#125; &#125; return max //返回本次最大值&#125;func main()&#123; var str string fmt.Printf("请输入一个字符串\n") fmt.Scanf("%s",&amp;str) var max int for i:=len(str);i&gt;0;i--&#123; tmp:=lengthOfLongestSubstring(str) //每求一次最大值往后退一次，确保能得到真正的最大值 if tmp&gt;max&#123; max=tmp &#125; str=str[1:] //每次用切片割掉第一个元素 &#125; fmt.Println(max) //输出最大值，这里也可以输出最长的子字符串&#125; 这样就可以求出来了，如果想要优化的伙伴可以自己去稍加研究。]]></content>
      <categories>
        <category>数据结构和算法</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>编程题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一条简单区块链的实现]]></title>
    <url>%2F2018%2F09%2F02%2F%E4%B8%80%E6%9D%A1%E7%AE%80%E5%8D%95%E5%8C%BA%E5%9D%97%E9%93%BE%E7%9A%84%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[在上一章中我们讲解了一个简单的区块创建，那么我们今天来讲解一下一条简单区块链的实现 思路分析： 创建一个创世块，就是区块链的头 //上一章节中讲解了如果实现一个区块的简单实现 定义一个结构体，用来保存区块链中的区块，结构体里面的元素可以就是那条链 用方法来实现区块的添加，每次调用方法都加进相应的区块 第一步实现 1234567type Block struct &#123; //创建一个区块的结构体 Time int64 //时间戳 Data []byte //数据信息 PreviousHash []byte //前一个哈希值 Hash []byte //当前的哈希&#125; 第二步实现 123type Blockchain struct&#123; //创建一个区块链类型 blocks []*Block //一系列区块储存，这里用切片来保存&#125; 第三步实现 123456789101112func (blockchain *Blockchain)Addblock(data string)&#123; //添加区块的方法 newblock:=Block&#123;&#125; //一个新区块 newblock.Data=[]byte(data) //初始化数据 newblock.PreviousHash=blockchain.blocks[len(blockchain.blocks)-1].Hash //得到前一个区块的哈希 newblock.Sethash() //得到哈希 blockchain.blocks=append(blockchain.blocks,&amp;newblock) //新区块添加到这条链当中&#125;func Newblockchain()*Blockchain&#123; // 创建一条新的区块链 blos:=Blockchain&#123;[]*Block&#123;Firstblock()&#125;&#125; //初始化 return &amp;blos&#125; 这样新的区块链就完成的差不多了，加上上一章的简单实现区块链的代码就已经实现了，下面我们整理一下将得到如下的代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package mainimport ( "time" "strconv" "bytes" "crypto/sha256" "fmt")/*一个简单的区块链创建*/type Block struct &#123; //创建一个区块的结构体 Time int64 //时间戳 Data []byte //数据信息 PreviousHash []byte //前一个哈希值 Hash []byte //当前的哈希&#125;type Blockchain struct&#123; //创建一个区块链类型 blocks []*Block //一系列区块储存，这里用切片来保存&#125;func (blockchain *Blockchain)Addblock(data string)&#123; //添加区块的方法 newblock:=Block&#123;&#125; //一个新区块 newblock.Data=[]byte(data) //初始化数据 newblock.PreviousHash=blockchain.blocks[len(blockchain.blocks)-1].Hash //得到前一个区块的哈希 newblock.Sethash() //得到哈希 blockchain.blocks=append(blockchain.blocks,&amp;newblock) //新区块添加到这条链当中&#125;func Newblockchain()*Blockchain&#123; // 创建一条新的区块链 blos:=Blockchain&#123;[]*Block&#123;Firstblock()&#125;&#125; //初始化 return &amp;blos&#125;func Firstblock()*Block&#123; firstblock:=Newblock("firstblock",[]byte&#123;&#125;) return firstblock&#125;func (block *Block)Sethash()&#123; timer:=[]byte(strconv.FormatInt(block.Time,10)) herds:=bytes.Join([][]byte&#123;timer,[]byte(block.Data),block.PreviousHash&#125;,[]byte&#123;&#125;) hash:=sha256.Sum256(herds) block.Hash=hash[:]&#125;func Newblock(data string,prevhash []byte)*Block&#123; block:=Block&#123;&#125; block.Time=time.Now().Unix() block.Data=[]byte(data) block.PreviousHash=prevhash block.Sethash() return &amp;block&#125;func main() &#123; //创建一个区块链 blocks:=Newblockchain() blocks.Addblock("seng one BTC to sary") //信息（数据）为seng one BTC to sary 添加到区块链中 blocks.Addblock("send two ETH to wuman")//信息（数据）为send two ETH to wuman 添加到区块链中 blocks.Addblock("send one ADA to zijian")//信息（数据）为send one ADA to zijian 添加到区块链中 for _,v:=range blocks.blocks&#123; //循环遍历打印一下看结果 fmt.Println("=======================================") fmt.Printf("data=:%s\n",v.Data) fmt.Printf("prevhash:=%x\n",v.PreviousHash) fmt.Printf("hash:=%x\n",v.Hash) &#125;&#125; 我们来看一下结果：]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个简单区块的实现]]></title>
    <url>%2F2018%2F09%2F01%2F%E5%8C%BA%E5%9D%97%E9%93%BE%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B01%2F</url>
    <content type="text"><![CDATA[简单区块实现区块链技术如今已经越来越成熟，但是我们怎么深入到本质用技术的眼光来看待区块链技术，见名知意，区块链的意思就是用链条把区块链接起来，那我们先用代码来看一下，今天我们用go语言来简单的实现一个区块并打印。 我们可以先来理一下思路，我们想要实现一个区块该怎么办，思路理好然后再来代码一步一步实现 创建一个结构体来保存一个区块的信息 //大概包括时间戳，数据，前哈希，本哈希这几个数据 创建第一个区块并给其中的数据赋值，也就相当于一个创世块，注意这里创世块的前哈希传一个空值就可以 给这个区块的数据处理一下然后加密得到本区块的哈希 主函数里面打印看一下本区块的哈希 //哈希用16进制打印 大概这样思路就可以理顺了，然后我们就可以一步一步实现了 1：第一步创建一个区块结构体 1234567type Block struct &#123; //创建一个区块结构体 Timer int64 //时间戳 Data []byte //数据 prevHash []byte //前一个区块的哈希值 Hash []byte //本区块的哈希值&#125; 区块结构体创建完成，继续下一步 2：创建第一个区块 1234func Firstblosk() *Block &#123; //创建第一个区块信息，相当于一个创始块 firstblock := NewBlock("This is firstblock", []byte&#123;&#125;) //传入参数，返回结构体指针类型 return firstblock //返回的是结构体指针类型&#125; 123456789func NewBlock(data string, prevhash []byte) *Block &#123; //创建区块的函数 block1 := Block&#123;&#125; //创建一个区块结构体 block1.Timer = time.Now().Unix() //得到时间 block1.Data = []byte(data) //传入数据参数 block1.prevHash = prevhash //前一个哈希值为传入的数据 block1.setHash() //setHash 方法加密得到自己的hash return &amp;block1 //返回区块指针&#125; 用来创建第一个区块 3：给区块信息数据处理 123456func (block *Block) setHash() &#123; time := []byte(strconv.FormatInt(block.Timer, 10)) //将区块的时间转为字符切片类型，方便加密 heards := bytes.Join([][]byte&#123;time, block.Data, block.prevHash&#125;, []byte&#123;&#125;) //将时间，数据，前一个哈希拼接一下 hash := sha256.Sum256(heards) //用sha256包的Sum256函数加密 block.Hash = hash[:] //加密后的直接赋值给本哈希&#125; 4:主函数里面打印看一下本区块的哈希 //哈希用16进制打印 12345func main() &#123; firstblock := Firstblosk() fmt.Printf("%x",string(firstblock.Hash)) //16进制打印&#125; 这样一个简单的区块就创建成功了，我们把所有代码连接起来然后来看一下打印结果 123456789101112131415161718192021222324252627282930313233343536373839404142package main/*一个简单的区块创建实现*/import ( "time" "strconv" "bytes" "crypto/sha256" "fmt")type Block struct &#123; //创建一个区块结构体 Timer int64 //时间戳 Data []byte //数据 prevHash []byte //前一个区块的哈希值 Hash []byte //本区块的哈希值&#125;func (block *Block) setHash() &#123; time := []byte(strconv.FormatInt(block.Timer, 10)) //将区块的时间转为字符切片类型，方便加密 heards := bytes.Join([][]byte&#123;time, block.Data, block.prevHash&#125;, []byte&#123;&#125;) //将时间，数据，前一个哈希拼接一下 hash := sha256.Sum256(heards) //用sha256包的Sum256函数加密 block.Hash = hash[:] //加密后的直接赋值给本哈希&#125;func Firstblosk() *Block &#123; //创建第一个区块信息，相当于一个创始块 firstblock := NewBlock("This is firstblock", []byte&#123;&#125;) //传入参数，返回结构体指针类型 return firstblock //返回的是结构体指针类型&#125;func NewBlock(data string, prevhash []byte) *Block &#123; //创建区块的函数 block1 := Block&#123;&#125; //创建一个区块结构体 block1.Timer = time.Now().Unix() //得到时间 block1.Data = []byte(data) //传入数据参数 block1.prevHash = prevhash //前一个哈希值为传入的数据 block1.setHash() //setHash 方法加密得到自己的hash return &amp;block1 //返回区块指针&#125;func main() &#123; firstblock := Firstblosk() fmt.Printf("%x",string(firstblock.Hash)) //16进制打印&#125; 我们来看一下哈希打印结果 这样一个简单的区块就实现了，那么如果要实现一个简单的区块链呢？其实也按照这样的思路写下去也很容易实现，记住：区块链的本区块的哈希是下一个区块的前哈希，这样链接，下一章我们将讲解一个简单的区块链实现。]]></content>
      <categories>
        <category>区块链</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>区块链</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo+github博客搭建]]></title>
    <url>%2F2018%2F09%2F01%2F%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA1%2F</url>
    <content type="text"><![CDATA[技术小白搭建个人博客 github+hexo本文主要讲解博客的搭建过程，next主题优化，next配置文件详解等。 不做过多介绍了，快速开始 准备安装软件依次安装 1、Node.js 2、Git 注册github访问https://github.com/ 右上角signup uername 最好都用小写，因为最后建立的博客地址是：http://username.github.io；邮箱十分重要，GitHub 上很多通知都是通过邮箱的。 创建Repository Repository 名字应该是http://username.github.io。比如我的username 就是wumansgy 其他的可以选择添加一些描述也可以选择默认什么也不添加 ，点击creat repository 配置和使用Github开始—所有应用—找到git bash 配置SSH keysssh keys就是用来使本地git 项目与github联系 1. 检查SSH keys的设置首先要检查自己电脑上现有的 SSH key： 1$ cd ~/. ssh 如果显示“No such file or directory”，说明这是你第一次使用 git 2、生成新的 SSH Key：123$ ssh-keygen -t rsa -C &quot;邮件地址@youremail.com&quot;Generating public/private rsa key pair.Enter file in which to save the key (/Users/your_user_directory/.ssh/id_rsa):&lt;回车就好&gt; 【提示1】这里的邮箱地址，输入注册 Github 的邮箱地址； 【提示2】「-C」的是大写的「C」 然后系统会要你输入密码： 12Enter passphrase (empty for no passphrase):&lt;设置密码&gt;Enter same passphrase again:&lt;再次输入密码&gt; 在回车中会提示你输入一个密码，这个密码会在你提交项目时使用，如果为空的话提交项目时则不用输入。这个设置是防止别人往你的项目里提交内容。 注意：输入密码的时候没有输入痕迹的，不要以为什么也没有输入。 最后看到这样的界面，就成功设置ssh key了： 3、添加SSH Key到GitHub在本地文件夹找到id_rsa.pub文件，看上面的图片第四行的位置告诉你存在哪里了 没找到的勾选一下文件扩展名 隐藏的项目 .ssh文件夹里记事本打开这个文件复制全部内容到 github相应位置。不要着急…（记得期末考试复习概率论看汤家凤老师的视频时老师的口头禅…） 你的github主页 点击头像后边的箭头（为什么我每次想要上传头像都没反应呢？希望有知道的小伙伴能看到告诉我一下） Title最好写，随便写。网上有说不写title也有可能后期出现乱七八糟的错误 Key部分就是放刚才复制的内容啦 点击Add SSH key 测试git bash 里 输入以下代码 不要改任何一个字 我就是自作聪明以为代表的是自己注册时候的邮箱然后… 1$ ssh -T git@github.com 如果得到以下反馈 123The authenticity of host &apos;GitHub.com (207.97.227.239)&apos; can&apos;t be established.RSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48.Are you sure you want to continue connecting (yes/no) 输入yes回车 1Enter passphrase for key &apos;/c/Users/lenovo/.ssh/id_rsa&apos;: 输入刚才设置的密码回车 设置用户信息现在已经可以通过 SSH 链接到 GitHub 啦!当然还需要完善一些个人信息: 12$ git config --global user.name &quot;wuyalan&quot;//输入注册时的username$ git config --global user.email &quot;alan.wyl@foxmail.com&quot;//填写注册邮箱 GitHub 也是用这些信息来做权限的处理，输入下面的代码进行个人信息的设置，把名称和邮箱替换成你自己的，名字必须是你的真名，而不是GitHub的昵称。 SSH Key配置成功本机已成功连接到 github。 如有问题，请重新设置。常见错误请参考： 错误1 错误2 搭建hexo博客利用npm命令安装hexo 12$ cd$ npm install -g hexo 1. 创建独立博客项目文件夹 安装完成后，关掉前面那个 Git Bash 窗口。在本地创建一个与 Repository 中博客项目同名的文件夹（如E:[http://username.github.io]）在文件夹上点击鼠标右键，选择 Git bash here； 【提示】在进行博客搭建工作时，每次使用命令都要在 H:[http://username.github.io] 目录下。 执行下面的指令，Hexo 就会自动在 H:[http://username.github.io]文件夹建立独立博客所需要的所有文件啦！ 1$ hexo init 2. 安装依赖包 1$ npm install 3. 确保git部署 1$ npm install hexo-deployer-git --save 4.本地查看 现在已经搭建好本地的 Hexo 博客了，执行完下面的命令就可以到浏览器输入 localhost:4000 查看到啦 12$ hexo g$ hexo s hexo g 每次进行相应改动都要hexo g 生成一下 hexo s 启动服务预览 5. 用Hexo克隆主题 执行完 hexo init 命令后会给一个默认的主题：landscape 你可以到官网找你喜欢的主题进行下载 hexo themes 知乎：有哪些好看的 Hexo 主题？ 找到它所在的 Github Repository （怎么找，我喜欢的那个，恰好博主放了他的github地址，emmm） 找到之后通过git命令下载 在主题的repository点击clone 复制一下那个地址 1$ git clone +复制的地址+themes/archer 后面就是clone之后放到你本地的博客文件夹themes文件夹下 名字纹archer的文件 我下载的是archer主题~（有喜欢同样的小伙伴在个性化自己主题的时候欢迎来交流一下呀~真的是技术小白~还没研究清楚要怎么改，不过主题作者也会在readme说明的，细心看就是） 6. 修改整站配置文件 自己把 http://blog.io 中文件都点开看一遍，主要配置文件是 _config.yml ，可以用记事本打开，推荐使用 sublime 或者nodepad++打开。 修订清单如下，文档内有详细注释，可按注释逐个修订 博客名字及作者信息：_config.yml 个人介绍页面：about.md 代表作页面：milestone.md 博客参考 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889这里贴一份网上看到的 可以复制替换原来的 但是替换之前最好备份 可能会出错那要么你就对照着看一下改就好# Hexo Configuration## Docs: http://zespia.tw/hexo/docs/configure.html## Source: https://github.com/tommy351/hexo/# Site 这里的配置，哪项配置反映在哪里，可以参考我的博客title: My Blog #博客名subtitle: to be continued... #副标题description: My blog #给搜索引擎看的，对网站的描述，可以自定义author: Yourname #作者，在博客底部可以看到email: yourname@yourmail.com #你的联系邮箱language: zh-CN #中文。如果不填则默认英文# URL #这项暂不配置，绑定域名后，欲创建sitemap.xml需要配置该项## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;url: http://yoursite.comroot: /permalink: :year/:month/:day/:title/tag_dir: tagsarchive_dir: archivescategory_dir: categories# Writing 文章布局、写作格式的定义，不修改new_post_name: :title.md # File name of new postsdefault_layout: postauto_spacing: false # Add spaces between asian characters and western characterstitlecase: false # Transform title into titlecasemax_open_file: 100filename_case: 0highlight: enable: true backtick_code_block: true line_number: true tab_replace:# Category &amp; Tagdefault_category: uncategorizedcategory_map:tag_map:# Archives 默认值为2，这里都修改为1，相应页面就只会列出标题，而非全文## 2: Enable pagination## 1: Disable pagination## 0: Fully Disablearchive: 1category: 1tag: 1# Server 不修改## Hexo uses Connect as a server## You can customize the logger format as defined in## http://www.senchalabs.org/connect/logger.htmlport: 4000logger: falselogger_format:# Date / Time format 日期格式，可以修改成自己喜欢的格式## Hexo uses Moment.js to parse and display date## You can customize the date format as defined in## http://momentjs.com/docs/#/displaying/format/date_format: YYYY-M-Dtime_format: H:mm:ss# Pagination 每页显示文章数，可以自定义，贴主设置的是10## Set per_page to 0 to disable paginationper_page: 10pagination_dir: page# Disqus Disqus插件，我们会替换成“多说”，不修改disqus_shortname:# Extensions 这里配置站点所用主题和插件，暂时默认## Plugins: https://github.com/tommy351/hexo/wiki/Plugins## Themes: https://github.com/tommy351/hexo/wiki/Themestheme: landscapeexclude_generator:plugins:- hexo-generator-feed- hexo-generator-sitemap# Deployment 站点部署到github要配置## Docs: http://zespia.tw/hexo/docs/deploy.htmldeploy: type: git repository: branch: master 7. 启用新下载的主题 在刚打开的的_config.yml 文件中，找到“# Extensions”，把默认主题 landscape 修改为刚刚下载下来的主题名： 【提示】http://username.github.io 里有两个 config.yml 文件，一个在根目录，一个在 theme 下，现在修改的是在根目录下的。 8. 更新主题 git bash 里执行 12$ cd themes/主题名$ git pull 9. 本地查看调试 每次修改都要hexo g 生成一下 12$ hexo g #生成$ hexo s #启动本地服务，进行文章预览调试，退出服务用Ctrl+c 浏览器输入 localhost：4000 预览效果 将博客部署到http://username.github.io1. 复制SSH码进入 Github 个人主页中的 Repository，复制新建的独立博客项目:http://username.github.io 的 SSH 码 2. 编辑整站配置文件打开 H:\username.github.io_config.yml,把刚刚复制的 SSH 码粘贴到“repository：”后面，别忘了冒号后要空一格。 1234deploy: type: git repository: git@github.com:username/username.github.io.git branch: master 3. 执行下列指令即可完成部署。【提示】每次修改本地文件后，需要 hexo g 才能保存。每次使用命令时，都要在你的博客文件夹目录下 12$ hexo g$ hexo d （ps：我在第一次hexo d 的时候出现了错误，具体错误提示忘了，原因是我没有deploy 的权限 在repository的setting （这里我有一点小疑惑 为什么delete不了这个公钥呢，我想要delete是因为第一次设置时没有勾选 ..如下 emm里面的内容就是重复配置SSH key的步骤，记得勾选这个小框框，我就是没有勾选设置之后还是没有deploy成功 ） 因为我看到的教程里大多数没有讲这一部分，所以我也不确定这一步是否必须，如果有遇到相同问题的小伙伴可以参考 ） 【提示】如果在配置 SSH key 时设置了密码，执行 hexo d 命令上传文件时需要输入密码进行确认，会出现一个小框框。 输入密码之后在浏览器输入： username.github.io 如果得到你想要的效果，那么恭喜你，博客已经搭建好啦！ 允许你偷偷激动一下…哈哈哈 之后就是写博文了，我还没开始…要好好写博客好好写博客 你看技术大神们哪个没有自己的优秀博客。 不懂技术的小伙伴也可以在自己的小天地写文，很爽又很有逼格是不是~ 我的博客地址：进入 next主题使用及优化启用主题与所有 Hexo 主题启用的模式一样。 当 克隆/下载 完成后，打开 站点配置文件， 找到 theme 字段，并将其值更改为 next。 启用 NexT 主题 1theme: next 到此，NexT 主题安装完成。下一步我们将验证主题是否正确启用。在切换主题之后、验证之前， 我们最好使用 hexo clean 来清除 Hexo 的缓存。 选择 SchemeScheme 是 NexT 提供的一种特性，借助于 Scheme，NexT 为你提供多种不同的外观。同时，几乎所有的配置都可以 在 Scheme 之间共用。目前 NexT 支持三种 Scheme，他们是： Muse - 默认 Scheme，这是 NexT 最初的版本，黑白主调，大量留白 Mist - Muse 的紧凑版本，整洁有序的单栏外观 Pisces - 双栏 Scheme，小家碧玉似的清新 Scheme 的切换通过更改 主题配置文件，搜索 scheme 关键字。 你会看到有三行 scheme 的配置，将你需用启用的 scheme 前面注释 # 去除即可。 选择 Pisces Scheme 123#scheme: Muse#scheme: Mistscheme: Pisces 设置 语言编辑 站点配置文件， 将 language 设置成你所需要的语言。建议明确设置你所需要的语言，例如选用简体中文，配置如下： 1language: zh-CN Local Search添加百度/谷歌/本地 自定义站点内容搜索 安装 hexo-generator-searchdb，在站点的根目录下执行以下命令： 1$ npm install hexo-generator-searchdb --save 编辑 站点配置文件，新增以下内容到任意位置： 12345search: path: search.xml field: post format: html limit: 10000 编辑 主题配置文件，启用本地搜索功能： 123# Local searchlocal_search: enable: true 文章模块的美化文章内代码美化 行内代码在主题目录下，将source/css/_custom/custom.styl文件修改如下： 123456789//行内代码样式code &#123; color: #ff7600; background: #fbf7f8; border: 1px solid #d6d6d6; padding:1px 4px; word-break: break-all; border-radius:4px;&#125; 区块代码在主题目录下，修改config.yml文件： 12# 样式可选： normal | night | night eighties | night blue | night brighthighlight_theme: night 文章结束语 添加模块文件 在主题目录下layout/_macro中新建 passage-end-tag.swig文件,并添加以下内容： 1234567&lt;div&gt; &#123;% if not is_index %&#125; &lt;div style=&quot;text-align:center;color: #ccc;font-size:14px;&quot;&gt; -------------本文结束&lt;i class=&quot;fa fa-paw&quot;&gt;&lt;/i&gt;感谢您的阅读------------- &lt;/div&gt; &#123;% endif %&#125;&lt;/div&gt; 导入模板文件 在layout/_macro/post.swig文件中，找到： 123&#123;#####################&#125;&#123;### END POST BODY ###&#125;&#123;#####################&#125; 在上面代码之前添加： 12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include &apos;passage-end-tag.swig&apos; %&#125; &#123;% endif %&#125;&lt;/div&gt; 配置在主题配置文件中添加： 123# 文章末尾添加“本文结束”标记passage_end_tag: enabled: true 增强文章底部版权信息 增加文章md文件的头部信息中添加copyright: true时，添加版权声明 增加文章标题、发布时间、更新时间等信息 在目录 next/layout/_macro/下添加 my-copyright.swig： 123456789101112131415161718192021222324252627282930&#123;% if page.copyright %&#125;&lt;div class=&quot;my_post_copyright&quot;&gt; &lt;script src=&quot;//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js&quot;&gt;&lt;/script&gt; &lt;!-- JS库 sweetalert 可修改路径 --&gt; &lt;script src=&quot;https://cdn.bootcss.com/jquery/2.0.0/jquery.min.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;https://unpkg.com/sweetalert/dist/sweetalert.min.js&quot;&gt;&lt;/script&gt; &lt;p&gt;&lt;span&gt;本文标题:&lt;/span&gt;&lt;a href=&quot;&#123;&#123; url_for(page.path) &#125;&#125;&quot;&gt;&#123;&#123; page.title &#125;&#125;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;文章作者:&lt;/span&gt;&lt;a href=&quot;/&quot; title=&quot;访问 &#123;&#123; theme.author &#125;&#125; 的个人博客&quot;&gt;&#123;&#123; theme.author &#125;&#125;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;发布时间:&lt;/span&gt;&#123;&#123; page.date.format(&quot;YYYY年MM月DD日 - HH:MM&quot;) &#125;&#125;&lt;/p&gt; &lt;p&gt;&lt;span&gt;最后更新:&lt;/span&gt;&#123;&#123; page.updated.format(&quot;YYYY年MM月DD日 - HH:MM&quot;) &#125;&#125;&lt;/p&gt; &lt;p&gt;&lt;span&gt;原始链接:&lt;/span&gt;&lt;a href=&quot;&#123;&#123; url_for(page.path) &#125;&#125;&quot; title=&quot;&#123;&#123; page.title &#125;&#125;&quot;&gt;&#123;&#123; page.permalink &#125;&#125;&lt;/a&gt; &lt;span class=&quot;copy-path&quot; title=&quot;点击复制文章链接&quot;&gt;&lt;i class=&quot;fa fa-clipboard&quot; data-clipboard-text=&quot;&#123;&#123; page.permalink &#125;&#125;&quot; aria-label=&quot;复制成功！&quot;&gt;&lt;/i&gt;&lt;/span&gt; &lt;/p&gt; &lt;p&gt;&lt;span&gt;许可协议:&lt;/span&gt;&lt;i class=&quot;fa fa-creative-commons&quot;&gt;&lt;/i&gt; &lt;a rel=&quot;license&quot; href=&quot;https://creativecommons.org/licenses/by-nc-nd/4.0/&quot; target=&quot;_blank&quot; title=&quot;Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)&quot;&gt;署名-非商业性使用-禁止演绎 4.0 国际&lt;/a&gt; 转载请保留原文链接及作者。&lt;/p&gt; &lt;/div&gt;&lt;script&gt; var clipboard = new Clipboard(&apos;.fa-clipboard&apos;); $(&quot;.fa-clipboard&quot;).click(function()&#123; clipboard.on(&apos;success&apos;, function()&#123; swal(&#123; title: &quot;&quot;, text: &apos;复制成功&apos;, icon: &quot;success&quot;, showConfirmButton: true &#125;); &#125;); &#125;); &lt;/script&gt;&#123;% endif %&#125; 在目录next/source/css/_common/components/post/下添加my-post-copyright.styl： 123456789101112131415161718192021222324252627282930313233343536373839404142434445.my_post_copyright &#123; width: 85%; max-width: 45em; margin: 2.8em auto 0; padding: 0.5em 1.0em; border: 1px solid #d3d3d3; font-size: 0.93rem; line-height: 1.6em; word-break: break-all; background: rgba(255,255,255,0.4);&#125;.my_post_copyright p&#123;margin:0;&#125;.my_post_copyright span &#123; display: inline-block; width: 5.2em; color: #b5b5b5; font-weight: bold;&#125;.my_post_copyright .raw &#123; margin-left: 1em; width: 5em;&#125;.my_post_copyright a &#123; color: #808080; border-bottom:0;&#125;.my_post_copyright a:hover &#123; color: #a3d2a3; text-decoration: underline;&#125;.my_post_copyright:hover .fa-clipboard &#123; color: #000;&#125;.my_post_copyright .post-url:hover &#123; font-weight: normal;&#125;.my_post_copyright .copy-path &#123; margin-left: 1em; width: 1em; +mobile()&#123;display:none;&#125;&#125;.my_post_copyright .copy-path:hover &#123; color: #808080; cursor: pointer;&#125; 修改next/layout/_macro/post.swig，在代码 123&#123;#####################&#125;&#123;### END POST BODY ###&#125;&#123;#####################&#125; 之前添加增加如下代码： 12345&lt;div&gt; &#123;% if not is_index %&#125; &#123;% include &apos;my-copyright.swig&apos; %&#125; &#123;% endif %&#125;&lt;/div&gt; 修改next/source/css/_common/components/post/post.styl文件，在最后一行增加代码： 1@import &quot;my-post-copyright&quot; 保存重新生成即可。 微信：sgsgy5 qq:869087033 欢迎交流，搭建走了很多坑。 友情链接 参考： 技术小白搭建hexo+github博客 next最新版主题下载使用 next主题官方文档 next主题个性化教程 next主题配置文件详解 NexT v6.0+ 背景动画Canvas_nest设置无效的解决方案 给Hexo搭建的博客增加百度谷歌搜索引擎验证 添加文章字数和读取文章的时间 hexo + next主题高级配置 关于博客图片上传方法]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go语言简单排序之冒泡和插入排序]]></title>
    <url>%2F2018%2F09%2F01%2Fgo%E8%AF%AD%E8%A8%80%E5%86%92%E6%B3%A1%E5%92%8C%E6%8F%92%E5%8F%991%2F</url>
    <content type="text"><![CDATA[编程即数学，在编程中也会遇到很多的数学问题的集合，今天我们来讲解一下编程中最常见的冒泡排序，以及冒泡排序之后的插入排序 1：冒泡排序：见名知意，冒泡在我们生活当中可以有哪些常见的事物呢，比如在生活当中，大家都见到过烧开水的状态，那么水中的气泡就会不断的往上面漂浮，应用物理学上的知识来讲就是气泡的质量比较轻，在水中有浮力，就会不断的上浮，那么我们应该怎样应用到编程中的冒泡排序呢，我们先来看一段代码，然后慢慢分析 12345678910111213141516171819package mainimport "fmt"//func main() &#123; arr := [10]int&#123;9, 1, 5, 6, 3, 7, 10, 8, 2, 4&#125; //先定义一个乱序数组 //冒泡排序 for i := 0; i &lt; 10-1; i++ &#123; //外面的循环用来循环次数 for j := 0; j &lt; 10-1-i; j++ &#123; //里面的循环用来循环 每次对比到哪里 if arr[j] &gt; arr[j+1] &#123; //数据交换 arr[j], arr[j+1] = arr[j+1], arr[j] //go语言的多个数据交换格式 //temp := arr[j] //普通数据交换格式 //arr[j] = arr[j+1] //arr[j+1] = temp &#125; &#125; &#125; fmt.Println(arr)&#125; 我们看到这个代码和这张图片，在图片中我们只写了前面几次，先来看第一次，第一个元素和第二个相比4比2大，如果第一个元素比第二个大那么就交换一下，然后第二个元素和第三个相比，如果大就交换，然后第三第四相比，第四第五相比，一直比到最后一个和倒数第一个，有没有发现这样比一次就能确定一个最大的数，而且最大的数是放在最后一个元素里面的，这样一次就是外面的外循环 1for i := 0; i &lt; 10-1; i++ &#123; //这句话就是外面的循环 然后确定第一个最大的放最后一个，那么我们然后怎么办呢 ，然后我们当然继续下一次对比然后再确定一个第二大的放在倒数第二的位置啊，最大的确定下来后，我们继续从第一个开始遍历，但是这次遍历要注意了，不需要遍历到最后一个元素，而只需遍历到倒数第二个就行了，这是为什么呢，因为最后一个元素已经确定下来是最大的了，所以就不需要对比了，我们来看内循环 12for j := 0; j &lt; 10-1-i; j++ &#123; //里面的循环用来循环 每次对比到哪里//这里的判断条件是 小于10-1-i，i是什么呢，就是外循环的次数，所以只需要对比到10-1-i就行 然后内循环每次对比相邻的两个元素，如果前面大于后面的那么就交换， 12345//数据交换 arr[j], arr[j+1] = arr[j+1], arr[j] //go语言的多个数据交换格式 //temp := arr[j] //普通数据交换格式 //arr[j] = arr[j+1] //arr[j+1] = temp 这里面数据交换 有两种格式，第一种就是GO语言里面的简单交换格式，第二种是常见的交换数据格式，需要定义一个临时变量 然后可以打印出来数组，就变成从小到大的升序数组了， 那么如果要变成降序排序怎么改呢？ 来看这句话 1if arr[j] &gt; arr[j+1] &#123; 我们只需要把这里的大于号改成小于号就行啦 不喜勿喷，谢谢哈哈插入排序后续]]></content>
      <categories>
        <category>数据结构和算法</category>
      </categories>
      <tags>
        <tag>go语言</tag>
        <tag>排序问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[go语言中的面向对象，接口类型，工厂设计模式解读]]></title>
    <url>%2F2018%2F09%2F01%2Fgo%E8%AF%AD%E8%A8%80%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[工厂模式： 定义一个用于创建对象的接口，让子类决定实例化哪一个类抽象工厂模式：为创建一组相关或相互依赖的对象提供一个接口，而且无需指定他们的具体类个人觉得这个区别在于产品，如果产品单一，最合适用工厂模式，但是如果有多个业务品种、业务分类时，通过抽象工厂模式产生需要的对象是一种非常好的解决方式。再通俗深化理解下：工厂模式针对的是一个产品等级结构 ，抽象工厂模式针对的是面向多个产品等级结构的。工厂方法模式 抽象工厂模式针对的是一个产品等级结构 针对的是面向多个产品等级结构一个抽象产品类 多个抽象产品类可以派生出多个具体产品类 每个抽象产品类可以派生出多个具体产品类一个抽象工厂类，可以派生出多个具体工厂类 一个抽象工厂类，可以派生出多个具体工厂类每个具体工厂类只能创建一个具体产品类的实例 每个具体工厂类可以创建多个具体产品类的实例加减乘除四则运算器工厂模式举例子 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package mainimport "fmt"type operation struct&#123; //定义一个父类两个数据 num1 float64 num2 float64&#125;type operationAdd struct&#123; //加法子类 operation&#125;func (op *operationAdd)getresult()float64&#123; //加法类的方法 return op.num1+op.num2&#125;type operationSub struct&#123; //减法子类 operation&#125;func (sub *operationSub)getresult()float64&#123; //减法类的方法 return sub.num1-sub.num2&#125;type operationMult struct&#123; //乘法子类 operation&#125;func (mult *operationMult)getresult()float64&#123; return mult.num1*mult.num2&#125;type operationDivi struct&#123; operation&#125; //除法子类func (divi *operationDivi)getresult()float64&#123; return divi.num1/divi.num2&#125;type operationer interface&#123; //定义接口 getresult() float64 //加法的方法&#125;type operationfactor struct &#123; //operation //用于创建对象的类，工厂模式&#125;func (op *operationfactor)creatoperation(ope string,num1 float64,num2 float64)float64&#123; //用于构件对象类 var result float64 switch ope &#123; case "+": add:=&amp;operationAdd&#123;operation&#123;num1,num2&#125;&#125; //按照传过来的符号来创建相应的对象 result=operationwho(add) //传递给多态的函数，直接调用 case "-": sub:=&amp;operationSub&#123;operation&#123;num1,num2&#125;&#125; result=operationwho(sub) case "*": mult:=&amp;operationMult&#123;operation&#123;num1,num2&#125;&#125; result=operationwho(mult) case "/": divi:=&amp;operationDivi&#123;operation&#123;num1,num2&#125;&#125; result=operationwho(divi) &#125; return result&#125;func operationwho(i operationer)float64&#123; return i.getresult() //此处为创建一个多态的函数&#125;func main()&#123; //m:=&amp;operationAdd&#123;operation&#123;3,4&#125;&#125; //var iop operationer //iop=m //sum:=iop.getresult() //fmt.Println(sum) var op1 operationfactor //直接创建工厂类对象 sum:=op1.creatoperation("+",9,6) //直接调用工厂类的方法 fmt.Println(sum) var op2 operationfactor sub:=op2.creatoperation("-",9,8) fmt.Println(sub) var op3 operationfactor mult:=op3.creatoperation("*",3,4) fmt.Println(mult) var op4 operationfactor div:=op4.creatoperation("/",9,10) fmt.Println(div)&#125; 在上面的例子当中，如果对面向对象没有接触的话可能会有一些不好理解，在go语言当中面向对象可能和别的语言有一些不同，go语言是利用匿名字段来实现继承，在上面的例子中多态函数的实现可以让函数调用更加方便，比如每个结构体类都有10几个甚至更多的函数，那么直接都把这些函数封装在多态的函数里面，那么每次调用直接传递一个结构体类给多态函数就直接全部调用了，这样就是很方便的]]></content>
      <categories>
        <category>GO语言</category>
      </categories>
      <tags>
        <tag>go语言</tag>
      </tags>
  </entry>
</search>
